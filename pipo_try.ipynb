{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All try code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\pipo\\notch_EDF\\sess1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\pipo\\notch_EDF\\sess2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\pipo\\notch_EDF\\sess3.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\NutF8\\notch_EDF\\sess1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\NutF8\\notch_EDF\\sess2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\NutF8\\notch_EDF\\sess3.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\AJpang\\notch_EDF\\sess1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\AJpang\\notch_EDF\\sess2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\AJpang\\notch_EDF\\sess3.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\Aoomim\\notch_EDF\\sess1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\Aoomim\\notch_EDF\\sess2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\Aoomim\\notch_EDF\\sess3.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\voen\\notch_EDF\\sess1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\voen\\notch_EDF\\sess2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\voen\\notch_EDF\\sess3.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Successful to create Data of ['pipo', 'NutF8', 'AJpang', 'Aoomim', 'voen']\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from mne.datasets import eegbci\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from mne.channels import make_standard_montage\n",
    "from scipy import signal\n",
    "from scipy.linalg import sqrtm, inv \n",
    "\n",
    "def GetRawEDF(target_subjects= \"all\", condition=\"offline\"):\n",
    "\n",
    "    EEG_data = {}\n",
    "\n",
    "    if condition == \"offline\":\n",
    "        condition = \"Offline_Experiment\"\n",
    "    elif condition == \"online\":\n",
    "        condition = \"Online_Experiment\"\n",
    "\n",
    "    if target_subjects == \"all\":\n",
    "        target_subjects = [\"pipo\",\"NutF8\",\"AJpang\",\"Aoomim\",\"voen\"]\n",
    "\n",
    "    for i in range (0,len(target_subjects)):\n",
    "\n",
    "        path = \"C:\\\\git\\Senior_Thesis\\\\DataSet\\\\\"+condition+\"\\\\\"+ target_subjects[i] +\"\\\\notch_EDF\\\\\"\n",
    "        list_dir = listdir(path)\n",
    "        raw_each = [0] * len(list_dir)\n",
    "        for j in range(len(list_dir)):\n",
    "            raw_each[j] = mne.io.read_raw_edf(path+list_dir[j],preload = False)\n",
    "            \n",
    "        raw_edf = mne.concatenate_raws(raw_each)\n",
    "\n",
    "        eegbci.standardize(raw_edf)  # set channel names\n",
    "        montage = make_standard_montage(\"standard_1005\")\n",
    "        raw_edf.set_montage(montage)\n",
    "\n",
    "        EEG_data[target_subjects[i]] = {\"Raw_data\": raw_edf.copy()}\n",
    "\n",
    "    print(f\"Successful to create Data of {target_subjects}\")\n",
    "\n",
    "    return EEG_data\n",
    "\n",
    "EEG_data = GetRawEDF(target_subjects = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipo\n",
      "nutF8\n",
      "AJpang\n",
      "Aoomim\n"
     ]
    }
   ],
   "source": [
    "target_sub = [\"pipo\",\"nutF8\",\"AJpang\",\"Aoomim\"]\n",
    "\n",
    "a = [0,0,0,0]\n",
    "\n",
    "EEG_data = {}\n",
    "\n",
    "for i in target_sub:\n",
    "    EEG_data[i] = {\"Raw_data\": a.copy()}\n",
    "\n",
    "nor = [5,5,5]\n",
    "\n",
    "for key in EEG_data:\n",
    "    print(key)\n",
    "    EEG_data[key][\"EA_data\"] = nor\n",
    "\n",
    "\n",
    "# for sub_key in EEG_data:\n",
    "#     for data_key in EEG_data[sub_key]:\n",
    "#         print(sub_key,data_key)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pipo': {'data': [0, 0, 0, 0], 'EA_data': [5, 5, 5]},\n",
       " 'nutF8': {'data': [0, 0, 0, 0], 'EA_data': [5, 5, 5]},\n",
       " 'AJpang': {'data': [0, 0, 0, 0], 'EA_data': [5, 5, 5]},\n",
       " 'Aoomim': {'data': [0, 0, 0, 0], 'EA_data': [5, 5, 5]}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EEG_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try CSP with multiple subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from mne.decoding import CSP\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit,StratifiedKFold ,cross_val_score, cross_val_predict, KFold\n",
    "\n",
    "def computeCSPFeatures(data, target_subjects,condition):\n",
    "\n",
    "    all_data = None\n",
    "    label = None\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        query = \"Raw_Epoch\"\n",
    "    else:\n",
    "        query = \"EA_Epoch\"\n",
    "\n",
    "    for sub in target_subjects:\n",
    "        if all_data is None:\n",
    "            all_data = data[sub]['Raw_Epoch']\n",
    "        else:\n",
    "            all_data = np.concatenate((all_data, data[sub][query]), axis=0)\n",
    "\n",
    "        if label is None:\n",
    "            label = data[sub]['label']\n",
    "        else:\n",
    "            label = np.concatenate((label, data[sub]['label']), axis=0)\n",
    "\n",
    "\n",
    "    print(np.array(all_data).shape, np.array(label).shape)\n",
    "    \n",
    "    csp = CSP(n_components=8, reg=None, log=None, rank= 'info')\n",
    "\n",
    "    train_data_csp, label = shuffle(all_data, label, random_state = 0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_data_csp, label, test_size= 0.3, random_state = 0, stratify=label)\n",
    "    csp.fit(X_train, y_train)\n",
    "\n",
    "    X_train = csp.transform(X_train)\n",
    "    X_test  = csp.transform(X_test)\n",
    "\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    score = cross_val_score(lda, X_train, y_train, cv= 10)\n",
    "    print(\"LDA only Cross-validation scores:\", np.mean(score))\n",
    "    lda.fit(X_train, y_train)\n",
    "        \n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "    y_pred = lda.predict(X_train)\n",
    "\n",
    "    print(\"Classification TRAIN DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_train, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_train, y_pred=y_pred))\n",
    "\n",
    "    y_pred = lda.predict(X_test)\n",
    "\n",
    "    print(\"Classification TEST DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "computeCSPFeatures(EEG_Epochs, target_subjects = [\"pipo\",\"voen\",\"AJpang\"] ,condition = \"noEA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class LogReg_TL(BaseEstimator):\n",
    "\n",
    "    def __init__(self, learningRate=1e-5, num_iter=100, penalty=None, intercept = True,\\\n",
    "                 lambd=1, Sigma_TL=np.array([[0, 0],[0, 0]]), mu=0):\n",
    "        \n",
    "        self.learningRate = learningRate\n",
    "        self.num_iter = num_iter\n",
    "        self.penalty = penalty\n",
    "        self.intercept = intercept\n",
    "        self.Sigma_TL = Sigma_TL\n",
    "        self.lambd = lambd\n",
    "        self.mu = mu\n",
    "\n",
    "    def __softmax(self,z): #Change from sigmoid to softmax for multi-classification\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def __reg_logLL1(self,z, y, weights): # cal sum of negative log-likelihood (2)\n",
    "        reg = self.lambd * np.linalg.norm(weights)**2\n",
    "        return (-1 * np.sum((y * np.log10(self.__softmax(z))) + ((1 - y) * np.log10(1 - self.__softmax(z)))) ) + reg\n",
    "    \n",
    "    def __reg_logLL2(self, z, y, weights): # cal L2 weight (target_subjects) (4)\n",
    "        Sigma_TL_det = np.log10(np.linalg.det(self.Sigma_TL))\n",
    "        reg = 0.5 * self.lambd * np.sum( ((weights-self.mu)**2)@self.Sigma_TL) + Sigma_TL_det \n",
    "\n",
    "        return (-1 * np.sum((y * np.log10(self.__softmax(z))) + ((1 - y) * np.log10(1 - self.__softmax(z)))) ) + reg\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.costs = []\n",
    "\n",
    "        n_classes = np.unique(y_train).size\n",
    "\n",
    "        if self.intercept:\n",
    "            X_train = np.c_[np.ones([np.shape(X_train)[0], 1]), X_train]\n",
    "\n",
    "        self.weights = np.zeros((np.shape(X_train)[1], n_classes))\n",
    "\n",
    "        y_train_onehot = np.eye(n_classes)[y_train]\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X_train, self.weights)\n",
    "            err = self.__softmax(z) - y_train_onehot\n",
    "            # print(self.__softmax(z))\n",
    "\n",
    "            if self.penalty == 'L1':\n",
    "                \n",
    "                # weight update\n",
    "                delta_w = np.dot(X_train.T, err)\n",
    "                self.weights += -self.learningRate * delta_w\n",
    "                self.weights[1:] += -self.learningRate *self.lambd * self.weights[1:]\n",
    "                \n",
    "                # costs\n",
    "                self.costs.append(self.__reg_logLL1(z, y_train_onehot, self.weights))\n",
    "\n",
    "            elif self.penalty == 'L2':\n",
    "\n",
    "                # weight update\n",
    "                delta_w = np.dot(X_train.T, err)\n",
    "                self.weights += -self.learningRate * delta_w\n",
    "                self.weights[1:] += -self.learningRate *self.lambd * ((self.weights - self.mu)@(np.linalg.inv(self.Sigma_TL)))[1:]\n",
    "\n",
    "                self.costs.append(self.__reg_logLL2(z, y_train_onehot, self.weights))\n",
    "                \n",
    "\n",
    "        return self\n",
    "\n",
    "def build_clf_params(data, target_subjects ,condition):\n",
    "\n",
    "    for sub in data.keys():\n",
    "\n",
    "        if sub  == target_subjects: #Don't apply weight to target subject\n",
    "            pass \n",
    "\n",
    "        else:\n",
    "            # Where the tranining data is stored\n",
    "            if condition == \"noEA\":\n",
    "                X = data[sub]['Raw_csp']\n",
    "                y = data[sub]['Raw_csp_label']\n",
    "                store_ws = 'ws_Raw'\n",
    "\n",
    "            else:\n",
    "                X = data[sub]['EA_csp']\n",
    "                y = data[sub]['EA_csp_label']\n",
    "                store_ws = 'ws_EA'\n",
    "            \n",
    "            # Use this model when training subject as source \n",
    "            model_L1 = LogReg_TL(learningRate=0.001, num_iter=30000, penalty='L1', lambd=1)\n",
    "            \n",
    "            # Fit model and store weight\n",
    "            model_L1.fit(X, y)\n",
    "            print(\"weights of \", str(sub), \": \", model_L1.weights)\n",
    "            print(\"costs of \", str(sub), \": \", model_L1.costs[len(model_L1.costs)-1])\n",
    "            data[sub][store_ws] = model_L1.weights\n",
    "\n",
    "# X_train = np.random.rand(120, 5)\n",
    "# y_train = np.random.choice([0, 1, 2, 3], size=120)\n",
    "\n",
    "build_clf_params(CSP2D_Epoch, target_subjects= target_data ,condition = \"EA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class LogReg_TL(BaseEstimator):\n",
    "\n",
    "    def __init__(self, learningRate=1e-5, num_iter=100, penalty=None, intercept = True,\\\n",
    "                 lambd=1, Sigma_TL=np.array([[0, 0],[0, 0]]), mu=0):\n",
    "        \n",
    "        self.learningRate = learningRate\n",
    "        self.num_iter = num_iter\n",
    "        self.penalty = penalty\n",
    "        self.intercept = intercept\n",
    "        self.Sigma_TL = Sigma_TL\n",
    "        self.lambd = lambd\n",
    "        self.mu = mu\n",
    "\n",
    "    def softmax(self,z): #Change from sigmoid to softmax for multi-classification\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def reg_logLL1(self, ws, X, y): # cal sum of negative log-likelihood (2)\n",
    "        # Compute predictions\n",
    "        logits = np.dot(X, ws)\n",
    "        predictions = self.softmax(logits)\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        error = -np.sum(y * np.log(predictions))\n",
    "        cost = error / m + (self.lambd / 2) * np.sum(ws**2)  # Regularization term\n",
    "\n",
    "        # Compute gradient\n",
    "        gradient = np.dot(X.T, (predictions - y)) / m + self.lambd * ws\n",
    "        return cost, gradient\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.costs = 0.0\n",
    "        n_classes = np.unique(y_train).size\n",
    "        self.weights = np.zeros((np.shape(X_train)[1], n_classes))\n",
    "        y_train_onehot = np.eye(n_classes)[y_train]\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "\n",
    "            if self.penalty == 'L1':\n",
    "                \n",
    "                self.costs, gradient = self.reg_logLL1(self.weights, X_train, y_train_onehot)\n",
    "                self.weights -= self.learningRate * gradient\n",
    "                \n",
    "        return self\n",
    "\n",
    "def build_clf_params(data, target_subjects ,condition):\n",
    "\n",
    "    for sub in data.keys():\n",
    "\n",
    "        if sub  == target_subjects: #Don't apply weight to target subject\n",
    "            pass \n",
    "\n",
    "        else:\n",
    "            # Where the tranining data is stored\n",
    "            if condition == \"noEA\":\n",
    "                X = data[sub]['Raw_csp']\n",
    "                y = data[sub]['Raw_csp_label']\n",
    "                store_ws = 'ws_Raw'\n",
    "\n",
    "            else:\n",
    "                X = data[sub]['EA_csp']\n",
    "                y = data[sub]['EA_csp_label']\n",
    "                store_ws = 'ws_EA'\n",
    "            \n",
    "            # Use this model when training subject as source \n",
    "            model_L1 = LogReg_TL(learningRate=0.001, num_iter=30000, penalty='L1', lambd=1)\n",
    "            \n",
    "            # Fit model and store weight\n",
    "            model_L1.fit(X, y)\n",
    "            print(\"weights of \", str(sub), \": \", model_L1.weights)\n",
    "            print(\"costs of \", str(sub), \": \", model_L1.costs)\n",
    "            data[sub][store_ws] = model_L1.weights\n",
    "\n",
    "build_clf_params(CSP2D_Epoch, target_subjects= target_data ,condition = \"EA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.random.rand(120, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Custom loss function with L2 regularization\n",
    "class CustomLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, lambda_t, mu, sigma_TL, lambda_s, model):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.mu = tf.convert_to_tensor(mu, dtype=tf.float32)\n",
    "        self.sigma_TL = tf.convert_to_tensor(sigma_TL, dtype=tf.float32)\n",
    "        self.lambda_s = lambda_s\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        wt = self.get_weights_from_model()\n",
    "        reg_term_tl = self.regularization_term_tl(wt)\n",
    "        reg_term_l2 = self.lambda_s * tf.norm(wt, ord='euclidean')\n",
    "        return ce_loss + self.lambda_t * reg_term_tl + reg_term_l2\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        return tf.concat([tf.reshape(w, [-1]) for w in model_weights], axis=0)\n",
    "\n",
    "    def regularization_term_tl(self, wt):\n",
    "        diff = wt - self.mu\n",
    "        reg_term = 0.5 * tf.linalg.matmul(tf.linalg.matmul(diff[tf.newaxis, :], tf.linalg.inv(self.sigma_TL)), diff[:, tf.newaxis])\n",
    "        reg_term += 0.5 * tf.math.log(tf.linalg.det(self.sigma_TL))\n",
    "        return reg_term\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss = custom_loss(y, y_pred)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data (X: features, y: labels)\n",
    "    X = np.array([[0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5]])\n",
    "    y = np.array([0, 1, 2, 1])  # Multi-class labels\n",
    "\n",
    "    # One-hot encode y\n",
    "    y_one_hot = tf.keras.utils.to_categorical(y, num_classes=3)\n",
    "\n",
    "    # Precomputed mu and sigma_TL (example values)\n",
    "    mu = np.array([0.2, 0.3, 0.4])\n",
    "    sigma_TL = np.array([[1, 0.1, 0.2], [0.1, 1, 0.3], [0.2, 0.3, 1]])\n",
    "\n",
    "    # Hyperparameters\n",
    "    lambda_t = 0.1  # Regularization parameter for TL\n",
    "    lambda_s = 0.01  # Regularization parameter for L2\n",
    "\n",
    "    # Build the model\n",
    "    model = Sequential([\n",
    "        Dense(3, input_shape=(2,), activation='softmax')  # Adjust input_shape to match the number of features in X\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.01)\n",
    "    custom_loss = CustomLoss(lambda_t, mu, sigma_TL, lambda_s, model)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X, y_one_hot, custom_loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    # Get the trained weights\n",
    "    trained_weights = [layer.get_weights() for layer in model.layers]\n",
    "    print(\"Trained weights:\\n\", trained_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'solver': 'svd'}\n",
      "Best cross-validation score: 0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load sample dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Create an LDA model\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'solver': ['svd', 'lsqr', 'eigen'],\n",
    "    'shrinkage': [None, 'auto', 0.1, 0.5, 1.0]  # Note: 'shrinkage' parameter is only used with 'lsqr' and 'eigen' solvers\n",
    "}\n",
    "\n",
    "# Handle the 'shrinkage' parameter only when the solver is 'lsqr' or 'eigen'\n",
    "class LDAParamGridSearch:\n",
    "    def __init__(self, param_grid):\n",
    "        self.param_grid = param_grid\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        best_params = None\n",
    "        best_score = -np.inf\n",
    "        for solver in self.param_grid['solver']:\n",
    "            if solver == 'svd':\n",
    "                grid = {'solver': ['svd']}\n",
    "            else:\n",
    "                grid = {'solver': [solver], 'shrinkage': self.param_grid['shrinkage']}\n",
    "            grid_search = GridSearchCV(lda, param_grid=grid, cv=5, scoring='accuracy')\n",
    "            grid_search.fit(X, y)\n",
    "            if grid_search.best_score_ > best_score:\n",
    "                best_score = grid_search.best_score_\n",
    "                best_params = grid_search.best_params_\n",
    "        self.best_params_ = best_params\n",
    "        self.best_score_ = best_score\n",
    "\n",
    "# Perform grid search\n",
    "lda_grid_search = LDAParamGridSearch(param_grid)\n",
    "lda_grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters found:\", lda_grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", lda_grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
