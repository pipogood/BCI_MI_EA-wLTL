{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCI Comprehensive Data Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Get epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Successful to create Data of ['pipo', 'NutF8', 'AJpang', 'Aoomim', 'voen', 'pipo_HCI', 'Kawin']\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from mne.datasets import eegbci\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from mne.channels import make_standard_montage\n",
    "from BCIAllFunction import BCIFuntions\n",
    "from main_utilize import Unicorn\n",
    "\n",
    "# ['Fz','C3', 'Cz','C4','Pz','PO7','Oz','PO8']\n",
    "\n",
    "target_class = [\"Left\", \"Right\", \"Non\", \"Feet\"] \n",
    "AllBCIClass = Unicorn(selectclass = target_class, desired_fz = 128, ch_pick = ['Fz','C3','Cz','C4','Pz'])\n",
    "\n",
    "target_data_0 = \"pipo\" #this subject will be test_set otherwise are train_set\n",
    "calibrate_size = 80 # (trial)\n",
    "\n",
    "condition_wLTL = \"noEA\"\n",
    "train_svm = True\n",
    "\n",
    "all_data = [\"pipo\",\"NutF8\",\"AJpang\",\"Aoomim\",\"voen\",\"pipo_HCI\",\"Kawin\"]\n",
    "\n",
    "EEG_data = AllBCIClass.GetRawEDF(target_subjects= all_data, condition=\"Offline_Experiment\") #Input data -> 250Hz with 6-32 Hz filtered + CAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n",
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n",
      "NutF8 -104.04128118288183 6\n",
      "NutF8 86.93096401141673 6\n",
      "NutF8 -104.31893713412965 11\n",
      "NutF8 91.19854783269322 11\n",
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n",
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n",
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n",
      "voen -1044.3428689131774 1\n",
      "voen 694.2336071556814 1\n",
      "voen -1039.3090824149497 2\n",
      "voen 1021.4912607644604 2\n",
      "voen -952.1519349924243 3\n",
      "voen 1248.5144240059017 3\n",
      "voen -1608.9284132711775 4\n",
      "voen 965.6585709158061 4\n",
      "voen -1275.969071501411 5\n",
      "voen 920.2083808985063 5\n",
      "voen -1051.3249792289928 6\n",
      "voen 1127.088200732215 6\n",
      "voen -1118.8426560634346 7\n",
      "voen 1023.6485819775758 7\n",
      "voen -1110.100150071416 8\n",
      "voen 1582.339498768439 8\n",
      "voen -1366.6406708885727 9\n",
      "voen 1388.9085742043058 9\n",
      "voen -1394.9850131738478 10\n",
      "voen 1248.7950534581623 10\n",
      "voen -1058.9534477025018 11\n",
      "voen 1235.0663150799874 11\n",
      "voen -909.8656260370225 12\n",
      "voen 1493.0761987368824 12\n",
      "voen -1314.4848276191683 13\n",
      "voen 1633.0580035876765 13\n",
      "voen -1545.6211509595928 14\n",
      "voen 1774.5966791494222 14\n",
      "voen -1175.509996496856 15\n",
      "voen 1226.8960635721676 15\n",
      "voen -1299.1713236171295 16\n",
      "voen 700.759425551083 16\n",
      "voen -1363.6523253095506 18\n",
      "voen 722.045752907745 18\n",
      "voen -635.5815560003313 19\n",
      "voen 780.722404087494 19\n",
      "voen -1445.0522911441922 20\n",
      "voen 756.7294013586838 20\n",
      "voen -1216.0510499369348 21\n",
      "voen 1272.2356704397362 21\n",
      "voen -1010.8657520785717 23\n",
      "voen 1109.6048169111684 23\n",
      "voen -1344.0138775139894 24\n",
      "voen 1508.221602749368 24\n",
      "voen -1503.223035047152 25\n",
      "voen 797.1176275013338 25\n",
      "voen -1064.2178893901716 26\n",
      "voen 983.050669923388 26\n",
      "voen -1410.970211201973 28\n",
      "voen 803.9811435859516 28\n",
      "voen -752.0616855454584 29\n",
      "voen 921.7948060237121 29\n",
      "voen -849.1962190199905 30\n",
      "voen 1602.5690688556285 30\n",
      "voen -766.7164827403427 31\n",
      "voen 1341.8865944180618 31\n",
      "voen -994.9920799511679 33\n",
      "voen 985.9863603784096 33\n",
      "voen -1131.2043066252386 35\n",
      "voen 837.2311436301288 35\n",
      "voen -835.8577684948345 36\n",
      "voen 1158.897599693552 36\n",
      "voen -1052.5863080096524 38\n",
      "voen 1229.5050666201091 38\n",
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n",
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n",
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n",
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n",
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipo_HCI -55.97212970445327 79\n",
      "pipo_HCI 116.32214024347675 79\n",
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n",
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n"
     ]
    }
   ],
   "source": [
    "EEG_Epochs = AllBCIClass.GetEpoch(EEG_data ,tmin= -2.0, tmax= 6.0, crop = (0,4) ,baseline= (-0.5,0.0), band_pass=(6,32),trial_removal_th = 100)\n",
    "calibrate_size = calibrate_size / EEG_Epochs[target_data_0]['Raw_Epoch'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Apply Data Alignment on RAW data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: After performing EA for all subjects, they share the same mean covariance matrix, i.e., the distances between the mean covariance matrices of different subjects are minimized (they become zero), and hence data distributions from different subjects become more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignmentMethod = \"EA\"\n",
    "\n",
    "if alignmentMethod == \"LA\":\n",
    "    AllBCIClass.ComputeLA(EEG_Epochs, target_subject= target_data_0, calibrate_size=calibrate_size)\n",
    "    if calibrate_size != 0:\n",
    "        target_data = target_data_0 + \"_test\"\n",
    "    count = 0\n",
    "\n",
    "    for index in range(len(EEG_Epochs[target_data]['KMediod_label'])):\n",
    "        if EEG_Epochs[target_data]['label'][index] == EEG_Epochs[target_data]['KMediod_label'][index]:\n",
    "            count += 1\n",
    "    print(count/len(EEG_Epochs[target_data]['KMediod_label']) * 100)\n",
    "    \n",
    "else:\n",
    "    AllBCIClass.ComputeEA(EEG_Epochs, target_subject= target_data_0, calibrate_size=calibrate_size)\n",
    "    if calibrate_size != 0:\n",
    "        target_data = target_data_0 + \"_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipo_test'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: vistualize raw data with and without EA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if calibrate_size != 0:\n",
    "#     AllBCIClass.plot_rawEA(EEG_Epochs, no_trial= 1, target_subject= target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 : CSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "CSP2D_Epoch = AllBCIClass.computeCSPFeatures(EEG_Epochs, target_subject = target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5 : T-SNE Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AllBCIClass.TSNE_Plot(CSP2D_Epoch, target_subject= target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 CSP+LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.38      0.55      0.45       202\n",
      "       Right       0.31      0.35      0.33       200\n",
      "         Non       0.37      0.31      0.33       202\n",
      "        Feet       0.39      0.23      0.29       201\n",
      "\n",
      "    accuracy                           0.36       805\n",
      "   macro avg       0.36      0.36      0.35       805\n",
      "weighted avg       0.36      0.36      0.35       805\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[111  44  31  16]\n",
      " [ 67  70  34  29]\n",
      " [ 65  49  62  26]\n",
      " [ 47  66  42  46]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.67      0.20      0.31        10\n",
      "       Right       1.00      0.40      0.57        10\n",
      "         Non       0.54      0.70      0.61        10\n",
      "        Feet       0.50      1.00      0.67        10\n",
      "\n",
      "    accuracy                           0.57        40\n",
      "   macro avg       0.68      0.57      0.54        40\n",
      "weighted avg       0.68      0.57      0.54        40\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[ 2  0  5  3]\n",
      " [ 0  4  1  5]\n",
      " [ 1  0  7  2]\n",
      " [ 0  0  0 10]]\n"
     ]
    }
   ],
   "source": [
    "AllBCIClass.classifyCSP_LDA(EEG_Epochs, target_subjects= target_data, calibrate_data= target_data_0, condition = \"noEA\") #target_sub is used for target_data otherwise are source_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.45      0.52      0.48       202\n",
      "       Right       0.43      0.39      0.41       200\n",
      "         Non       0.40      0.37      0.38       202\n",
      "        Feet       0.39      0.40      0.39       201\n",
      "\n",
      "    accuracy                           0.42       805\n",
      "   macro avg       0.42      0.42      0.42       805\n",
      "weighted avg       0.42      0.42      0.42       805\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[105  30  34  33]\n",
      " [ 48  77  33  42]\n",
      " [ 44  32  75  51]\n",
      " [ 34  41  46  80]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.60      0.90      0.72        10\n",
      "       Right       0.89      0.80      0.84        10\n",
      "         Non       0.78      0.70      0.74        10\n",
      "        Feet       0.57      0.40      0.47        10\n",
      "\n",
      "    accuracy                           0.70        40\n",
      "   macro avg       0.71      0.70      0.69        40\n",
      "weighted avg       0.71      0.70      0.69        40\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[9 0 1 0]\n",
      " [1 8 0 1]\n",
      " [1 0 7 2]\n",
      " [4 1 1 4]]\n"
     ]
    }
   ],
   "source": [
    "AllBCIClass.classifyCSP_LDA(EEG_Epochs, target_subjects= target_data, calibrate_data= target_data_0, condition = \"EA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 CSP+SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Best parameters: {'C': 1, 'kernel': 'rbf'}\n",
      "Best cross-validation score: 0.381\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.43      0.49      0.46       202\n",
      "       Right       0.48      0.41      0.44       200\n",
      "         Non       0.44      0.43      0.43       202\n",
      "        Feet       0.48      0.49      0.49       201\n",
      "\n",
      "    accuracy                           0.46       805\n",
      "   macro avg       0.46      0.46      0.46       805\n",
      "weighted avg       0.46      0.46      0.46       805\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[99 31 40 32]\n",
      " [47 83 33 37]\n",
      " [44 35 86 37]\n",
      " [40 25 37 99]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       1.00      0.20      0.33        10\n",
      "       Right       0.75      0.30      0.43        10\n",
      "         Non       0.00      0.00      0.00        10\n",
      "        Feet       0.29      1.00      0.45        10\n",
      "\n",
      "    accuracy                           0.38        40\n",
      "   macro avg       0.51      0.38      0.30        40\n",
      "weighted avg       0.51      0.38      0.30        40\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[ 2  1  0  7]\n",
      " [ 0  3  0  7]\n",
      " [ 0  0  0 10]\n",
      " [ 0  0  0 10]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "if train_svm == True:\n",
    "    AllBCIClass.classifyCSP_SVM(EEG_Epochs, target_subjects= target_data, calibrate_data= target_data_0,condition = \"noEA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Best parameters: {'C': 1, 'kernel': 'linear'}\n",
      "Best cross-validation score: 0.406\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.46      0.55      0.50       202\n",
      "       Right       0.45      0.38      0.41       200\n",
      "         Non       0.39      0.37      0.38       202\n",
      "        Feet       0.40      0.40      0.40       201\n",
      "\n",
      "    accuracy                           0.42       805\n",
      "   macro avg       0.42      0.42      0.42       805\n",
      "weighted avg       0.42      0.42      0.42       805\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[112  25  37  28]\n",
      " [ 51  75  32  42]\n",
      " [ 45  30  75  52]\n",
      " [ 37  38  46  80]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.64      0.90      0.75        10\n",
      "       Right       0.89      0.80      0.84        10\n",
      "         Non       0.64      0.70      0.67        10\n",
      "        Feet       0.50      0.30      0.37        10\n",
      "\n",
      "    accuracy                           0.68        40\n",
      "   macro avg       0.67      0.68      0.66        40\n",
      "weighted avg       0.67      0.68      0.66        40\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[9 0 1 0]\n",
      " [0 8 1 1]\n",
      " [1 0 7 2]\n",
      " [4 1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "if train_svm == True:\n",
    "    AllBCIClass.classifyCSP_SVM(EEG_Epochs, target_subjects= target_data, calibrate_data= target_data_0 ,condition = \"EA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 CSP+wLTL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9907100200653076\n",
      "Epoch 20, Loss: 1.7840031385421753\n",
      "Epoch 40, Loss: 1.7137718200683594\n",
      "Epoch 60, Loss: 1.6891484260559082\n",
      "Epoch 80, Loss: 1.6981854438781738\n",
      "Epoch 100, Loss: 1.7261500358581543\n",
      "Epoch 120, Loss: 1.7595738172531128\n",
      "Epoch 140, Loss: 1.791982889175415\n",
      "Epoch 160, Loss: 1.822627305984497\n",
      "Epoch 180, Loss: 1.8515746593475342\n",
      "Epoch 200, Loss: 1.8788220882415771\n",
      "Epoch 220, Loss: 1.9042681455612183\n",
      "Epoch 240, Loss: 1.9278955459594727\n",
      "Epoch 260, Loss: 1.9497299194335938\n",
      "Epoch 280, Loss: 1.9698561429977417\n",
      "Epoch 300, Loss: 1.9884016513824463\n",
      "Epoch 320, Loss: 2.00551700592041\n",
      "Epoch 340, Loss: 2.021357536315918\n",
      "Epoch 360, Loss: 2.036071538925171\n",
      "Epoch 380, Loss: 2.049793004989624\n",
      "Epoch 400, Loss: 2.06264066696167\n",
      "Epoch 420, Loss: 2.0747153759002686\n",
      "Epoch 440, Loss: 2.086101770401001\n",
      "Epoch 460, Loss: 2.096871852874756\n",
      "Epoch 480, Loss: 2.1070847511291504\n",
      "weights of  NutF8 :  [[array([[ 0.69943035,  0.10467657,  0.4449077 ,  0.62720466],\n",
      "       [ 0.6684168 ,  0.09534825,  0.27931866,  0.18694066],\n",
      "       [ 0.6472708 ,  0.06173044, -0.31343254, -0.47282535],\n",
      "       [-0.67609555,  0.44919395, -0.25261745,  0.2041718 ],\n",
      "       [ 0.04840541, -0.1918064 ,  0.1372729 ,  0.08163242]],\n",
      "      dtype=float32), array([ 0.037612  ,  0.22086479, -0.26071757, -0.01339855], dtype=float32)]]\n",
      "Lowest loss of  NutF8 :  1.6888537\n",
      "Epoch 0, Loss: 2.5122008323669434\n",
      "Epoch 20, Loss: 1.879012942314148\n",
      "Epoch 40, Loss: 1.688209056854248\n",
      "Epoch 60, Loss: 1.6643121242523193\n",
      "Epoch 80, Loss: 1.6562808752059937\n",
      "Epoch 100, Loss: 1.6686064004898071\n",
      "Epoch 120, Loss: 1.7007942199707031\n",
      "Epoch 140, Loss: 1.7528417110443115\n",
      "Epoch 160, Loss: 1.8240277767181396\n",
      "Epoch 180, Loss: 1.913283109664917\n",
      "Epoch 200, Loss: 2.019237518310547\n",
      "Epoch 220, Loss: 2.1403985023498535\n",
      "Epoch 240, Loss: 2.2752182483673096\n",
      "Epoch 260, Loss: 2.422168016433716\n",
      "Epoch 280, Loss: 2.579782247543335\n",
      "Epoch 300, Loss: 2.7466881275177\n",
      "Epoch 320, Loss: 2.9216229915618896\n",
      "Epoch 340, Loss: 3.1034433841705322\n",
      "Epoch 360, Loss: 3.2911252975463867\n",
      "Epoch 380, Loss: 3.483759880065918\n",
      "Epoch 400, Loss: 3.6805496215820312\n",
      "Epoch 420, Loss: 3.8807945251464844\n",
      "Epoch 440, Loss: 4.083888053894043\n",
      "Epoch 460, Loss: 4.289304256439209\n",
      "Epoch 480, Loss: 4.496591567993164\n",
      "weights of  AJpang :  [[array([[ 0.30782485,  0.18931529,  0.8630094 ,  0.02949555],\n",
      "       [ 0.21449111, -0.3768586 , -0.65763706, -0.12398765],\n",
      "       [ 0.13373758, -0.2410624 ,  0.07241704,  0.52191967],\n",
      "       [-0.43471453,  0.65094864, -0.39099896,  0.01807468],\n",
      "       [-0.18502545, -0.32448575,  0.3660178 , -0.22541241]],\n",
      "      dtype=float32), array([ 0.29770547, -0.3172863 ,  0.25958055,  0.25934613], dtype=float32)]]\n",
      "Lowest loss of  AJpang :  1.6560228\n",
      "Epoch 0, Loss: 1.7343707084655762\n",
      "Epoch 20, Loss: 1.6536694765090942\n",
      "Epoch 40, Loss: 1.709935188293457\n",
      "Epoch 60, Loss: 1.8542747497558594\n",
      "Epoch 80, Loss: 2.0553550720214844\n",
      "Epoch 100, Loss: 2.2844250202178955\n",
      "Epoch 120, Loss: 2.5303306579589844\n",
      "Epoch 140, Loss: 2.7850728034973145\n",
      "Epoch 160, Loss: 3.043458938598633\n",
      "Epoch 180, Loss: 3.3018035888671875\n",
      "Epoch 200, Loss: 3.557013988494873\n",
      "Epoch 220, Loss: 3.806400775909424\n",
      "Epoch 240, Loss: 4.0476155281066895\n",
      "Epoch 260, Loss: 4.278696060180664\n",
      "Epoch 280, Loss: 4.498104095458984\n",
      "Epoch 300, Loss: 4.7047271728515625\n",
      "Epoch 320, Loss: 4.897841453552246\n",
      "Epoch 340, Loss: 5.0770721435546875\n",
      "Epoch 360, Loss: 5.242338180541992\n",
      "Epoch 380, Loss: 5.393804550170898\n",
      "Epoch 400, Loss: 5.531830787658691\n",
      "Epoch 420, Loss: 5.656933307647705\n",
      "Epoch 440, Loss: 5.769742488861084\n",
      "Epoch 460, Loss: 5.870973110198975\n",
      "Epoch 480, Loss: 5.961394309997559\n",
      "weights of  Aoomim :  [[array([[ 0.44551784,  0.50124705,  1.0118136 ,  0.07927709],\n",
      "       [-0.45644793, -0.4333766 , -0.00668003, -0.34445435],\n",
      "       [-0.39098734, -0.2745194 , -0.0091504 , -0.40671146],\n",
      "       [ 0.19791952,  0.39831126, -0.14329189,  0.17993197],\n",
      "       [ 0.17732051, -0.31142735,  0.05584546, -0.29344583]],\n",
      "      dtype=float32), array([-0.15705521,  0.17934546,  0.21735133,  0.01853872], dtype=float32)]]\n",
      "Lowest loss of  Aoomim :  1.6533542\n",
      "Epoch 0, Loss: 1.9426602125167847\n",
      "Epoch 20, Loss: 1.8073773384094238\n",
      "Epoch 40, Loss: 1.7673755884170532\n",
      "Epoch 60, Loss: 1.809708595275879\n",
      "Epoch 80, Loss: 1.9227275848388672\n",
      "Epoch 100, Loss: 2.092203378677368\n",
      "Epoch 120, Loss: 2.3068246841430664\n",
      "Epoch 140, Loss: 2.558267116546631\n",
      "Epoch 160, Loss: 2.8404970169067383\n",
      "Epoch 180, Loss: 3.1480941772460938\n",
      "Epoch 200, Loss: 3.4762563705444336\n",
      "Epoch 220, Loss: 3.8208723068237305\n",
      "Epoch 240, Loss: 4.178391933441162\n",
      "Epoch 260, Loss: 4.545764923095703\n",
      "Epoch 280, Loss: 4.920395851135254\n",
      "Epoch 300, Loss: 5.300094127655029\n",
      "Epoch 320, Loss: 5.6830315589904785\n",
      "Epoch 340, Loss: 6.067695140838623\n",
      "Epoch 360, Loss: 6.452853202819824\n",
      "Epoch 380, Loss: 6.837515830993652\n",
      "Epoch 400, Loss: 7.220898628234863\n",
      "Epoch 420, Loss: 7.602390766143799\n",
      "Epoch 440, Loss: 7.981534004211426\n",
      "Epoch 460, Loss: 8.357992172241211\n",
      "Epoch 480, Loss: 8.731523513793945\n",
      "weights of  voen :  [[array([[-0.63026553, -0.6017191 ,  0.33547992,  0.1114205 ],\n",
      "       [ 1.0158901 ,  0.5442106 , -0.15259041, -0.3063144 ],\n",
      "       [-0.38913342,  0.07165652,  0.3498587 ,  1.1206174 ],\n",
      "       [-0.26700288, -0.39934734,  0.11964832, -0.17234136],\n",
      "       [ 0.40024373,  0.20270035, -0.4679286 , -0.35625157]],\n",
      "      dtype=float32), array([ 0.09156931, -0.0490059 ,  0.1194631 ,  0.02215698], dtype=float32)]]\n",
      "Lowest loss of  voen :  1.7672889\n",
      "Epoch 0, Loss: 1.870598316192627\n",
      "Epoch 20, Loss: 1.6779414415359497\n",
      "Epoch 40, Loss: 1.638485312461853\n",
      "Epoch 60, Loss: 1.668443202972412\n",
      "Epoch 80, Loss: 1.7736527919769287\n",
      "Epoch 100, Loss: 1.9358432292938232\n",
      "Epoch 120, Loss: 2.1436166763305664\n",
      "Epoch 140, Loss: 2.3841145038604736\n",
      "Epoch 160, Loss: 2.6466193199157715\n",
      "Epoch 180, Loss: 2.9227423667907715\n",
      "Epoch 200, Loss: 3.206009864807129\n",
      "Epoch 220, Loss: 3.4916791915893555\n",
      "Epoch 240, Loss: 3.776259422302246\n",
      "Epoch 260, Loss: 4.057210445404053\n",
      "Epoch 280, Loss: 4.332683086395264\n",
      "Epoch 300, Loss: 4.601344108581543\n",
      "Epoch 320, Loss: 4.862244606018066\n",
      "Epoch 340, Loss: 5.114726543426514\n",
      "Epoch 360, Loss: 5.358358383178711\n",
      "Epoch 380, Loss: 5.592868804931641\n",
      "Epoch 400, Loss: 5.8181257247924805\n",
      "Epoch 420, Loss: 6.03408670425415\n",
      "Epoch 440, Loss: 6.240792751312256\n",
      "Epoch 460, Loss: 6.43834114074707\n",
      "Epoch 480, Loss: 6.62687349319458\n",
      "weights of  pipo_HCI :  [[array([[-0.33112738,  0.07389996, -0.5549277 , -0.05321503],\n",
      "       [ 0.24651302,  0.05904262, -0.6631204 ,  0.09129556],\n",
      "       [-0.07514906, -0.3865533 ,  0.70075476,  0.11216149],\n",
      "       [ 0.10082091, -0.33628067,  0.03006439, -0.23004216],\n",
      "       [ 0.25040123,  1.0714712 ,  0.10438707,  0.23349786]],\n",
      "      dtype=float32), array([ 0.05965287,  0.23797967, -0.18930419,  0.05492524], dtype=float32)]]\n",
      "Lowest loss of  pipo_HCI :  1.6379787\n",
      "Epoch 0, Loss: 1.8414475917816162\n",
      "Epoch 20, Loss: 1.714311122894287\n",
      "Epoch 40, Loss: 1.760269045829773\n",
      "Epoch 60, Loss: 1.8394958972930908\n",
      "Epoch 80, Loss: 1.9383575916290283\n",
      "Epoch 100, Loss: 2.050126552581787\n",
      "Epoch 120, Loss: 2.1713991165161133\n",
      "Epoch 140, Loss: 2.292947769165039\n",
      "Epoch 160, Loss: 2.407952308654785\n",
      "Epoch 180, Loss: 2.5125954151153564\n",
      "Epoch 200, Loss: 2.605407238006592\n",
      "Epoch 220, Loss: 2.6863527297973633\n",
      "Epoch 240, Loss: 2.756132125854492\n",
      "Epoch 260, Loss: 2.8157801628112793\n",
      "Epoch 280, Loss: 2.8664398193359375\n",
      "Epoch 300, Loss: 2.9092297554016113\n",
      "Epoch 320, Loss: 2.9451792240142822\n",
      "Epoch 340, Loss: 2.9752161502838135\n",
      "Epoch 360, Loss: 3.000164031982422\n",
      "Epoch 380, Loss: 3.020753860473633\n",
      "Epoch 400, Loss: 3.037631034851074\n",
      "Epoch 420, Loss: 3.0513672828674316\n",
      "Epoch 440, Loss: 3.062462568283081\n",
      "Epoch 460, Loss: 3.0713562965393066\n",
      "Epoch 480, Loss: 3.0784287452697754\n",
      "weights of  Kawin :  [[array([[-0.6931322 , -0.44537336,  0.23424761, -0.508628  ],\n",
      "       [ 0.43187472,  0.24775814, -0.05925572, -0.5557117 ],\n",
      "       [-0.5116306 , -0.2976284 , -0.63064986,  0.40611452],\n",
      "       [ 0.1156373 , -0.24461624, -0.24811108,  0.07855127],\n",
      "       [-0.6346621 ,  0.39299738,  0.57051045, -0.06009809]],\n",
      "      dtype=float32), array([-0.19785088,  0.17325489,  0.17813168, -0.13411121], dtype=float32)]]\n",
      "Lowest loss of  Kawin :  1.7137226\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Custom loss function\n",
    "class CustomLossLL1(tf.keras.losses.Loss):\n",
    "    def __init__(self, lambda_t, model):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        ws = self.get_weights_from_model()\n",
    "        reg_term = self.regularization_term(ws)\n",
    "        return ce_loss + self.lambda_t * reg_term\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        # return tf.concat([tf.reshape(w, [-1]) for w in model_weights], axis=0)\n",
    "        return model_weights\n",
    "\n",
    "    def regularization_term(self, ws):\n",
    "        reg_term = tf.pow(tf.norm(ws, ord='euclidean'),2)\n",
    "        return reg_term\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True) # Perform a forward pass and compute the predictions\n",
    "        loss = custom_loss(y, y_pred) # Compute the custom loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_weight_LL(X_train, y_train, lambd, num_tier=10, learning_rate = 0.01):\n",
    "    n_classes = np.unique(y_train).size\n",
    "    _, sequential_indices = np.unique(y_train, return_inverse=True)\n",
    "    y_one_hot = tf.keras.utils.to_categorical(sequential_indices, num_classes=n_classes)\n",
    "\n",
    "    lambda_t = lambd  # Regularization parameter\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_classes, input_shape=(X_train.shape[1],), activation='softmax')  # Adjust input_shape to match the number of features in X\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate)\n",
    "    custom_loss = CustomLossLL1(lambda_t, model)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = num_tier\n",
    "    lowest_loss = float('inf')\n",
    "    best_weights = None\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X_train, y_one_hot, custom_loss)\n",
    "        loss_value = loss.numpy()\n",
    "        if loss_value < lowest_loss:\n",
    "            lowest_loss = loss_value\n",
    "            best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    # best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "    return best_weights, lowest_loss\n",
    "\n",
    "def build_clf_params(data, target_subjects ,condition):\n",
    "\n",
    "    for sub in data.keys():\n",
    "        if (sub  != target_subjects) and  (sub != target_data_0): #Don't apply weight to target subject\n",
    "            # Where the tranining data is stored\n",
    "            if condition == \"noEA\":\n",
    "                X = data[sub]['Raw_csp']\n",
    "                y = data[sub]['Raw_csp_label']\n",
    "                store_ws = 'ws_Raw'\n",
    "\n",
    "            else:\n",
    "                X = data[sub]['EA_csp']\n",
    "                y = data[sub]['EA_csp_label']\n",
    "                store_ws = 'ws_EA'\n",
    "\n",
    "            weights, loss = train_weight_LL(X_train=X, y_train=y, lambd= 0.1, num_tier=500, learning_rate= 0.01)\n",
    "            print(\"weights of \", str(sub), \": \", weights)\n",
    "            print(\"Lowest loss of \", str(sub), \": \", loss)\n",
    "            data[sub][store_ws] = weights\n",
    "\n",
    "build_clf_params(CSP2D_Epoch, target_subjects= target_data ,condition = condition_wLTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "Weights (array): These are the parameters that the model learns during training. They represent the strength of the connections between neurons. The shape of the weights array is (input_dim, output_dim).\n",
    "\n",
    "Biases (array): These are additional parameters that are added to the outputs of each layer. The shape of the biases array is (output_dim,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Raw_csp', 'Raw_csp_label', 'EA_csp', 'EA_csp_label'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSP2D_Epoch[target_data].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the kl divergence\n",
    "def KL_div(P, Q):\n",
    "    # First convert to np array\n",
    "    P = np.array(P)\n",
    "    Q = np.array(Q)\n",
    "    \n",
    "    # Then compute their means, datain shape of samples x feat\n",
    "    mu_P = np.mean(P, axis=0)\n",
    "    mu_Q = np.mean(Q, axis=0)    \n",
    "\n",
    "    \n",
    "    # Compute their covariance\n",
    "    sigma_P = np.cov(P, rowvar=False)\n",
    "    sigma_Q = np.cov(Q, rowvar=False)  \n",
    "\n",
    "    diff = mu_Q - mu_P\n",
    "\n",
    "    inv_sigma_Q = np.linalg.inv(sigma_Q)\n",
    "    term1 = np.dot(np.dot(diff.T, inv_sigma_Q), diff)\n",
    "    \n",
    "    # Calculate the trace term trace(Sigma_Q^{-1} * Sigma_P)\n",
    "    term2 = np.trace(np.dot(inv_sigma_Q, sigma_P))\n",
    "    \n",
    "    # Calculate the determinant term ln(det(Sigma_P) / det(Sigma_Q))\n",
    "    det_sigma0 = np.linalg.det(sigma_P)\n",
    "    det_sigma1 = np.linalg.det(sigma_Q)\n",
    "\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    term3 = np.log((det_sigma0+epsilon) / (det_sigma1+epsilon))\n",
    "    \n",
    "    print(term3)\n",
    "    \n",
    "    # Dimensionality of the data\n",
    "    K = mu_P.shape[0]\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_div = 0.5 * (term1 + term2 - term3 - K)\n",
    "    \n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3046770928094429\n",
      "-0.30313800831733073\n",
      "-0.3956192787448664\n",
      "0.12879469476507202\n",
      "0.36642578263452724\n",
      "0.11106366462871675\n",
      "0.4395169503165339\n",
      "0.2499712563543412\n",
      "0.34141913578085487\n",
      "0.09826709477295835\n",
      "0.32016139349977046\n",
      "0.145372676226133\n",
      "-0.07811140654969706\n",
      "-0.07603681956520206\n",
      "0.36610202802832825\n",
      "0.21080000037352828\n",
      "0.0327858806979166\n",
      "-0.2657393129127305\n",
      "-0.5695613042016362\n",
      "0.012880758601432106\n",
      "0.07732334004047996\n",
      "-0.17556017824454417\n",
      "0.22941825284682085\n",
      "-0.11652401440392393\n"
     ]
    }
   ],
   "source": [
    "# Compute kl divergence of target subject to each source subject\n",
    "def compute_all_kl_div(data, target_subjects , condition):\n",
    "    '''\n",
    "    Parameter:\n",
    "    data, is the whole data containing target and source data\n",
    "    '''\n",
    "    kl_div_score = []\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        target_data = 'Raw_csp'\n",
    "        label_name = 'Raw_csp_label'\n",
    "\n",
    "    else:\n",
    "        target_data = 'EA_csp'\n",
    "        label_name = 'EA_csp_label'\n",
    "        \n",
    "    # cal P from target data\n",
    "    label_tgt =  data[target_subjects][label_name]\n",
    "    P_left =  data[target_subjects][target_data][np.where(label_tgt == 0)]\n",
    "    P_right = data[target_subjects][target_data][np.where(label_tgt == 1)]\n",
    "    P_non = data[target_subjects][target_data][np.where(label_tgt == 2)]\n",
    "    P_feet = data[target_subjects][target_data][np.where(label_tgt == 3)]\n",
    "\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "\n",
    "    #cal Q from each source subject\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            label_src =  data[sub][label_name]\n",
    "            Q_left =  data[sub][target_data][np.where(label_src == 0)]\n",
    "            Q_right = data[sub][target_data][np.where(label_src == 1)]\n",
    "            Q_non = data[sub][target_data][np.where(label_src == 2)]\n",
    "            Q_feet = data[sub][target_data][np.where(label_src == 3)]\n",
    "\n",
    "            kl_left = KL_div(P_left, Q_left)\n",
    "            kl_right = KL_div(P_right, Q_right)\n",
    "            kl_non = KL_div(P_non, Q_non)\n",
    "            kl_feet = KL_div(P_feet, Q_feet)\n",
    "\n",
    "            kl_div_temp = [kl_left, kl_right, kl_non, kl_feet]\n",
    "\n",
    "            kl_div_score.append(kl_div_temp)\n",
    "\n",
    "    data[target_subjects]['kl_div'] = kl_div_score\n",
    "\n",
    "\n",
    "compute_all_kl_div(CSP2D_Epoch, target_subjects=target_data_0 ,condition = condition_wLTL) #target_sub for cal KL is calibrate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 47.98235552,  77.3001835 , 100.18540757, 201.00360609],\n",
       "       [130.99981818, 209.72691605, 139.41718465, 244.78983674],\n",
       "       [ 64.9535467 ,  66.44475356,  59.71977882, 116.68759599],\n",
       "       [ 15.21296407,  27.28200274,  31.31743398,  66.05890354],\n",
       "       [ 20.68926923,  15.95620305,  15.81935952,  15.3332422 ],\n",
       "       [ 22.65021433,  22.42584721,  26.14708848,  16.88023443]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(CSP2D_Epoch[target_data_0]['kl_div'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8865578300743208e-07, 3.3955951325675196e-09, 5.6180531693435696e-08, 1.866952301447292e-05, 5.457716834220984e-06, 3.7992954400499027e-06]\n",
      "[2.800758438881444e-08, 5.168713840362148e-10, 5.130439914067606e-08, 1.8050475207749819e-06, 1.542662406030965e-05, 3.9536372313637e-06]\n",
      "[[6.69591275e-03 1.31706575e-03 5.15976987e-04 2.01082633e-05]\n",
      " [1.20519013e-04 2.43060446e-05 1.37588920e-04 9.14148148e-06]\n",
      " [1.99400163e-03 2.41260602e-03 4.08671424e-03 1.77047813e-04]\n",
      " [6.62632734e-01 8.48829455e-02 5.40380556e-02 1.72370424e-03]\n",
      " [1.93709385e-01 7.25442003e-01 8.30010099e-01 5.93804537e-01]\n",
      " [1.34847447e-01 1.85921074e-01 1.11211565e-01 4.04265461e-01]]\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_weights(data, target_subjects):\n",
    "    kl = data[target_subjects]['kl_div']\n",
    "    KL_inv_left = []\n",
    "    KL_inv_right = []\n",
    "    KL_inv_non = []\n",
    "    KL_inv_feet = []\n",
    "\n",
    "    alpha_s = []\n",
    "    eps = 0.0001\n",
    "    \n",
    "    #equation (9)\n",
    "    for val in kl:\n",
    "        if val != 0: \n",
    "            KL_inv_left.append(1/((val[0] + eps)**4))\n",
    "            KL_inv_right.append(1/((val[1] + eps)**4))\n",
    "            KL_inv_non.append(1/((val[2] + eps)**4))\n",
    "            KL_inv_feet.append(1/((val[3] + eps)**4))\n",
    "\n",
    "    print(KL_inv_left)\n",
    "    print(KL_inv_right)\n",
    "    \n",
    "    for i in range(0,len(KL_inv_left)):\n",
    "        temp = [KL_inv_left[i]/sum(KL_inv_left), KL_inv_right[i]/sum(KL_inv_right), KL_inv_non[i]/sum(KL_inv_non), KL_inv_feet[i]/sum(KL_inv_feet)]\n",
    "        alpha_s.append(temp)\n",
    "\n",
    "    alpha_s = np.array(alpha_s)\n",
    "    print(np.array(alpha_s[:, ~np.isnan(alpha_s).any(axis=0)]))\n",
    "                \n",
    "    data[target_subjects]['alpha_s'] = alpha_s[:, ~np.isnan(alpha_s).any(axis=0)]\n",
    "\n",
    "compute_similarity_weights(CSP2D_Epoch, target_subjects=target_data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.56963538 -0.07891807 -0.41193258 -0.23700109]\n",
      " [ 0.78274249  0.13416051 -0.56520589 -0.17102968]\n",
      " [-0.33783114 -0.33026183  0.5302145   0.23263494]\n",
      " [-0.14598631 -0.32176081  0.00305676 -0.10510545]\n",
      " [ 0.2287925   0.86655066  0.12515315  0.11369006]]\n",
      "[[ 2.68236572 -0.7282491  -0.42109064  0.57807879 -1.21808628]\n",
      " [-0.7282491   4.73896302 -3.13848736 -0.72429017  0.99904081]\n",
      " [-0.42109064 -3.13848736  2.76197986  0.65866076 -1.31708557]\n",
      " [ 0.57807879 -0.72429017  0.65866076  0.65936736 -1.55546182]\n",
      " [-1.21808628  0.99904081 -1.31708557 -1.55546182  4.04919775]]\n"
     ]
    }
   ],
   "source": [
    "def compute_ETL_and_mu_ws(data, target_subjects, condition):\n",
    "\n",
    "    mu_ws = 0\n",
    "    temp_ws = 0\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        ws_name = 'ws_Raw'\n",
    "    else:\n",
    "        ws_name = 'ws_EA'\n",
    "\n",
    "    alpha_s = np.array(data[target_subjects]['alpha_s'])\n",
    "\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "    index_count = 0\n",
    "\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            ws = data[sub][ws_name][0][0]\n",
    "            # mu_ws += ws @ alpha_s  #equation (10)\n",
    "            # mu_ws += np.dot(ws, np.transpose(alpha_s))\n",
    "            mu_ws += ws * alpha_s[index_count]\n",
    "            index_count += 1\n",
    "\n",
    "    print(np.array(mu_ws))\n",
    "\n",
    "    index_count = 0\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            ws = data[sub][ws_name][0][0]\n",
    "            # ws_min_mu = np.dot((np.dot(ws,np.transpose(alpha_s)) - mu_ws) , np.transpose((np.dot(ws,np.transpose(alpha_s)) - mu_ws)))\n",
    "            ws_min_mu = np.dot(((ws * alpha_s[index_count]) - mu_ws), np.transpose((ws * alpha_s[index_count]) - mu_ws))\n",
    "            temp_ws += ws_min_mu #equation (11)\n",
    "            index_count += 1\n",
    "\n",
    "    print(np.array(temp_ws))\n",
    "    \n",
    "    # den = np.diag(temp_ws) #get array in diagonal line\n",
    "\n",
    "    den = temp_ws\n",
    "    nom = np.trace(temp_ws) #Return the sum along diagonals of the array.\n",
    "    Sigma_TL = den/nom\n",
    "\n",
    "\n",
    "    data[target_subjects]['Sigma_TL'] = Sigma_TL\n",
    "    data[target_subjects]['mu_ws'] = mu_ws\n",
    "\n",
    "compute_ETL_and_mu_ws(CSP2D_Epoch, target_subjects = target_data_0, condition=condition_wLTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(CSP2D_Epoch[target_data_0]['Sigma_TL']).shape)\n",
    "print(np.array(CSP2D_Epoch[target_data_0]['mu_ws']).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ACC with target subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.408442735671997\n",
      "Epoch 20, Loss: 4.188305854797363\n",
      "Epoch 40, Loss: 4.640628814697266\n",
      "Epoch 60, Loss: 5.1712236404418945\n",
      "Epoch 80, Loss: 5.953761100769043\n",
      "Epoch 100, Loss: 7.131471157073975\n",
      "Epoch 120, Loss: 8.748147010803223\n",
      "Epoch 140, Loss: 10.88715648651123\n",
      "Epoch 160, Loss: 13.598322868347168\n",
      "Epoch 180, Loss: 16.937984466552734\n",
      "Epoch 200, Loss: 20.959732055664062\n",
      "Epoch 220, Loss: 25.71445655822754\n",
      "Epoch 240, Loss: 31.249401092529297\n",
      "Epoch 260, Loss: 37.607696533203125\n",
      "Epoch 280, Loss: 44.82745361328125\n",
      "Epoch 300, Loss: 52.94129180908203\n",
      "Epoch 320, Loss: 61.97607421875\n",
      "Epoch 340, Loss: 71.9527587890625\n",
      "Epoch 360, Loss: 82.88658142089844\n",
      "Epoch 380, Loss: 94.78746795654297\n",
      "Epoch 400, Loss: 107.65995025634766\n",
      "Epoch 420, Loss: 121.504150390625\n",
      "Epoch 440, Loss: 136.31591796875\n",
      "Epoch 460, Loss: 152.0874481201172\n",
      "Epoch 480, Loss: 168.8076629638672\n",
      "Epoch 500, Loss: 186.4626007080078\n",
      "Epoch 520, Loss: 205.03628540039062\n",
      "Epoch 540, Loss: 224.5105743408203\n",
      "Epoch 560, Loss: 244.8659210205078\n",
      "Epoch 580, Loss: 266.0815124511719\n",
      "Epoch 600, Loss: 288.13580322265625\n",
      "Epoch 620, Loss: 311.00640869140625\n",
      "Epoch 640, Loss: 334.670654296875\n",
      "Epoch 660, Loss: 359.10540771484375\n",
      "Epoch 680, Loss: 384.2877502441406\n",
      "Epoch 700, Loss: 410.1949768066406\n",
      "Epoch 720, Loss: 436.8045349121094\n",
      "Epoch 740, Loss: 464.09393310546875\n",
      "Epoch 760, Loss: 492.04132080078125\n",
      "Epoch 780, Loss: 520.6251831054688\n",
      "Epoch 800, Loss: 549.8246459960938\n",
      "Epoch 820, Loss: 579.6197509765625\n",
      "Epoch 840, Loss: 609.989990234375\n",
      "Epoch 860, Loss: 640.9168701171875\n",
      "Epoch 880, Loss: 672.3811645507812\n",
      "Epoch 900, Loss: 704.3651123046875\n",
      "Epoch 920, Loss: 736.8511962890625\n",
      "Epoch 940, Loss: 769.8233032226562\n",
      "Epoch 960, Loss: 803.2647705078125\n",
      "Epoch 980, Loss: 837.1600341796875\n",
      "Epoch 1000, Loss: 871.4940185546875\n",
      "Epoch 1020, Loss: 906.2523193359375\n",
      "Epoch 1040, Loss: 941.4207763671875\n",
      "Epoch 1060, Loss: 976.9857177734375\n",
      "Epoch 1080, Loss: 1012.9353637695312\n",
      "Epoch 1100, Loss: 1049.2562255859375\n",
      "Epoch 1120, Loss: 1085.9351806640625\n",
      "Epoch 1140, Loss: 1122.9619140625\n",
      "Epoch 1160, Loss: 1160.323974609375\n",
      "Epoch 1180, Loss: 1198.0107421875\n",
      "Epoch 1200, Loss: 1236.0115966796875\n",
      "Epoch 1220, Loss: 1274.3162841796875\n",
      "Epoch 1240, Loss: 1312.9136962890625\n",
      "Epoch 1260, Loss: 1351.7943115234375\n",
      "Epoch 1280, Loss: 1390.94775390625\n",
      "Epoch 1300, Loss: 1430.365234375\n",
      "Epoch 1320, Loss: 1470.0372314453125\n",
      "Epoch 1340, Loss: 1509.9537353515625\n",
      "Epoch 1360, Loss: 1550.107421875\n",
      "Epoch 1380, Loss: 1590.4876708984375\n",
      "Epoch 1400, Loss: 1631.0859375\n",
      "Epoch 1420, Loss: 1671.8939208984375\n",
      "Epoch 1440, Loss: 1712.902587890625\n",
      "Epoch 1460, Loss: 1754.103515625\n",
      "Epoch 1480, Loss: 1795.48828125\n",
      "Epoch 1500, Loss: 1837.0478515625\n",
      "Epoch 1520, Loss: 1878.775146484375\n",
      "Epoch 1540, Loss: 1920.6614990234375\n",
      "Epoch 1560, Loss: 1962.6978759765625\n",
      "Epoch 1580, Loss: 2004.876708984375\n",
      "Epoch 1600, Loss: 2047.189697265625\n",
      "Epoch 1620, Loss: 2089.628173828125\n",
      "Epoch 1640, Loss: 2132.184326171875\n",
      "Epoch 1660, Loss: 2174.849609375\n",
      "Epoch 1680, Loss: 2217.617431640625\n",
      "Epoch 1700, Loss: 2260.47802734375\n",
      "Epoch 1720, Loss: 2303.423828125\n",
      "Epoch 1740, Loss: 2346.447509765625\n",
      "Epoch 1760, Loss: 2389.5419921875\n",
      "Epoch 1780, Loss: 2432.69677734375\n",
      "Epoch 1800, Loss: 2475.907470703125\n",
      "Epoch 1820, Loss: 2519.163330078125\n",
      "Epoch 1840, Loss: 2562.457275390625\n",
      "Epoch 1860, Loss: 2605.7822265625\n",
      "Epoch 1880, Loss: 2649.12939453125\n",
      "Epoch 1900, Loss: 2692.491943359375\n",
      "Epoch 1920, Loss: 2735.86279296875\n",
      "Epoch 1940, Loss: 2779.23388671875\n",
      "Epoch 1960, Loss: 2822.59716796875\n",
      "Epoch 1980, Loss: 2865.946044921875\n",
      "Epoch 2000, Loss: 2909.27294921875\n",
      "Epoch 2020, Loss: 2952.572998046875\n",
      "Epoch 2040, Loss: 2995.837158203125\n",
      "Epoch 2060, Loss: 3039.05810546875\n",
      "Epoch 2080, Loss: 3082.229736328125\n",
      "Epoch 2100, Loss: 3125.34521484375\n",
      "Epoch 2120, Loss: 3168.3974609375\n",
      "Epoch 2140, Loss: 3211.38037109375\n",
      "Epoch 2160, Loss: 3254.2890625\n",
      "Epoch 2180, Loss: 3297.11328125\n",
      "Epoch 2200, Loss: 3339.850830078125\n",
      "Epoch 2220, Loss: 3382.49365234375\n",
      "Epoch 2240, Loss: 3425.03564453125\n",
      "Epoch 2260, Loss: 3467.470703125\n",
      "Epoch 2280, Loss: 3509.79541015625\n",
      "Epoch 2300, Loss: 3552.000732421875\n",
      "Epoch 2320, Loss: 3594.08349609375\n",
      "Epoch 2340, Loss: 3636.04150390625\n",
      "Epoch 2360, Loss: 3677.86328125\n",
      "Epoch 2380, Loss: 3719.54931640625\n",
      "Epoch 2400, Loss: 3761.091796875\n",
      "Epoch 2420, Loss: 3802.487060546875\n",
      "Epoch 2440, Loss: 3843.731201171875\n",
      "Epoch 2460, Loss: 3884.819580078125\n",
      "Epoch 2480, Loss: 3925.747314453125\n",
      "Epoch 2500, Loss: 3966.51171875\n",
      "Epoch 2520, Loss: 4007.10693359375\n",
      "Epoch 2540, Loss: 4047.52978515625\n",
      "Epoch 2560, Loss: 4087.77783203125\n",
      "Epoch 2580, Loss: 4127.849609375\n",
      "Epoch 2600, Loss: 4167.73974609375\n",
      "Epoch 2620, Loss: 4207.44140625\n",
      "Epoch 2640, Loss: 4246.95361328125\n",
      "Epoch 2660, Loss: 4286.2763671875\n",
      "Epoch 2680, Loss: 4325.40625\n",
      "Epoch 2700, Loss: 4364.3408203125\n",
      "Epoch 2720, Loss: 4403.076171875\n",
      "Epoch 2740, Loss: 4441.609375\n",
      "Epoch 2760, Loss: 4479.9423828125\n",
      "Epoch 2780, Loss: 4518.0712890625\n",
      "Epoch 2800, Loss: 4555.99365234375\n",
      "Epoch 2820, Loss: 4593.7080078125\n",
      "Epoch 2840, Loss: 4631.2119140625\n",
      "Epoch 2860, Loss: 4668.5078125\n",
      "Epoch 2880, Loss: 4705.5888671875\n",
      "Epoch 2900, Loss: 4742.45849609375\n",
      "Epoch 2920, Loss: 4779.11572265625\n",
      "Epoch 2940, Loss: 4815.5556640625\n",
      "Epoch 2960, Loss: 4851.77978515625\n",
      "Epoch 2980, Loss: 4887.7900390625\n",
      "weights of  pipo :  [[array([[-0.56659424,  0.07003823, -0.6331554 ,  0.4739833 ],\n",
      "       [ 0.7541387 , -0.35905054,  0.6712518 , -0.26469663],\n",
      "       [ 0.33820158, -0.16142207,  0.76000404,  0.33809388],\n",
      "       [-0.5985127 , -0.41596344, -0.13064754, -0.45658144],\n",
      "       [-0.08378422,  0.5302053 , -0.58526766,  0.53456366]],\n",
      "      dtype=float32), array([ 0.00499983,  0.00499939, -0.00499872, -0.00499985], dtype=float32)]]\n",
      "loss of  pipo :  3.4084427\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85        20\n",
      "           1       0.79      0.75      0.77        20\n",
      "           2       0.81      0.85      0.83        20\n",
      "           3       0.85      0.85      0.85        20\n",
      "\n",
      "    accuracy                           0.82        80\n",
      "   macro avg       0.82      0.83      0.82        80\n",
      "weighted avg       0.82      0.82      0.82        80\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[17  1  1  1]\n",
      " [ 2 15  3  0]\n",
      " [ 0  1 17  2]\n",
      " [ 1  2  0 17]]\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.70      0.74        10\n",
      "           1       0.89      0.80      0.84        10\n",
      "           2       0.73      0.80      0.76        10\n",
      "           3       0.82      0.90      0.86        10\n",
      "\n",
      "    accuracy                           0.80        40\n",
      "   macro avg       0.80      0.80      0.80        40\n",
      "weighted avg       0.80      0.80      0.80        40\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[7 1 2 0]\n",
      " [1 8 0 1]\n",
      " [1 0 8 1]\n",
      " [0 0 1 9]]\n"
     ]
    }
   ],
   "source": [
    "# Custom loss function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "class CustomLossLL2(tf.keras.losses.Loss):\n",
    "    def __init__(self, lambda_t, model, mu, sigma_TL):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "        self.mu = tf.convert_to_tensor(mu, dtype=tf.float32)\n",
    "        self.sigma_TL = tf.convert_to_tensor(sigma_TL, dtype=tf.float32)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        wt = self.get_weights_from_model()\n",
    "        reg_term = self.regularization_term(wt)\n",
    "\n",
    "        return ce_loss + (self.lambda_t * tf.linalg.matmul(reg_term, wt))\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        # return tf.concat([tf.reshape(w, [-1]) for w in model_weights], axis=0)\n",
    "        return model_weights\n",
    "\n",
    "    def regularization_term(self, wt):\n",
    "        diff = wt - self.mu\n",
    "        reg_term = 0.5 * tf.linalg.matmul(tf.linalg.matmul(tf.linalg.inv(self.sigma_TL), diff[0]), tf.transpose(diff[0]))\n",
    "        reg_term += 0.5 * tf.math.log(tf.linalg.det(self.sigma_TL))\n",
    "        return reg_term\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True) # Perform a forward pass and compute the predictions\n",
    "        loss = custom_loss(y, y_pred) # Compute the custom loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_weight_LL2(X_train, y_train, lambd, mu, sigma_TL, num_tier=10, learning_rate = 0.01):\n",
    "    n_classes = np.unique(y_train).size\n",
    "    _, sequential_indices = np.unique(y_train, return_inverse=True)\n",
    "    y_one_hot = tf.keras.utils.to_categorical(sequential_indices, num_classes=n_classes)\n",
    "\n",
    "    lambda_t = lambd  # Regularization parameter\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_classes, input_shape=(X_train.shape[1],), activation='softmax')  # Adjust input_shape to match the number of features in X\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate)\n",
    "    custom_loss = CustomLossLL2(lambda_t, model, mu, sigma_TL)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = num_tier\n",
    "    lowest_loss = float('inf')\n",
    "    best_weights = None\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X_train, y_one_hot, custom_loss)\n",
    "        loss_value = loss.numpy()\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        if (abs(loss_value) < lowest_loss):\n",
    "            lowest_loss = abs(loss_value)\n",
    "            best_model = model\n",
    "            best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "    return best_model, best_weights, lowest_loss\n",
    "\n",
    "def GetConfusionMatrix(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_prob = model.predict(X_train)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    _, y_train = np.unique(y_train, return_inverse=True)\n",
    "\n",
    "    print(\"Classification TRAIN DATA \\n=======================\")\n",
    "    print(classification_report(y_true= y_train, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true= y_train, y_pred=y_pred))\n",
    "\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    _, y_test = np.unique(y_test, return_inverse=True)\n",
    "\n",
    "    print(\"Classification TEST DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "\n",
    "def tgt_test_wLTL(data, target_subjects ,condition):\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        X = data[target_subjects]['Raw_csp']\n",
    "        y = data[target_subjects]['Raw_csp_label']\n",
    "        X_test = data[tgt_data]['Raw_csp']\n",
    "        y_test = data[tgt_data]['Raw_csp_label']\n",
    "        store_ws = 'wt_Raw'\n",
    "\n",
    "    else:\n",
    "        X = data[target_subjects]['EA_csp']\n",
    "        y = data[target_subjects]['EA_csp_label']\n",
    "        X_test = data[tgt_data]['EA_csp']\n",
    "        y_test = data[tgt_data]['EA_csp_label']\n",
    "        store_ws = 'wt_EA'\n",
    "\n",
    "    mu = data[target_subjects]['mu_ws']\n",
    "    sigma_TL = data[target_subjects]['Sigma_TL']\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state = 42, stratify=y)\n",
    "\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    model, weights, loss = train_weight_LL2(X_train=X_train, y_train=y_train, mu =mu, sigma_TL=sigma_TL,  lambd= 0.1, num_tier=3000, learning_rate= 0.005)\n",
    "    print(\"weights of \", str(target_subjects), \": \", weights)\n",
    "    print(\"loss of \", str(target_subjects), \": \", loss)\n",
    "    data[target_subjects][store_ws] = weights\n",
    "\n",
    "    GetConfusionMatrix(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "tgt_test_wLTL(CSP2D_Epoch, target_subjects= target_data_0 ,condition = condition_wLTL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
