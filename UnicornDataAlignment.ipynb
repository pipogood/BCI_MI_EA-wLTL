{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCI Comprehensive Data Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Get epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Successful to create Data of ['pipo', 'NutF8', 'AJpang', 'Aoomim', 'voen', 'pipo_HCI', 'Kawin']\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from mne.datasets import eegbci\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from mne.channels import make_standard_montage\n",
    "from BCIAllFunction import BCIFuntions\n",
    "from main_utilize import Unicorn\n",
    "\n",
    "target_class = [\"Left\", \"Right\", \"Feet\"] \n",
    "AllBCIClass = Unicorn(selectclass = target_class, desired_fz = 128, ch_pick = ['Fz','C3','Cz','C4','Pz'])\n",
    "\n",
    "target_data_0 = \"Kawin\" #this subject will be test_set otherwise are train_set\n",
    "calibrate_size = 30 # (trial)\n",
    "\n",
    "condition_wLTL = \"EA\"\n",
    "train_svm = True    \n",
    "\n",
    "all_data = [\"pipo\",\"NutF8\",\"AJpang\",\"Aoomim\",\"voen\",\"pipo_HCI\",\"Kawin\"]\n",
    "\n",
    "EEG_data = AllBCIClass.GetRawEDF(target_subjects= all_data, condition=\"Offline_Experiment\") #Input data -> 250Hz with 6-32 Hz filtered + CAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n",
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n",
      "NutF8 -104.04128118288183 6\n",
      "NutF8 86.93096401141673 6\n",
      "NutF8 -104.31893713412965 11\n",
      "NutF8 91.19854783269322 11\n",
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n",
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n",
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n",
      "voen -1044.3428689131774 1\n",
      "voen 694.2336071556814 1\n",
      "voen -1039.3090824149497 2\n",
      "voen 1021.4912607644604 2\n",
      "voen -952.1519349924243 3\n",
      "voen 1248.5144240059017 3\n",
      "voen -1608.9284132711775 4\n",
      "voen 965.6585709158061 4\n",
      "voen -1275.969071501411 5\n",
      "voen 920.2083808985063 5\n",
      "voen -1051.3249792289928 6\n",
      "voen 1127.088200732215 6\n",
      "voen -1118.8426560634346 7\n",
      "voen 1023.6485819775758 7\n",
      "voen -1110.100150071416 8\n",
      "voen 1582.339498768439 8\n",
      "voen -1366.6406708885727 9\n",
      "voen 1388.9085742043058 9\n",
      "voen -1394.9850131738478 10\n",
      "voen 1248.7950534581623 10\n",
      "voen -1058.9534477025018 11\n",
      "voen 1235.0663150799874 11\n",
      "voen -909.8656260370225 12\n",
      "voen 1493.0761987368824 12\n",
      "voen -1314.4848276191683 13\n",
      "voen 1633.0580035876765 13\n",
      "voen -1545.6211509595928 14\n",
      "voen 1774.5966791494222 14\n",
      "voen -1175.509996496856 15\n",
      "voen 1226.8960635721676 15\n",
      "voen -1299.1713236171295 16\n",
      "voen 700.759425551083 16\n",
      "voen -1363.6523253095506 18\n",
      "voen 722.045752907745 18\n",
      "voen -635.5815560003313 19\n",
      "voen 780.722404087494 19\n",
      "voen -1445.0522911441922 20\n",
      "voen 756.7294013586838 20\n",
      "voen -1216.0510499369348 21\n",
      "voen 1272.2356704397362 21\n",
      "voen -1010.8657520785717 23\n",
      "voen 1109.6048169111684 23\n",
      "voen -1344.0138775139894 24\n",
      "voen 1508.221602749368 24\n",
      "voen -1503.223035047152 25\n",
      "voen 797.1176275013338 25\n",
      "voen -1064.2178893901716 26\n",
      "voen 983.050669923388 26\n",
      "voen -1410.970211201973 28\n",
      "voen 803.9811435859516 28\n",
      "voen -752.0616855454584 29\n",
      "voen 921.7948060237121 29\n",
      "voen -849.1962190199905 30\n",
      "voen 1602.5690688556285 30\n",
      "voen -766.7164827403427 31\n",
      "voen 1341.8865944180618 31\n",
      "voen -994.9920799511679 33\n",
      "voen 985.9863603784096 33\n",
      "voen -1131.2043066252386 35\n",
      "voen 837.2311436301288 35\n",
      "voen -835.8577684948345 36\n",
      "voen 1158.897599693552 36\n",
      "voen -1052.5863080096524 38\n",
      "voen 1229.5050666201091 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n",
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n",
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n",
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n",
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n",
      "pipo_HCI -55.97212970445327 79\n",
      "pipo_HCI 116.32214024347675 79\n",
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n",
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n"
     ]
    }
   ],
   "source": [
    "EEG_Epochs = AllBCIClass.GetEpoch(EEG_data ,tmin= -2.0, tmax= 6.0, crop = (0,4) ,baseline= (-0.5,0.0), band_pass=(6,32),trial_removal_th = 100)\n",
    "calibrate_size = calibrate_size / EEG_Epochs[target_data_0]['Raw_Epoch'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Apply Data Alignment on RAW data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: After performing EA for all subjects, they share the same mean covariance matrix, i.e., the distances between the mean covariance matrices of different subjects are minimized (they become zero), and hence data distributions from different subjects become more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignmentMethod = \"EA\"\n",
    "\n",
    "if alignmentMethod == \"LA\":\n",
    "    AllBCIClass.ComputeLA(EEG_Epochs, target_subject= target_data_0, calibrate_size=calibrate_size)\n",
    "    if calibrate_size != 0:\n",
    "        target_data = target_data_0 + \"_test\"\n",
    "    count = 0\n",
    "\n",
    "    for index in range(len(EEG_Epochs[target_data]['KMediod_label'])):\n",
    "        if EEG_Epochs[target_data]['label'][index] == EEG_Epochs[target_data]['KMediod_label'][index]:\n",
    "            count += 1\n",
    "    print(count/len(EEG_Epochs[target_data]['KMediod_label']) * 100)\n",
    "    \n",
    "else:\n",
    "    AllBCIClass.ComputeEA(EEG_Epochs, target_subject= target_data_0, calibrate_size=calibrate_size)\n",
    "    if calibrate_size != 0:\n",
    "        target_data = target_data_0 + \"_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kawin_test'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 : CSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "CSP2D_Epoch = AllBCIClass.computeCSPFeatures(EEG_Epochs, target_subject = target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 : T-SNE Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AllBCIClass.TSNE_Plot(CSP2D_Epoch, target_subject= target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Selection (Don't finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "# from mne.decoding import CSP\n",
    "# from sklearn.svm import SVC\n",
    "# from scipy import signal\n",
    "\n",
    "# def butter_bandpass(lowcut,highcut,fs,order):\n",
    "#     nyq = 0.5*fs\n",
    "#     low = lowcut/nyq\n",
    "#     high = highcut/nyq\n",
    "#     b,a = signal.butter(order,[low,high],'bandpass')\n",
    "#     return b,a\n",
    "\n",
    "# def butter_bandpass_filter(data,lowcut = 6,highcut = 30, order = 4):\n",
    "#     b,a = butter_bandpass(lowcut,highcut,128,order)\n",
    "#     y = signal.filtfilt(b,a,data,axis=2)\n",
    "#     return y\n",
    "\n",
    "# def discriminative_frequency_band_selection(data, true_label, label_1, label_2, x_test, y_test):\n",
    "#     Bh, Bl = 32, 6  # Initial upper and lower frequency limits\n",
    "#     A1, A2 = 0, 0  # Initial accuracy values\n",
    "\n",
    "#     # Step 1: Finding Bh\n",
    "#     while A1 >= A2:\n",
    "#         Bh -= 2\n",
    "#         A2 = A1        \n",
    "#         if(Bl >= Bh):\n",
    "#             break\n",
    "#         A1 = train_classifier(Bh, Bl, data, label_1, label_2, true_label, x_test, y_test)  # Design filter and train classifier using CSP\n",
    "\n",
    "#     Bh += 1  # Adjust Bh back\n",
    "#     A1 = train_classifier(Bh, Bl, data, label_1, label_2, true_label, x_test, y_test)\n",
    "\n",
    "#     if A1 >= A2:\n",
    "#         A2 = A1\n",
    "#     else:\n",
    "#         Bh = Bh + 1 \n",
    "#         A1 = A2\n",
    "\n",
    "#     # Step 2: Finding Bl\n",
    "#     while A1 >= A2:\n",
    "#         Bl += 2\n",
    "#         A2 = A1\n",
    "#         if(Bl >= Bh):\n",
    "#             break\n",
    "#         A1 = train_classifier(Bh, Bl, data, label_1, label_2, true_label, x_test, y_test)  # Design filter and train classifier using CSP\n",
    "\n",
    "#     Bl -= 1  # Adjust Bl back\n",
    "#     A1 = train_classifier(Bh, Bl, data, label_1, label_2, true_label, x_test, y_test)\n",
    "\n",
    "#     if A1 >= A2:\n",
    "#         A2 = A1\n",
    "#     else:\n",
    "#         Bh = Bh + 1 \n",
    "#         A1 = A2\n",
    "\n",
    "#     return Bh, Bl, A1\n",
    "\n",
    "\n",
    "# def train_classifier(Bh, Bl, data, label_1, label_2, true_label, x_test, y_test):\n",
    "\n",
    "#     init_data = data[np.where((true_label == label_1) | (true_label == label_2))]\n",
    "#     init_label = true_label[np.where((true_label == label_1) | (true_label == label_2))]\n",
    "\n",
    "#     print(init_data.shape, init_label.shape)\n",
    "\n",
    "#     x_test = x_test[np.where((y_test == label_1) | (y_test == label_2))]\n",
    "#     y_test = y_test[np.where((y_test == label_1) | (y_test == label_2))]\n",
    "\n",
    "#     filtered_data = butter_bandpass_filter(init_data, lowcut= Bl, highcut= Bh)\n",
    "#     x_test = butter_bandpass_filter(x_test, lowcut= Bl, highcut= Bh)\n",
    "\n",
    "#     csp = CSP(n_components = 5, reg=None, log=None, rank= 'info')\n",
    "#     csp.fit(filtered_data, init_label)\n",
    "\n",
    "#     x_train = csp.transform(filtered_data)\n",
    "#     x_test = csp.transform(x_test)\n",
    "\n",
    "#     # Initialize SVM with a linear kernel\n",
    "#     clf = SVC()\n",
    "\n",
    "#     param_grid = {\n",
    "#         'C':[1],\n",
    "#         'kernel': ['rbf'],  # Example kernels\n",
    "#     }\n",
    "\n",
    "#     grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "#     grid_search.fit(x_train, init_label)\n",
    "#     y_pred = grid_search.predict(x_test)\n",
    "\n",
    "#     return accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# def get_frequency_band(EEG_Epochs, target_data, condition = \"noEA\"):\n",
    "\n",
    "#     train_data = None\n",
    "#     train_label = None\n",
    "\n",
    "#     tgt_data = target_data + \"_test\"\n",
    "\n",
    "#     if condition == \"noEA\":\n",
    "#         query = \"Raw_Epoch\"\n",
    "#     else:\n",
    "#         query = \"EA_Epoch\"\n",
    "\n",
    "#     for sub in EEG_Epochs.keys():\n",
    "#         if sub == target_data or sub == tgt_data:\n",
    "#             pass\n",
    "\n",
    "#         else:\n",
    "#             if train_data is None:\n",
    "#                 train_data = EEG_Epochs[sub][query]\n",
    "#             else:\n",
    "#                 train_data = np.concatenate((train_data, EEG_Epochs[sub][query]), axis=0)\n",
    "\n",
    "#             if train_label is None:\n",
    "#                 train_label = EEG_Epochs[sub]['label']\n",
    "#             else:\n",
    "#                 train_label = np.concatenate((train_label, EEG_Epochs[sub]['label']), axis=0)\n",
    "\n",
    "#     indices = [0, 1, 2, 3]\n",
    "#     pairs = []\n",
    "#     band_high = []\n",
    "#     band_low = []\n",
    "#     acc= []\n",
    "#     class_name = ['left', 'right', 'non', 'feet']\n",
    "\n",
    "#     train_data, x_test_temp, train_label, y_test_temp = train_test_split(train_data, train_label, test_size=0.3, random_state = 42, stratify=train_label)\n",
    "\n",
    "#     # Nested loop to generate all pairs without reversing and without self-pairing\n",
    "#     for i in range(len(indices)):\n",
    "#         for j in range(i + 1, len(indices)):\n",
    "#             pairs.append((indices[i], indices[j]))\n",
    "#             Bh, Bl, A1 = discriminative_frequency_band_selection(data=train_data, true_label=train_label, label_1=indices[i], label_2=indices[j], x_test=x_test_temp, y_test=y_test_temp)\n",
    "#             band_high.append(Bh)\n",
    "#             band_low.append(Bl)\n",
    "#             acc.append(A1)\n",
    "\n",
    "#     csp_trained = []\n",
    "\n",
    "#     for i in range(0,len(band_high)):\n",
    "#         filter_x_train = butter_bandpass_filter(train_data ,lowcut= band_low[i], highcut=band_high[i])\n",
    "#         csp = CSP(n_components = 5, reg=None, log=None, rank= 'info')\n",
    "#         csp.fit(filter_x_train, train_label)\n",
    "#         csp_trained.append(csp)\n",
    "\n",
    "#     for sub in EEG_Epochs.keys():\n",
    "#         stack_csp_train = []\n",
    "#         for i in range(0,len(band_high)):\n",
    "#             filtered = butter_bandpass_filter(EEG_Epochs[sub][query] ,lowcut= band_low[i], highcut=band_high[i])\n",
    "#             filtered = csp_trained[i].transform(filtered)\n",
    "#             stack_csp_train.append(filtered)\n",
    "            \n",
    "#         EEG_Epochs[sub][\"Stack_epoch\"] = np.hstack(np.array(stack_csp_train))\n",
    "\n",
    "#     for i in range(len(indices)):\n",
    "#         for j in range(i + 1, len(indices)):\n",
    "#             print(f\"The selected frequency band of {class_name[i]} vs {class_name[j]} is: Bl = {band_low[j-1+i]}, Bh = {band_high[j-1+i]}, Acc = {acc[j-1+i]}\" )\n",
    "\n",
    "#     for sub in EEG_Epochs.keys():\n",
    "#         CSP2D_Epoch[sub]['Stack_epoch'] = EEG_Epochs[sub]['Stack_epoch']\n",
    "#         EEG_Epochs[sub][query] = EEG_Epochs[sub]['Stack_epoch']\n",
    "\n",
    "# get_frequency_band(EEG_Epochs, target_data_0, condition = \"EA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSP+wLTL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.5927988290786743\n",
      "Epoch 20, Loss: 1.5598080158233643\n",
      "Epoch 40, Loss: 1.5476295948028564\n",
      "Epoch 60, Loss: 1.5444977283477783\n",
      "Epoch 80, Loss: 1.5427542924880981\n",
      "Epoch 100, Loss: 1.541137933731079\n",
      "Epoch 120, Loss: 1.53972327709198\n",
      "Epoch 140, Loss: 1.538679838180542\n",
      "Epoch 160, Loss: 1.5380752086639404\n",
      "Epoch 180, Loss: 1.5379467010498047\n",
      "Epoch 200, Loss: 1.538329839706421\n",
      "Epoch 220, Loss: 1.539250135421753\n",
      "Epoch 240, Loss: 1.5407240390777588\n",
      "Epoch 260, Loss: 1.5427632331848145\n",
      "Epoch 280, Loss: 1.545376181602478\n",
      "Epoch 300, Loss: 1.5485680103302002\n",
      "Epoch 320, Loss: 1.5523416996002197\n",
      "Epoch 340, Loss: 1.5566978454589844\n",
      "Epoch 360, Loss: 1.561635971069336\n",
      "Epoch 380, Loss: 1.5671546459197998\n",
      "Epoch 400, Loss: 1.573250651359558\n",
      "Epoch 420, Loss: 1.5799205303192139\n",
      "Epoch 440, Loss: 1.5871593952178955\n",
      "Epoch 460, Loss: 1.5949627161026\n",
      "Epoch 480, Loss: 1.6033246517181396\n",
      "weights of  pipo :  [array([[-0.98477465, -0.76733655, -0.5602779 ],\n",
      "       [ 0.27725258, -0.2563901 ,  0.49166173],\n",
      "       [-0.04213428,  0.5614334 ,  0.7657149 ],\n",
      "       [-0.08775289, -0.7452479 , -0.82346433],\n",
      "       [ 0.48508725,  0.7418444 , -0.55048823]], dtype=float32), array([-0.09440918,  0.02307383,  0.18650487], dtype=float32)]\n",
      "Lowest loss of  pipo :  1.5379319\n",
      "Epoch 0, Loss: 1.9722955226898193\n",
      "Epoch 20, Loss: 1.8692245483398438\n",
      "Epoch 40, Loss: 1.7793982028961182\n",
      "Epoch 60, Loss: 1.703871488571167\n",
      "Epoch 80, Loss: 1.6446189880371094\n",
      "Epoch 100, Loss: 1.6009143590927124\n",
      "Epoch 120, Loss: 1.570159673690796\n",
      "Epoch 140, Loss: 1.5488173961639404\n",
      "Epoch 160, Loss: 1.5334606170654297\n",
      "Epoch 180, Loss: 1.5214823484420776\n",
      "Epoch 200, Loss: 1.5112481117248535\n",
      "Epoch 220, Loss: 1.5018852949142456\n",
      "Epoch 240, Loss: 1.4929927587509155\n",
      "Epoch 260, Loss: 1.4844110012054443\n",
      "Epoch 280, Loss: 1.4760879278182983\n",
      "Epoch 300, Loss: 1.4680131673812866\n",
      "Epoch 320, Loss: 1.460188388824463\n",
      "Epoch 340, Loss: 1.45261812210083\n",
      "Epoch 360, Loss: 1.4453065395355225\n",
      "Epoch 380, Loss: 1.4382562637329102\n",
      "Epoch 400, Loss: 1.4314693212509155\n",
      "Epoch 420, Loss: 1.4249459505081177\n",
      "Epoch 440, Loss: 1.4186855554580688\n",
      "Epoch 460, Loss: 1.4126871824264526\n",
      "Epoch 480, Loss: 1.4069488048553467\n",
      "weights of  NutF8 :  [array([[ 0.14088751, -0.26282167, -0.0352628 ],\n",
      "       [ 0.46985886, -0.3029962 ,  0.55907494],\n",
      "       [-0.5248726 , -0.1002894 , -0.59452856],\n",
      "       [-0.93255764,  0.16565308, -0.27894416],\n",
      "       [ 0.14577651,  0.39987686, -0.45087606]], dtype=float32), array([ 0.14991002,  0.0622874 , -0.15040658], dtype=float32)]\n",
      "Lowest loss of  NutF8 :  1.4017357\n",
      "Epoch 0, Loss: 1.433014154434204\n",
      "Epoch 20, Loss: 1.4230924844741821\n",
      "Epoch 40, Loss: 1.4187639951705933\n",
      "Epoch 60, Loss: 1.4142911434173584\n",
      "Epoch 80, Loss: 1.4104455709457397\n",
      "Epoch 100, Loss: 1.4070537090301514\n",
      "Epoch 120, Loss: 1.4041777849197388\n",
      "Epoch 140, Loss: 1.4018622636795044\n",
      "Epoch 160, Loss: 1.4001235961914062\n",
      "Epoch 180, Loss: 1.3989832401275635\n",
      "Epoch 200, Loss: 1.398453950881958\n",
      "Epoch 220, Loss: 1.398545503616333\n",
      "Epoch 240, Loss: 1.3992654085159302\n",
      "Epoch 260, Loss: 1.4006184339523315\n",
      "Epoch 280, Loss: 1.4026074409484863\n",
      "Epoch 300, Loss: 1.4052340984344482\n",
      "Epoch 320, Loss: 1.4084982872009277\n",
      "Epoch 340, Loss: 1.4123990535736084\n",
      "Epoch 360, Loss: 1.4169337749481201\n",
      "Epoch 380, Loss: 1.422100305557251\n",
      "Epoch 400, Loss: 1.4278942346572876\n",
      "Epoch 420, Loss: 1.4343116283416748\n",
      "Epoch 440, Loss: 1.4413470029830933\n",
      "Epoch 460, Loss: 1.4489953517913818\n",
      "Epoch 480, Loss: 1.4572502374649048\n",
      "weights of  AJpang :  [array([[ 0.55739987,  0.73211324,  0.49967924],\n",
      "       [-0.35807455,  0.2681979 , -0.5932101 ],\n",
      "       [ 0.14810944, -0.64109755,  0.32139775],\n",
      "       [-0.63308436,  0.42589852,  0.10360353],\n",
      "       [ 0.48981804, -0.4749633 ,  0.3108812 ]], dtype=float32), array([-0.10950572, -0.09823176,  0.22435518], dtype=float32)]\n",
      "Lowest loss of  AJpang :  1.3984149\n",
      "Epoch 0, Loss: 3.196692943572998\n",
      "Epoch 20, Loss: 2.9766292572021484\n",
      "Epoch 40, Loss: 2.7642111778259277\n",
      "Epoch 60, Loss: 2.5622916221618652\n",
      "Epoch 80, Loss: 2.3737919330596924\n",
      "Epoch 100, Loss: 2.201674222946167\n",
      "Epoch 120, Loss: 2.048666000366211\n",
      "Epoch 140, Loss: 1.916790246963501\n",
      "Epoch 160, Loss: 1.8068974018096924\n",
      "Epoch 180, Loss: 1.7184159755706787\n",
      "Epoch 200, Loss: 1.6494637727737427\n",
      "Epoch 220, Loss: 1.597273826599121\n",
      "Epoch 240, Loss: 1.5587366819381714\n",
      "Epoch 260, Loss: 1.5308606624603271\n",
      "Epoch 280, Loss: 1.5110418796539307\n",
      "Epoch 300, Loss: 1.4971617460250854\n",
      "Epoch 320, Loss: 1.4875690937042236\n",
      "Epoch 340, Loss: 1.481016755104065\n",
      "Epoch 360, Loss: 1.47658371925354\n",
      "Epoch 380, Loss: 1.4736026525497437\n",
      "Epoch 400, Loss: 1.4715996980667114\n",
      "Epoch 420, Loss: 1.4702450037002563\n",
      "Epoch 440, Loss: 1.4693125486373901\n",
      "Epoch 460, Loss: 1.4686511754989624\n",
      "Epoch 480, Loss: 1.4681607484817505\n",
      "weights of  Aoomim :  [array([[-0.6511152 ,  0.49048072, -0.10415485],\n",
      "       [ 0.34779778, -0.37405652, -0.44941354],\n",
      "       [ 0.5321376 , -0.9213226 , -0.6168324 ],\n",
      "       [-0.38375536,  0.51311624,  0.37350535],\n",
      "       [-0.06742098,  0.44653237,  0.6306114 ]], dtype=float32), array([ 0.20499463,  0.27680707, -0.24423383], dtype=float32)]\n",
      "Lowest loss of  Aoomim :  1.467794\n",
      "Epoch 0, Loss: 2.337000846862793\n",
      "Epoch 20, Loss: 2.1559081077575684\n",
      "Epoch 40, Loss: 1.9973690509796143\n",
      "Epoch 60, Loss: 1.8661106824874878\n",
      "Epoch 80, Loss: 1.7631107568740845\n",
      "Epoch 100, Loss: 1.686218023300171\n",
      "Epoch 120, Loss: 1.6312425136566162\n",
      "Epoch 140, Loss: 1.593242883682251\n",
      "Epoch 160, Loss: 1.567582607269287\n",
      "Epoch 180, Loss: 1.550475001335144\n",
      "Epoch 200, Loss: 1.5390808582305908\n",
      "Epoch 220, Loss: 1.5313787460327148\n",
      "Epoch 240, Loss: 1.52597975730896\n",
      "Epoch 260, Loss: 1.5219612121582031\n",
      "Epoch 280, Loss: 1.5187312364578247\n",
      "Epoch 300, Loss: 1.5159251689910889\n",
      "Epoch 320, Loss: 1.5133283138275146\n",
      "Epoch 340, Loss: 1.5108193159103394\n",
      "Epoch 360, Loss: 1.5083339214324951\n",
      "Epoch 380, Loss: 1.5058404207229614\n",
      "Epoch 400, Loss: 1.5033254623413086\n",
      "Epoch 420, Loss: 1.5007842779159546\n",
      "Epoch 440, Loss: 1.4982181787490845\n",
      "Epoch 460, Loss: 1.4956295490264893\n",
      "Epoch 480, Loss: 1.4930224418640137\n",
      "weights of  voen :  [array([[-4.1425601e-01, -8.8499361e-01, -3.1003717e-01],\n",
      "       [ 5.8566546e-01,  5.4273324e-04, -5.3919566e-01],\n",
      "       [ 9.0553559e-02,  3.9535347e-01,  4.5624718e-02],\n",
      "       [ 5.7232267e-01,  2.7519897e-01,  6.7896885e-01],\n",
      "       [-9.9736083e-01,  2.1886949e-01,  8.4473446e-02]], dtype=float32), array([ 0.17225437,  0.09224115, -0.13822185], dtype=float32)]\n",
      "Lowest loss of  voen :  1.4905319\n",
      "Epoch 0, Loss: 1.7116210460662842\n",
      "Epoch 20, Loss: 1.6352431774139404\n",
      "Epoch 40, Loss: 1.5739009380340576\n",
      "Epoch 60, Loss: 1.529989242553711\n",
      "Epoch 80, Loss: 1.5021445751190186\n",
      "Epoch 100, Loss: 1.4865261316299438\n",
      "Epoch 120, Loss: 1.4788541793823242\n",
      "Epoch 140, Loss: 1.4756193161010742\n",
      "Epoch 160, Loss: 1.474595546722412\n",
      "Epoch 180, Loss: 1.4746359586715698\n",
      "Epoch 200, Loss: 1.4752458333969116\n",
      "Epoch 220, Loss: 1.4762437343597412\n",
      "Epoch 240, Loss: 1.477570652961731\n",
      "Epoch 260, Loss: 1.4792088270187378\n",
      "Epoch 280, Loss: 1.4811521768569946\n",
      "Epoch 300, Loss: 1.4833979606628418\n",
      "Epoch 320, Loss: 1.4859440326690674\n",
      "Epoch 340, Loss: 1.4887880086898804\n",
      "Epoch 360, Loss: 1.4919283390045166\n",
      "Epoch 380, Loss: 1.4953629970550537\n",
      "Epoch 400, Loss: 1.4990899562835693\n",
      "Epoch 420, Loss: 1.5031076669692993\n",
      "Epoch 440, Loss: 1.5074141025543213\n",
      "Epoch 460, Loss: 1.5120068788528442\n",
      "Epoch 480, Loss: 1.5168845653533936\n",
      "weights of  pipo_HCI :  [array([[-0.11470736, -0.44575724, -0.47198117],\n",
      "       [ 0.00582589,  0.19532269, -0.5789312 ],\n",
      "       [ 0.57990026,  0.6554871 ,  0.96448183],\n",
      "       [-0.32219672, -0.5338947 ,  0.8494255 ],\n",
      "       [-0.05539512,  0.5280608 , -0.31875154]], dtype=float32), array([-0.14437006,  0.10193501, -0.09855397], dtype=float32)]\n",
      "Lowest loss of  pipo_HCI :  1.474522\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Custom loss function\n",
    "class CustomLossLL1(tf.keras.losses.Loss):\n",
    "    def __init__(self, lambda_t, model):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        ws = self.get_weights_from_model()\n",
    "        reg_term = self.regularization_term(ws)\n",
    "        return ce_loss + self.lambda_t * reg_term\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        return model_weights[0]\n",
    "\n",
    "    def regularization_term(self, ws):\n",
    "        reg_term = tf.pow(tf.norm(ws, ord='euclidean'),2)\n",
    "        return reg_term\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True) # Perform a forward pass and compute the predictions\n",
    "        loss = custom_loss(y, y_pred) # Compute the custom loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_weight_LL(X_train, y_train, lambd, num_tier=10, learning_rate = 0.01):\n",
    "    n_classes = np.unique(y_train).size\n",
    "    _, sequential_indices = np.unique(y_train, return_inverse=True)\n",
    "    y_one_hot = tf.keras.utils.to_categorical(sequential_indices, num_classes=n_classes)\n",
    "\n",
    "    lambda_t = lambd  # Regularization parameter\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_classes, input_shape=(X_train.shape[1],), activation='softmax')\n",
    "\n",
    "        # Dense(5, input_shape=(X_train.shape[1],), activation='relu'),  # Hidden layer with 5 neurons\n",
    "        # Dense(n_classes, activation='softmax')  # Output layer with 'n_classes' neurons\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate)\n",
    "    custom_loss = CustomLossLL1(lambda_t, model)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = num_tier\n",
    "    lowest_loss = float('inf')\n",
    "    best_weights = None\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X_train, y_one_hot, custom_loss)\n",
    "        loss_value = loss.numpy()\n",
    "        if loss_value < lowest_loss:\n",
    "            lowest_loss = loss_value\n",
    "            best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    return best_weights[0], lowest_loss\n",
    "\n",
    "def build_clf_params(data, target_subjects ,condition):\n",
    "\n",
    "    for sub in data.keys():\n",
    "        if (sub  != target_subjects) and  (sub != target_data_0): #Don't apply weight to target subject\n",
    "            # Where the tranining data is stored\n",
    "            if condition == \"noEA\":\n",
    "                X = data[sub]['Raw_csp']\n",
    "                y = data[sub]['Raw_csp_label']\n",
    "                store_ws = 'ws_Raw'\n",
    "\n",
    "            elif condition == \"stack\":\n",
    "                X = data[sub]['Stack_epoch']\n",
    "                y = data[sub]['Raw_csp_label']\n",
    "                store_ws = 'ws_stack'\n",
    "\n",
    "            else:\n",
    "                X = data[sub]['EA_csp']\n",
    "                y = data[sub]['EA_csp_label']\n",
    "                store_ws = 'ws_EA'\n",
    "\n",
    "            weights, loss = train_weight_LL(X_train=X, y_train=y, lambd= 0.1, num_tier=500, learning_rate= 0.001)\n",
    "            print(\"weights of \", str(sub), \": \", weights)\n",
    "            print(\"Lowest loss of \", str(sub), \": \", loss)\n",
    "            data[sub][store_ws] = weights\n",
    "\n",
    "build_clf_params(CSP2D_Epoch, target_subjects= target_data ,condition = condition_wLTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "Weights (array): These are the parameters that the model learns during training. They represent the strength of the connections between neurons. The shape of the weights array is (input_dim, output_dim).\n",
    "\n",
    "Biases (array): These are additional parameters that are added to the outputs of each layer. The shape of the biases array is (output_dim,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Raw_csp', 'Raw_csp_label', 'EA_csp', 'EA_csp_label', 'ws_EA'])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSP2D_Epoch['pipo'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the kl divergence\n",
    "def KL_div(P, Q):\n",
    "    # First convert to np array\n",
    "    P = np.array(P)\n",
    "    Q = np.array(Q)\n",
    "\n",
    "    # Then compute their means, datain shape of samples x feat\n",
    "    mu_P = np.mean(P, axis=0)\n",
    "    mu_Q = np.mean(Q, axis=0)    \n",
    "    \n",
    "    # Compute their covariance\n",
    "    sigma_P = np.cov(P, rowvar=False)\n",
    "    sigma_Q = np.cov(Q, rowvar=False)  \n",
    "\n",
    "    # # Check the condition number\n",
    "    # condition_number = np.linalg.cond(sigma_Q)\n",
    "\n",
    "    # # If the condition number is too high, use a regularized version\n",
    "    # if condition_number > 1e10:  # Threshold to decide if it's ill-conditioned\n",
    "    #     print(\"add small epsilon\")\n",
    "    #     epsilon = 1e-5  # Small regularization value\n",
    "    #     sigma_Q += epsilon * np.eye(sigma_Q.shape[0])\n",
    "    #     sigma_P += epsilon * np.eye(sigma_P.shape[0])\n",
    "\n",
    "    diff = mu_Q - mu_P\n",
    "\n",
    "    inv_sigma_Q = np.linalg.inv(sigma_Q)\n",
    "    term1 = np.dot(np.dot(diff.T, inv_sigma_Q), diff)\n",
    "\n",
    "    # Calculate the trace term trace(Sigma_Q^{-1} * Sigma_P)\n",
    "    term2 = np.trace(np.dot(inv_sigma_Q, sigma_P))\n",
    "    \n",
    "    # Calculate the determinant term ln(det(Sigma_P) / det(Sigma_Q))\n",
    "    det_sigma0 = np.linalg.det(sigma_P)\n",
    "    det_sigma1 = np.linalg.det(sigma_Q)\n",
    "    epsilon = 1e-5 \n",
    "    term3 = np.log((det_sigma0+epsilon) / (det_sigma1+epsilon))\n",
    "\n",
    "    \n",
    "    # Dimensionality of the data\n",
    "    K = mu_P.shape[0]\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_div = 0.5 * (term1 + term2 - term3 - K)\n",
    "    \n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipo\n",
      "NutF8\n",
      "AJpang\n",
      "Aoomim\n",
      "voen\n",
      "pipo_HCI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_17496\\2450190698.py:12: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  sigma_P = np.cov(P, rowvar=False)\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2705: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2705: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_17496\\2450190698.py:13: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  sigma_Q = np.cov(Q, rowvar=False)\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2139: RuntimeWarning: invalid value encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n"
     ]
    }
   ],
   "source": [
    "# Compute kl divergence of target subject to each source subject\n",
    "def compute_all_kl_div(data, target_subjects , condition):\n",
    "    '''\n",
    "    Parameter:\n",
    "    data, is the whole data containing target and source data\n",
    "    '''\n",
    "    kl_div_score = []\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        target_data = 'Raw_csp'\n",
    "        label_name = 'Raw_csp_label'\n",
    "\n",
    "    elif condition == \"stack\":\n",
    "        target_data = 'Stack_epoch'\n",
    "        label_name = 'Raw_csp_label'\n",
    "\n",
    "    else:\n",
    "        target_data = 'EA_csp'\n",
    "        label_name = 'EA_csp_label'\n",
    "        \n",
    "    # cal P from target data\n",
    "    label_tgt =  data[target_subjects][label_name]\n",
    "    P_left =  data[target_subjects][target_data][np.where(label_tgt == 0)]\n",
    "    P_right = data[target_subjects][target_data][np.where(label_tgt == 1)]\n",
    "    P_non = data[target_subjects][target_data][np.where(label_tgt == 2)]\n",
    "    P_feet = data[target_subjects][target_data][np.where(label_tgt == 3)]\n",
    "\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "\n",
    "    #cal Q from each source subject\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            print(sub)\n",
    "            label_src =  data[sub][label_name]\n",
    "            Q_left =  data[sub][target_data][np.where(label_src == 0)]\n",
    "            Q_right = data[sub][target_data][np.where(label_src == 1)]\n",
    "            Q_non = data[sub][target_data][np.where(label_src == 2)]\n",
    "            Q_feet = data[sub][target_data][np.where(label_src == 3)]\n",
    "\n",
    "            kl_left = KL_div(P_left, Q_left)\n",
    "            kl_right = KL_div(P_right, Q_right)\n",
    "            kl_non = KL_div(P_non, Q_non)\n",
    "            kl_feet = KL_div(P_feet, Q_feet)\n",
    "\n",
    "            kl_div_temp = [kl_left, kl_right, kl_non, kl_feet]\n",
    "\n",
    "            kl_div_score.append(kl_div_temp)\n",
    "\n",
    "    data[target_subjects]['kl_div'] = kl_div_score\n",
    "\n",
    "\n",
    "compute_all_kl_div(CSP2D_Epoch, target_subjects=target_data_0 ,condition = condition_wLTL) #target_sub for cal KL is calibrate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.27340679, 4.63490972,        nan, 3.92444181],\n",
       "       [3.29621271, 3.52234205,        nan, 5.27599493],\n",
       "       [5.94861201, 4.41529228,        nan, 5.12798001],\n",
       "       [4.30850458, 4.15402999,        nan, 2.59113261],\n",
       "       [6.60443883, 4.0799168 ,        nan, 6.18598141],\n",
       "       [3.80546706, 1.23233773,        nan, 1.78622043]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(CSP2D_Epoch[target_data_0]['kl_div'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0012930102333070183, 0.008470057869797635, 0.0007985612325167073, 0.002901706254530319, 0.0005255693228678851, 0.004767848875348591]\n",
      "[0.0021666931130809616, 0.006495679271546554, 0.0026310119603851477, 0.003357992110033306, 0.0036087117488219603, 0.43345104579426147]\n",
      "[[0.06893571 0.00479663 0.03292626]\n",
      " [0.45157376 0.01438016 0.01007971]\n",
      " [0.0425746  0.00582455 0.01129482]\n",
      " [0.15470194 0.00743394 0.17324978]\n",
      " [0.02802027 0.00798898 0.00533379]\n",
      " [0.25419371 0.95957574 0.76711564]]\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_weights(data, target_subjects):\n",
    "    kl = data[target_subjects]['kl_div']\n",
    "    KL_inv_left = []\n",
    "    KL_inv_right = []\n",
    "    KL_inv_non = []\n",
    "    KL_inv_feet = []\n",
    "\n",
    "    alpha_s = []\n",
    "    eps = 0.0001\n",
    "    \n",
    "    #equation (9)\n",
    "    for val in kl:\n",
    "        if val != 0: \n",
    "            KL_inv_left.append(1/((val[0] + eps)**4))\n",
    "            KL_inv_right.append(1/((val[1] + eps)**4))\n",
    "            KL_inv_non.append(1/((val[2] + eps)**4))\n",
    "            KL_inv_feet.append(1/((val[3] + eps)**4))\n",
    "\n",
    "    print(KL_inv_left)\n",
    "    print(KL_inv_right)\n",
    "    \n",
    "    for i in range(0,len(KL_inv_left)):\n",
    "        temp = [KL_inv_left[i]/sum(KL_inv_left), KL_inv_right[i]/sum(KL_inv_right), KL_inv_non[i]/sum(KL_inv_non), KL_inv_feet[i]/sum(KL_inv_feet)]\n",
    "        alpha_s.append(temp)\n",
    "\n",
    "    alpha_s = np.array(alpha_s)\n",
    "    print(np.array(alpha_s[:, ~np.isnan(alpha_s).any(axis=0)]))\n",
    "                \n",
    "    data[target_subjects]['alpha_s'] = alpha_s[:, ~np.isnan(alpha_s).any(axis=0)]\n",
    "\n",
    "compute_similarity_weights(CSP2D_Epoch, target_subjects=target_data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1220282  -0.43435765 -0.39492213]\n",
      " [ 0.28774006  0.18062573 -0.50972024]\n",
      " [-0.0013505   0.62281567  0.65609596]\n",
      " [-0.57935263 -0.50501128  0.69118368]\n",
      " [ 0.06766492  0.51832457 -0.15397431]]\n",
      "[[ 1.77329434  0.43882563 -2.68072431  0.03207642 -0.81602367]\n",
      " [ 0.43882563  1.7885217  -1.13702719 -2.92396239  0.97422508]\n",
      " [-2.68072431 -1.13702719  4.31341014  0.79393324  0.99396371]\n",
      " [ 0.03207642 -2.92396239  0.79393324  5.15432161 -2.0680427 ]\n",
      " [-0.81602367  0.97422508  0.99396371 -2.0680427   1.52367712]]\n"
     ]
    }
   ],
   "source": [
    "def compute_ETL_and_mu_ws(data, target_subjects, condition):\n",
    "\n",
    "    mu_ws = 0\n",
    "    temp_ws = 0\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        ws_name = 'ws_Raw'\n",
    "    elif condition == \"stack\":\n",
    "        ws_name = 'ws_stack'\n",
    "    else:\n",
    "        ws_name = 'ws_EA'\n",
    "\n",
    "    alpha_s = np.array(data[target_subjects]['alpha_s'])\n",
    "\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "    index_count = 0\n",
    "\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            ws = data[sub][ws_name][0]\n",
    "            mu_ws += ws * alpha_s[index_count]\n",
    "            index_count += 1\n",
    "\n",
    "    print(np.array(mu_ws))\n",
    "\n",
    "    index_count = 0\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            ws = data[sub][ws_name][0]\n",
    "            ws_min_mu = np.dot(((ws * alpha_s[index_count]) - mu_ws), np.transpose((ws * alpha_s[index_count]) - mu_ws))\n",
    "            temp_ws += ws_min_mu #equation (11)\n",
    "            index_count += 1\n",
    "\n",
    "    print(np.array(temp_ws))\n",
    "    \n",
    "    den = temp_ws\n",
    "    nom = np.trace(temp_ws) #Return the sum along diagonals of the array.\n",
    "    Sigma_TL = den/nom\n",
    "\n",
    "    data[target_subjects]['Sigma_TL'] = Sigma_TL\n",
    "    data[target_subjects]['mu_ws'] = mu_ws\n",
    "\n",
    "compute_ETL_and_mu_ws(CSP2D_Epoch, target_subjects = target_data_0, condition=condition_wLTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(CSP2D_Epoch[target_data_0]['Sigma_TL']).shape)\n",
    "print(np.array(CSP2D_Epoch[target_data_0]['mu_ws']).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ACC with target subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: -2.328381061553955\n",
      "Epoch 20, Loss: -4.695666790008545\n",
      "Epoch 40, Loss: -5.258418083190918\n",
      "Epoch 60, Loss: -5.038346290588379\n",
      "Epoch 80, Loss: -4.950857162475586\n",
      "Epoch 100, Loss: -4.946501731872559\n",
      "Epoch 120, Loss: -4.899456977844238\n",
      "Epoch 140, Loss: -4.866274833679199\n",
      "Epoch 160, Loss: -4.838717937469482\n",
      "Epoch 180, Loss: -4.81389856338501\n",
      "Epoch 200, Loss: -4.793702602386475\n",
      "Epoch 220, Loss: -4.775956153869629\n",
      "Epoch 240, Loss: -4.7598700523376465\n",
      "Epoch 260, Loss: -4.743887901306152\n",
      "Epoch 280, Loss: -4.726517677307129\n",
      "Epoch 300, Loss: -4.706107139587402\n",
      "Epoch 320, Loss: -4.681004047393799\n",
      "Epoch 340, Loss: -4.649541854858398\n",
      "Epoch 360, Loss: -4.610072612762451\n",
      "Epoch 380, Loss: -4.561047077178955\n",
      "Epoch 400, Loss: -4.50097131729126\n",
      "Epoch 420, Loss: -4.42847204208374\n",
      "Epoch 440, Loss: -4.34230899810791\n",
      "Epoch 460, Loss: -4.241371154785156\n",
      "Epoch 480, Loss: -4.1247124671936035\n",
      "Epoch 500, Loss: -3.991525173187256\n",
      "Epoch 520, Loss: -3.841184139251709\n",
      "Epoch 540, Loss: -3.673224449157715\n",
      "Epoch 560, Loss: -3.4873361587524414\n",
      "Epoch 580, Loss: -3.283367156982422\n",
      "Epoch 600, Loss: -3.06134033203125\n",
      "Epoch 620, Loss: -2.821394205093384\n",
      "Epoch 640, Loss: -2.5638058185577393\n",
      "Epoch 660, Loss: -2.2889931201934814\n",
      "Epoch 680, Loss: -1.9974558353424072\n",
      "Epoch 700, Loss: -1.6898179054260254\n",
      "Epoch 720, Loss: -1.366783857345581\n",
      "Epoch 740, Loss: -1.0291255712509155\n",
      "Epoch 760, Loss: -0.6776496767997742\n",
      "Epoch 780, Loss: -0.3132591247558594\n",
      "Epoch 800, Loss: 0.06314150243997574\n",
      "Epoch 820, Loss: 0.45062485337257385\n",
      "Epoch 840, Loss: 0.848250687122345\n",
      "Epoch 860, Loss: 1.2550405263900757\n",
      "Epoch 880, Loss: 1.6700605154037476\n",
      "Epoch 900, Loss: 2.0923783779144287\n",
      "Epoch 920, Loss: 2.5210251808166504\n",
      "Epoch 940, Loss: 2.955190420150757\n",
      "Epoch 960, Loss: 3.3939406871795654\n",
      "Epoch 980, Loss: 3.836458921432495\n",
      "Epoch 1000, Loss: 4.281944751739502\n",
      "Epoch 1020, Loss: 4.729645252227783\n",
      "Epoch 1040, Loss: 5.1788249015808105\n",
      "Epoch 1060, Loss: 5.6287922859191895\n",
      "Epoch 1080, Loss: 6.07891845703125\n",
      "Epoch 1100, Loss: 6.528589725494385\n",
      "Epoch 1120, Loss: 6.977254390716553\n",
      "Epoch 1140, Loss: 7.42439603805542\n",
      "Epoch 1160, Loss: 7.869512557983398\n",
      "Epoch 1180, Loss: 8.312201499938965\n",
      "Epoch 1200, Loss: 8.752028465270996\n",
      "Epoch 1220, Loss: 9.188607215881348\n",
      "Epoch 1240, Loss: 9.621636390686035\n",
      "Epoch 1260, Loss: 10.050833702087402\n",
      "Epoch 1280, Loss: 10.475844383239746\n",
      "Epoch 1300, Loss: 10.896478652954102\n",
      "Epoch 1320, Loss: 11.312541007995605\n",
      "Epoch 1340, Loss: 11.723846435546875\n",
      "Epoch 1360, Loss: 12.130170822143555\n",
      "Epoch 1380, Loss: 12.53140640258789\n",
      "Epoch 1400, Loss: 12.927390098571777\n",
      "Epoch 1420, Loss: 13.318069458007812\n",
      "Epoch 1440, Loss: 13.70333480834961\n",
      "Epoch 1460, Loss: 14.083086013793945\n",
      "Epoch 1480, Loss: 14.457304000854492\n",
      "Epoch 1500, Loss: 14.82587718963623\n",
      "Epoch 1520, Loss: 15.188828468322754\n",
      "Epoch 1540, Loss: 15.546109199523926\n",
      "Epoch 1560, Loss: 15.897686958312988\n",
      "Epoch 1580, Loss: 16.24358558654785\n",
      "Epoch 1600, Loss: 16.583812713623047\n",
      "Epoch 1620, Loss: 16.91830062866211\n",
      "Epoch 1640, Loss: 17.247087478637695\n",
      "Epoch 1660, Loss: 17.570268630981445\n",
      "Epoch 1680, Loss: 17.887773513793945\n",
      "Epoch 1700, Loss: 18.19960594177246\n",
      "Epoch 1720, Loss: 18.50586700439453\n",
      "Epoch 1740, Loss: 18.806499481201172\n",
      "Epoch 1760, Loss: 19.10158920288086\n",
      "Epoch 1780, Loss: 19.39112663269043\n",
      "Epoch 1800, Loss: 19.675174713134766\n",
      "Epoch 1820, Loss: 19.953807830810547\n",
      "Epoch 1840, Loss: 20.226930618286133\n",
      "Epoch 1860, Loss: 20.4946346282959\n",
      "Epoch 1880, Loss: 20.75699234008789\n",
      "Epoch 1900, Loss: 21.014009475708008\n",
      "Epoch 1920, Loss: 21.26569366455078\n",
      "Epoch 1940, Loss: 21.512113571166992\n",
      "Epoch 1960, Loss: 21.75326156616211\n",
      "Epoch 1980, Loss: 21.989206314086914\n",
      "weights of  Kawin :  [array([[-0.4378225 ,  1.0638207 ,  1.085318  ],\n",
      "       [ 1.238136  , -0.35478836, -0.32726678],\n",
      "       [ 0.16696404,  0.13789745, -0.58112395],\n",
      "       [-0.33688927, -0.54434496,  0.1072007 ],\n",
      "       [-1.5613121 ,  0.41440168, -0.6699368 ]], dtype=float32), array([-1.2634385 ,  0.92871296, -0.64155877], dtype=float32)]\n",
      "loss of  Kawin :  0.005952708\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50        10\n",
      "           1       0.57      0.40      0.47        10\n",
      "           2       0.38      0.50      0.43        10\n",
      "\n",
      "    accuracy                           0.47        30\n",
      "   macro avg       0.49      0.47      0.47        30\n",
      "weighted avg       0.49      0.47      0.47        30\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[5 2 3]\n",
      " [1 4 5]\n",
      " [4 1 5]]\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.45      0.41        20\n",
      "           1       0.29      0.35      0.32        20\n",
      "           2       0.42      0.25      0.31        20\n",
      "\n",
      "    accuracy                           0.35        60\n",
      "   macro avg       0.36      0.35      0.35        60\n",
      "weighted avg       0.36      0.35      0.35        60\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[ 9  7  4]\n",
      " [10  7  3]\n",
      " [ 5 10  5]]\n"
     ]
    }
   ],
   "source": [
    "# Custom loss function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "class CustomLossLL2(tf.keras.losses.Loss):\n",
    "    def __init__(self, lambda_t, model, mu, sigma_TL):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "        self.mu = tf.convert_to_tensor(mu, dtype=tf.float32)\n",
    "        self.sigma_TL = tf.convert_to_tensor(sigma_TL, dtype=tf.float32)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        wt = self.get_weights_from_model()\n",
    "        reg_term = self.regularization_term(wt)\n",
    "\n",
    "        return ce_loss + (self.lambda_t * tf.linalg.matmul(reg_term, wt))\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        return model_weights[0]\n",
    "\n",
    "    def regularization_term(self, wt):\n",
    "        diff = wt - self.mu\n",
    "        reg_term = 0.5 * tf.linalg.matmul(tf.linalg.matmul(tf.linalg.inv(self.sigma_TL), diff), tf.transpose(diff))\n",
    "        reg_term += 0.5 * tf.math.log(tf.linalg.det(self.sigma_TL))\n",
    "        return reg_term\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True) # Perform a forward pass and compute the predictions\n",
    "        loss = custom_loss(y, y_pred) # Compute the custom loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_weight_LL2(X_train, y_train, lambd, mu, sigma_TL, num_tier=10, learning_rate = 0.01):\n",
    "    n_classes = np.unique(y_train).size\n",
    "    _, sequential_indices = np.unique(y_train, return_inverse=True)\n",
    "    y_one_hot = tf.keras.utils.to_categorical(sequential_indices, num_classes=n_classes)\n",
    "\n",
    "    lambda_t = lambd  # Regularization parameter\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_classes, input_shape=(X_train.shape[1],), activation='softmax')\n",
    "\n",
    "        # Dense(5, input_shape=(X_train.shape[1],), activation='relu'),  # Hidden layer with 5 neurons\n",
    "        # Dense(n_classes, activation='softmax')  # Output layer with 'n_classes' neurons \n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate)\n",
    "    custom_loss = CustomLossLL2(lambda_t, model, mu, sigma_TL)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = num_tier\n",
    "    lowest_loss = float('inf')\n",
    "    best_weights = None\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X_train, y_one_hot, custom_loss)\n",
    "        loss_value = loss.numpy()\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        if (abs(loss_value) < lowest_loss):\n",
    "            lowest_loss = abs(loss_value)\n",
    "            best_model = model\n",
    "            best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "    return best_model, best_weights[0], lowest_loss\n",
    "\n",
    "def GetConfusionMatrix(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_prob = model.predict(X_train)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    _, y_train = np.unique(y_train, return_inverse=True)\n",
    "\n",
    "    print(\"Classification TRAIN DATA \\n=======================\")\n",
    "    print(classification_report(y_true= y_train, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true= y_train, y_pred=y_pred))\n",
    "\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    _, y_test = np.unique(y_test, return_inverse=True)\n",
    "\n",
    "    print(\"Classification TEST DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "def tgt_test_wLTL(data, target_subjects ,condition):\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        X = data[target_subjects]['Raw_csp']\n",
    "        y = data[target_subjects]['Raw_csp_label']\n",
    "        X_test = data[tgt_data]['Raw_csp']\n",
    "        y_test = data[tgt_data]['Raw_csp_label']\n",
    "        store_ws = 'wt_Raw'\n",
    "\n",
    "    elif condition == \"stack\":\n",
    "        X = data[target_subjects]['Stack_epoch']\n",
    "        y = data[target_subjects]['Raw_csp_label']\n",
    "        X_test = data[tgt_data]['Stack_epoch']\n",
    "        y_test = data[tgt_data]['Raw_csp_label']\n",
    "        store_ws = 'wt_stack'\n",
    "\n",
    "    else:\n",
    "        X = data[target_subjects]['EA_csp']\n",
    "        y = data[target_subjects]['EA_csp_label']\n",
    "        X_test = data[tgt_data]['EA_csp']\n",
    "        y_test = data[tgt_data]['EA_csp_label']\n",
    "        store_ws = 'wt_EA'\n",
    "\n",
    "    mu = data[target_subjects]['mu_ws']\n",
    "    sigma_TL = data[target_subjects]['Sigma_TL']\n",
    "\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    model, weights, loss = train_weight_LL2(X_train=X_train, y_train=y_train, mu =mu, sigma_TL=sigma_TL,  lambd= 0.1, num_tier=2000, learning_rate= 0.005)\n",
    "    print(\"weights of \", str(target_subjects), \": \", weights)\n",
    "    print(\"loss of \", str(target_subjects), \": \", loss)\n",
    "    data[target_subjects][store_ws] = weights\n",
    "\n",
    "    GetConfusionMatrix(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "tgt_test_wLTL(CSP2D_Epoch, target_subjects= target_data_0 ,condition = condition_wLTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noEA + LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.54      0.65      0.59       202\n",
      "       Right       0.47      0.48      0.48       200\n",
      "        Feet       0.49      0.38      0.43       201\n",
      "\n",
      "    accuracy                           0.50       603\n",
      "   macro avg       0.50      0.50      0.50       603\n",
      "weighted avg       0.50      0.50      0.50       603\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[131  41  30]\n",
      " [ 55  97  48]\n",
      " [ 56  69  76]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.32      0.55      0.41        20\n",
      "       Right       0.36      0.40      0.38        20\n",
      "        Feet       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.32        60\n",
      "   macro avg       0.23      0.32      0.26        60\n",
      "weighted avg       0.23      0.32      0.26        60\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[11  7  2]\n",
      " [10  8  2]\n",
      " [13  7  0]]\n"
     ]
    }
   ],
   "source": [
    "AllBCIClass.classifyCSP_LDA(EEG_Epochs, target_subjects= target_data, calibrate_data= target_data_0, condition = \"noEA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noEA + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Best parameters: {'C': 1, 'kernel': 'rbf'}\n",
      "Best cross-validation score: 0.489\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.56      0.65      0.60       202\n",
      "       Right       0.60      0.53      0.56       200\n",
      "        Feet       0.60      0.58      0.59       201\n",
      "\n",
      "    accuracy                           0.59       603\n",
      "   macro avg       0.59      0.59      0.58       603\n",
      "weighted avg       0.59      0.59      0.58       603\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[131  35  36]\n",
      " [ 52 106  42]\n",
      " [ 50  35 116]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.36      0.65      0.46        20\n",
      "       Right       0.44      0.35      0.39        20\n",
      "        Feet       0.12      0.05      0.07        20\n",
      "\n",
      "    accuracy                           0.35        60\n",
      "   macro avg       0.31      0.35      0.31        60\n",
      "weighted avg       0.31      0.35      0.31        60\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[13  3  4]\n",
      " [10  7  3]\n",
      " [13  6  1]]\n"
     ]
    }
   ],
   "source": [
    "if train_svm == True:\n",
    "    AllBCIClass.classifyCSP_SVM(EEG_Epochs, target_subjects= target_data, calibrate_data= target_data_0,condition = \"noEA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EA+LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.61      0.66      0.63       202\n",
      "       Right       0.60      0.50      0.54       200\n",
      "        Feet       0.57      0.62      0.59       201\n",
      "\n",
      "    accuracy                           0.59       603\n",
      "   macro avg       0.59      0.59      0.59       603\n",
      "weighted avg       0.59      0.59      0.59       603\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[133  29  40]\n",
      " [ 46 100  54]\n",
      " [ 39  38 124]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.44      0.55      0.49        20\n",
      "       Right       0.29      0.20      0.24        20\n",
      "        Feet       0.29      0.30      0.29        20\n",
      "\n",
      "    accuracy                           0.35        60\n",
      "   macro avg       0.34      0.35      0.34        60\n",
      "weighted avg       0.34      0.35      0.34        60\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[11  5  4]\n",
      " [ 5  4 11]\n",
      " [ 9  5  6]]\n"
     ]
    }
   ],
   "source": [
    "AllBCIClass.classifyCSP_LDA(EEG_Epochs, target_subjects= target_data, calibrate_data= target_data_0, condition = \"EA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Best parameters: {'C': 1, 'kernel': 'linear'}\n",
      "Best cross-validation score: 0.599\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.61      0.66      0.64       202\n",
      "       Right       0.61      0.51      0.56       200\n",
      "        Feet       0.57      0.62      0.60       201\n",
      "\n",
      "    accuracy                           0.60       603\n",
      "   macro avg       0.60      0.60      0.60       603\n",
      "weighted avg       0.60      0.60      0.60       603\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[134  28  40]\n",
      " [ 45 102  53]\n",
      " [ 40  36 125]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.46      0.60      0.52        20\n",
      "       Right       0.31      0.20      0.24        20\n",
      "        Feet       0.29      0.30      0.29        20\n",
      "\n",
      "    accuracy                           0.37        60\n",
      "   macro avg       0.35      0.37      0.35        60\n",
      "weighted avg       0.35      0.37      0.35        60\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[12  4  4]\n",
      " [ 5  4 11]\n",
      " [ 9  5  6]]\n"
     ]
    }
   ],
   "source": [
    "if train_svm == True:\n",
    "    AllBCIClass.classifyCSP_SVM(EEG_Epochs, target_subjects= target_data, calibrate_data= target_data_0,condition = \"EA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
