{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCI competition 2a\n",
    "\n",
    "C:\\Users\\pipo_\\Downloads\\desc_2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from mne.datasets import eegbci\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from mne.channels import make_standard_montage\n",
    "from scipy import signal\n",
    "from scipy.linalg import sqrtm, inv \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.utils import shuffle\n",
    "from mne.decoding import CSP\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit,StratifiedKFold ,cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pyriemann.utils.distance import distance_riemann\n",
    "from scipy.linalg import logm, expm\n",
    "\n",
    "\n",
    "target_data_0 = \"A01\"\n",
    "calibrate_size = 60\n",
    "alignmentMethod = \"EA\"\n",
    "\n",
    "condition_wLTL= \"EA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRawEDF(target_subjects=\"pipo\"):\n",
    "    EEG_data = {}\n",
    "\n",
    "    if target_subjects == \"all\":\n",
    "        target_subjects = [\"A01\",\"A02\",\"A03\",\"A04\",\"A05\",\"A06\",\"A07\",\"A08\",\"A09\"]\n",
    "\n",
    "    path = \"D:\\\\BCI_Competition_2a\\\\Training\\\\\"\n",
    "    list_dir = listdir(path)\n",
    "\n",
    "    for i in range (0,len(target_subjects)):\n",
    "\n",
    "        raw_gdf = mne.io.read_raw_gdf(path+list_dir[i],preload = False, verbose = False)\n",
    "\n",
    "        EEG_data[target_subjects[i]] = {\"Raw_data\": raw_gdf.copy()}\n",
    "\n",
    "    print(f\"Successful to create Data of {target_subjects}\")\n",
    "\n",
    "    return EEG_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful to create Data of ['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07', 'A08', 'A09']\n"
     ]
    }
   ],
   "source": [
    "Raw_data = GetRawEDF(target_subjects=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut,highcut,fs,order):\n",
    "    nyq = 0.5*fs\n",
    "    low = lowcut/nyq\n",
    "    high = highcut/nyq\n",
    "    b,a = signal.butter(order,[low,high],'bandpass')\n",
    "    return b,a\n",
    "\n",
    "def butter_bandpass_filter(data,lowcut = 6,highcut = 30, order = 4):\n",
    "    b,a = butter_bandpass(lowcut,highcut,250,order)\n",
    "    y = signal.filtfilt(b,a,data,axis=2)\n",
    "    return y\n",
    "\n",
    "def GetEpoch(EEG_data, tmin=-2.0, tmax=6.0, crop=(0,2),baseline = (-0.5,0.0), trial_removal_th = 100):\n",
    "\n",
    "    EEG_epoch = {}\n",
    "\n",
    "    for key_subs in EEG_data:\n",
    "        raw_edf = EEG_data[key_subs][\"Raw_data\"]\n",
    "\n",
    "        events, event_dict = mne.events_from_annotations(raw_edf)\n",
    "\n",
    "        if key_subs == 'A04':\n",
    "            event_dict =  {'769': 5,\n",
    "            '770': 6,\n",
    "            '772': 8,\n",
    "            '771': 7}\n",
    "            mapping = {5: 0, 6: 1, 8: 2, 7: 3}\n",
    "            selected_events = events[np.isin(events[:, 2], [5, 6, 7, 8])]\n",
    "\n",
    "        else:\n",
    "            event_dict =  {'769': 7,\n",
    "            '770': 8,\n",
    "            '772': 10,\n",
    "            '771': 9}\n",
    "            mapping = {7: 0, 8: 1, 10: 2, 9: 3}\n",
    "            selected_events = events[np.isin(events[:, 2], [7, 8, 9, 10])]\n",
    "\n",
    "        Epochs = mne.Epochs(raw_edf, selected_events, \n",
    "            tmin= tmin,  \n",
    "            tmax= tmax,    \n",
    "            event_id=event_dict,\n",
    "            preload = True,\n",
    "            event_repeated='drop',\n",
    "            baseline=baseline,\n",
    "            verbose=False\n",
    "            )\n",
    "        \n",
    "        selected_ch = ['EEG-Fz', 'EEG-Cz', 'EEG-C3', 'EEG-C4', 'EEG-Pz']\n",
    "        \n",
    "        EEG_epoch[key_subs] =  {\"Raw_Epoch\": Epochs.copy().pick(selected_ch).crop(tmin= crop[0], tmax= crop[1])}\n",
    "\n",
    "        train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5 \n",
    "\n",
    "        labels = EEG_epoch[key_subs][\"Raw_Epoch\"].copy().events[:,-1]\n",
    "\n",
    "        labels = np.vectorize(mapping.get)(labels)\n",
    "\n",
    "        outlier_trial = []\n",
    "        for ii in range(0,train_data.shape[0]):\n",
    "            if train_data[ii].max() > trial_removal_th or train_data[ii].min() < -trial_removal_th:\n",
    "                outlier_trial.append(ii)\n",
    "                print(key_subs,train_data[ii].min(), ii)\n",
    "                print(key_subs,train_data[ii].max(), ii)\n",
    "\n",
    "        EEG_epoch[key_subs]['Raw_Epoch'] = np.delete(train_data, outlier_trial, axis = 0)\n",
    "        EEG_epoch[key_subs]['label'] = np.delete(labels, outlier_trial)\n",
    "\n",
    "        filtered_data = butter_bandpass_filter(EEG_epoch[key_subs]['Raw_Epoch'], lowcut= 6, highcut= 32)\n",
    "        EEG_epoch[key_subs]['Raw_Epoch'] = filtered_data\n",
    "\n",
    "    return EEG_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n",
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_13236\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_13236\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_13236\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_13236\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_13236\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_13236\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_13236\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_13236\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_13236\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    }
   ],
   "source": [
    "EEG_Epochs = GetEpoch(Raw_data ,tmin= -1.0, tmax= 4.0, crop = (0,4) ,baseline= (-0.5,0.0), trial_removal_th = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "LDA only Cross-validation scores: 0.6516666666666666\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.66      0.69        50\n",
      "           1       0.69      0.75      0.72        51\n",
      "           2       0.72      0.78      0.75        50\n",
      "           3       0.65      0.60      0.63        50\n",
      "\n",
      "    accuracy                           0.70       201\n",
      "   macro avg       0.70      0.70      0.69       201\n",
      "weighted avg       0.70      0.70      0.69       201\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[33 14  0  3]\n",
      " [ 9 38  0  4]\n",
      " [ 2  0 39  9]\n",
      " [ 2  3 15 30]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.27      0.33        22\n",
      "           1       0.54      0.62      0.58        21\n",
      "           2       0.76      0.73      0.74        22\n",
      "           3       0.54      0.68      0.60        22\n",
      "\n",
      "    accuracy                           0.57        87\n",
      "   macro avg       0.57      0.58      0.56        87\n",
      "weighted avg       0.57      0.57      0.56        87\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[ 6 10  0  6]\n",
      " [ 7 13  0  1]\n",
      " [ 0  0 16  6]\n",
      " [ 1  1  5 15]]\n"
     ]
    }
   ],
   "source": [
    "def GetConfusionMatrix(models, X_train, X_test, y_train, y_test):\n",
    "    y_pred = models.predict(X_train)\n",
    "    print(\"Classification TRAIN DATA \\n=======================\")\n",
    "    print(classification_report(y_true= y_train, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true= y_train, y_pred=y_pred))\n",
    "\n",
    "    y_pred = models.predict(X_test)\n",
    "    print(\"Classification TEST DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "    \n",
    "\n",
    "label_target = EEG_Epochs[target_data_0]['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(EEG_Epochs[target_data_0]['Raw_Epoch'], label_target, test_size=0.3, random_state = 42, stratify=label_target)\n",
    "\n",
    "csp = CSP(n_components = 5, reg=None, log=None, rank= 'info')\n",
    "csp.fit(x_train, y_train)   \n",
    "\n",
    "x_train = csp.transform(x_train)\n",
    "x_test = csp.transform(x_test)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "score = cross_val_score(lda, x_train, y_train, cv= 10)\n",
    "print(\"LDA only Cross-validation scores:\", np.mean(score))\n",
    "lda.fit(x_train, y_train)\n",
    "\n",
    "GetConfusionMatrix(lda, x_train, x_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BCIAllFunction import BCIFuntions\n",
    "\n",
    "calibrate_size = calibrate_size / EEG_Epochs[target_data_0]['Raw_Epoch'].shape[0]\n",
    "AllBCIClass = BCIFuntions(numclass = 4, frequency = 250, ch_pick = ['Fz','C3', 'Cz','C4','Pz'])\n",
    "\n",
    "\n",
    "if alignmentMethod == \"LA\":\n",
    "    AllBCIClass.ComputeLA(EEG_Epochs, target_subject= target_data_0, calibrate_size=calibrate_size)\n",
    "    if calibrate_size != 0:\n",
    "        target_data = target_data_0 + \"_test\"\n",
    "    count = 0\n",
    "\n",
    "    for index in range(len(EEG_Epochs[target_data]['KMediod_label'])):\n",
    "        if EEG_Epochs[target_data]['label'][index] == EEG_Epochs[target_data]['KMediod_label'][index]:\n",
    "            count += 1\n",
    "    print(count/len(EEG_Epochs[target_data]['KMediod_label']) * 100)\n",
    "    \n",
    "else:\n",
    "    AllBCIClass.GetRawSet_ComputeEA(EEG_Epochs, target_subject= target_data_0, calibrate_size=calibrate_size)\n",
    "    if calibrate_size != 0:\n",
    "        target_data = target_data_0 + \"_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Raw_Epoch', 'label', 'Raw_left', 'Raw_right', 'Raw_non', 'Raw_feet', 'EA_left', 'EA_right', 'EA_feet', 'EA_non', 'EA_Epoch'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EEG_Epochs['A01'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "LDA only Cross-validation scores: 0.3815377376246941\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.42      0.39       576\n",
      "           1       0.41      0.50      0.45       576\n",
      "           2       0.41      0.36      0.38       576\n",
      "           3       0.39      0.28      0.33       576\n",
      "\n",
      "    accuracy                           0.39      2304\n",
      "   macro avg       0.39      0.39      0.39      2304\n",
      "weighted avg       0.39      0.39      0.39      2304\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[243 147 104  82]\n",
      " [136 286  80  74]\n",
      " [143 131 206  96]\n",
      " [161 141 111 163]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.54      0.54        57\n",
      "           1       0.37      0.51      0.43        57\n",
      "           2       0.38      0.14      0.21        57\n",
      "           3       0.30      0.37      0.33        57\n",
      "\n",
      "    accuracy                           0.39       228\n",
      "   macro avg       0.40      0.39      0.38       228\n",
      "weighted avg       0.40      0.39      0.38       228\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[31 10  4 12]\n",
      " [18 29  3  7]\n",
      " [ 4 15  8 30]\n",
      " [ 5 25  6 21]]\n"
     ]
    }
   ],
   "source": [
    "AllBCIClass.classifyCSP_LDA(EEG_Epochs, target_subjects= target_data, calibrate_data= target_data_0, condition = \"noEA\") #target_sub is used for target_data otherwise are source_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "LDA only Cross-validation scores: 0.4674948240165631\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.53      0.51       576\n",
      "           1       0.49      0.50      0.49       576\n",
      "           2       0.47      0.51      0.49       576\n",
      "           3       0.39      0.31      0.35       576\n",
      "\n",
      "    accuracy                           0.46      2304\n",
      "   macro avg       0.46      0.46      0.46      2304\n",
      "weighted avg       0.46      0.46      0.46      2304\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[307 120  70  79]\n",
      " [102 289  95  90]\n",
      " [ 96  76 295 109]\n",
      " [116 110 170 180]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.89      0.66        57\n",
      "           1       0.52      0.28      0.36        57\n",
      "           2       0.51      0.60      0.55        57\n",
      "           3       0.34      0.19      0.25        57\n",
      "\n",
      "    accuracy                           0.49       228\n",
      "   macro avg       0.47      0.49      0.45       228\n",
      "weighted avg       0.47      0.49      0.45       228\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[51  3  1  2]\n",
      " [40 16  1  0]\n",
      " [ 3  1 34 19]\n",
      " [ 4 11 31 11]]\n"
     ]
    }
   ],
   "source": [
    "AllBCIClass.classifyCSP_LDA(EEG_Epochs, target_subjects= target_data, calibrate_data= target_data_0,condition = \"EA\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WLTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "CSP2D_Epoch = AllBCIClass.computeCSPFeatures(EEG_Epochs, target_subject = target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.6863042116165161\n",
      "Epoch 20, Loss: 1.6186344623565674\n",
      "Epoch 40, Loss: 1.5694645643234253\n",
      "Epoch 60, Loss: 1.540996789932251\n",
      "Epoch 80, Loss: 1.53383469581604\n",
      "Epoch 100, Loss: 1.546771764755249\n",
      "Epoch 120, Loss: 1.5777798891067505\n",
      "Epoch 140, Loss: 1.624530553817749\n",
      "Epoch 160, Loss: 1.6846745014190674\n",
      "Epoch 180, Loss: 1.755986213684082\n",
      "Epoch 200, Loss: 1.8364484310150146\n",
      "Epoch 220, Loss: 1.924293041229248\n",
      "Epoch 240, Loss: 2.018007516860962\n",
      "Epoch 260, Loss: 2.116323709487915\n",
      "Epoch 280, Loss: 2.2181975841522217\n",
      "Epoch 300, Loss: 2.3227806091308594\n",
      "Epoch 320, Loss: 2.4293899536132812\n",
      "Epoch 340, Loss: 2.5374841690063477\n",
      "Epoch 360, Loss: 2.646635055541992\n",
      "Epoch 380, Loss: 2.7565059661865234\n",
      "Epoch 400, Loss: 2.8668341636657715\n",
      "Epoch 420, Loss: 2.977416753768921\n",
      "Epoch 440, Loss: 3.088094711303711\n",
      "Epoch 460, Loss: 3.198747158050537\n",
      "Epoch 480, Loss: 3.3092832565307617\n",
      "Epoch 500, Loss: 3.4196338653564453\n",
      "Epoch 520, Loss: 3.529752254486084\n",
      "Epoch 540, Loss: 3.6396048069000244\n",
      "Epoch 560, Loss: 3.7491745948791504\n",
      "Epoch 580, Loss: 3.8584530353546143\n",
      "Epoch 600, Loss: 3.967442035675049\n",
      "Epoch 620, Loss: 4.0761518478393555\n",
      "Epoch 640, Loss: 4.184601783752441\n",
      "Epoch 660, Loss: 4.292812824249268\n",
      "Epoch 680, Loss: 4.400812149047852\n",
      "Epoch 700, Loss: 4.508631706237793\n",
      "Epoch 720, Loss: 4.616302490234375\n",
      "Epoch 740, Loss: 4.723862171173096\n",
      "Epoch 760, Loss: 4.8313446044921875\n",
      "Epoch 780, Loss: 4.938787460327148\n",
      "Epoch 800, Loss: 5.046226501464844\n",
      "Epoch 820, Loss: 5.153698444366455\n",
      "Epoch 840, Loss: 5.261237621307373\n",
      "Epoch 860, Loss: 5.368879795074463\n",
      "Epoch 880, Loss: 5.476655006408691\n",
      "Epoch 900, Loss: 5.584595680236816\n",
      "Epoch 920, Loss: 5.6927337646484375\n",
      "Epoch 940, Loss: 5.801092624664307\n",
      "Epoch 960, Loss: 5.909701824188232\n",
      "Epoch 980, Loss: 6.018584251403809\n",
      "weights of  A02 :  [[array([[-0.28188884,  0.32527527, -0.03625604,  0.05302818],\n",
      "       [ 0.07861408,  0.45948958, -0.15600847,  0.10135359],\n",
      "       [ 0.56817394, -0.19406451,  0.30105072, -0.10920246],\n",
      "       [-0.6745921 , -0.5386857 ,  0.1415069 ,  0.36296856],\n",
      "       [ 0.09434424, -0.2147618 , -0.2529451 ,  0.06182817]],\n",
      "      dtype=float32), array([-0.30000907, -0.03378867, -0.369512  ,  0.29209414], dtype=float32)]]\n",
      "Lowest loss of  A02 :  1.5335783\n",
      "Epoch 0, Loss: 4.109429359436035\n",
      "Epoch 20, Loss: 2.9792017936706543\n",
      "Epoch 40, Loss: 2.069502353668213\n",
      "Epoch 60, Loss: 1.6577918529510498\n",
      "Epoch 80, Loss: 1.5878993272781372\n",
      "Epoch 100, Loss: 1.5746121406555176\n",
      "Epoch 120, Loss: 1.5617810487747192\n",
      "Epoch 140, Loss: 1.5504645109176636\n",
      "Epoch 160, Loss: 1.5402923822402954\n",
      "Epoch 180, Loss: 1.5315463542938232\n",
      "Epoch 200, Loss: 1.5243912935256958\n",
      "Epoch 220, Loss: 1.5189486742019653\n",
      "Epoch 240, Loss: 1.51531183719635\n",
      "Epoch 260, Loss: 1.5135550498962402\n",
      "Epoch 280, Loss: 1.5137311220169067\n",
      "Epoch 300, Loss: 1.5158771276474\n",
      "Epoch 320, Loss: 1.5200165510177612\n",
      "Epoch 340, Loss: 1.526160717010498\n",
      "Epoch 360, Loss: 1.5343116521835327\n",
      "Epoch 380, Loss: 1.5444636344909668\n",
      "Epoch 400, Loss: 1.556603193283081\n",
      "Epoch 420, Loss: 1.5707118511199951\n",
      "Epoch 440, Loss: 1.5867654085159302\n",
      "Epoch 460, Loss: 1.6047359704971313\n",
      "Epoch 480, Loss: 1.6245919466018677\n",
      "Epoch 500, Loss: 1.646299123764038\n",
      "Epoch 520, Loss: 1.6698200702667236\n",
      "Epoch 540, Loss: 1.6951167583465576\n",
      "Epoch 560, Loss: 1.7221484184265137\n",
      "Epoch 580, Loss: 1.7508738040924072\n",
      "Epoch 600, Loss: 1.7812504768371582\n",
      "Epoch 620, Loss: 1.8132355213165283\n",
      "Epoch 640, Loss: 1.8467860221862793\n",
      "Epoch 660, Loss: 1.8818581104278564\n",
      "Epoch 680, Loss: 1.9184091091156006\n",
      "Epoch 700, Loss: 1.9563952684402466\n",
      "Epoch 720, Loss: 1.9957740306854248\n",
      "Epoch 740, Loss: 2.0365030765533447\n",
      "Epoch 760, Loss: 2.0785410404205322\n",
      "Epoch 780, Loss: 2.1218461990356445\n",
      "Epoch 800, Loss: 2.166378974914551\n",
      "Epoch 820, Loss: 2.212099075317383\n",
      "Epoch 840, Loss: 2.2589681148529053\n",
      "Epoch 860, Loss: 2.3069491386413574\n",
      "Epoch 880, Loss: 2.356003522872925\n",
      "Epoch 900, Loss: 2.4060964584350586\n",
      "Epoch 920, Loss: 2.4571924209594727\n",
      "Epoch 940, Loss: 2.5092568397521973\n",
      "Epoch 960, Loss: 2.5622575283050537\n",
      "Epoch 980, Loss: 2.616161346435547\n",
      "weights of  A03 :  [[array([[ 0.10829629,  0.34913173, -0.27868974, -0.1297284 ],\n",
      "       [ 0.5672372 , -0.6223026 ,  0.40174744,  0.5370289 ],\n",
      "       [-0.23091994,  0.5930756 , -0.26229417,  0.41010284],\n",
      "       [-0.42908993, -0.10449957,  0.3873584 , -0.15074857],\n",
      "       [-0.23567306,  0.06375346,  0.32354277, -0.19658186]],\n",
      "      dtype=float32), array([-0.5666731 ,  0.11084146, -0.11724841,  0.3804505 ], dtype=float32)]]\n",
      "Lowest loss of  A03 :  1.513391\n",
      "Epoch 0, Loss: 1.9314048290252686\n",
      "Epoch 20, Loss: 1.7635189294815063\n",
      "Epoch 40, Loss: 1.7478684186935425\n",
      "Epoch 60, Loss: 1.7326505184173584\n",
      "Epoch 80, Loss: 1.71896493434906\n",
      "Epoch 100, Loss: 1.7081457376480103\n",
      "Epoch 120, Loss: 1.7009344100952148\n",
      "Epoch 140, Loss: 1.6978956460952759\n",
      "Epoch 160, Loss: 1.6993225812911987\n",
      "Epoch 180, Loss: 1.7053427696228027\n",
      "Epoch 200, Loss: 1.7159523963928223\n",
      "Epoch 220, Loss: 1.7310478687286377\n",
      "Epoch 240, Loss: 1.7504554986953735\n",
      "Epoch 260, Loss: 1.7739492654800415\n",
      "Epoch 280, Loss: 1.8012685775756836\n",
      "Epoch 300, Loss: 1.8321301937103271\n",
      "Epoch 320, Loss: 1.8662391901016235\n",
      "Epoch 340, Loss: 1.9032957553863525\n",
      "Epoch 360, Loss: 1.9430022239685059\n",
      "Epoch 380, Loss: 1.985067367553711\n",
      "Epoch 400, Loss: 2.0292086601257324\n",
      "Epoch 420, Loss: 2.0751566886901855\n",
      "Epoch 440, Loss: 2.122654438018799\n",
      "Epoch 460, Loss: 2.171459913253784\n",
      "Epoch 480, Loss: 2.221346378326416\n",
      "Epoch 500, Loss: 2.272101640701294\n",
      "Epoch 520, Loss: 2.323528289794922\n",
      "Epoch 540, Loss: 2.3754444122314453\n",
      "Epoch 560, Loss: 2.4276819229125977\n",
      "Epoch 580, Loss: 2.4800868034362793\n",
      "Epoch 600, Loss: 2.532517910003662\n",
      "Epoch 620, Loss: 2.5848469734191895\n",
      "Epoch 640, Loss: 2.6369571685791016\n",
      "Epoch 660, Loss: 2.688743829727173\n",
      "Epoch 680, Loss: 2.7401113510131836\n",
      "Epoch 700, Loss: 2.7909765243530273\n",
      "Epoch 720, Loss: 2.8412630558013916\n",
      "Epoch 740, Loss: 2.8909053802490234\n",
      "Epoch 760, Loss: 2.939845085144043\n",
      "Epoch 780, Loss: 2.988032341003418\n",
      "Epoch 800, Loss: 3.035423755645752\n",
      "Epoch 820, Loss: 3.081982374191284\n",
      "Epoch 840, Loss: 3.1276791095733643\n",
      "Epoch 860, Loss: 3.1724894046783447\n",
      "Epoch 880, Loss: 3.2163946628570557\n",
      "Epoch 900, Loss: 3.259380340576172\n",
      "Epoch 920, Loss: 3.3014369010925293\n",
      "Epoch 940, Loss: 3.342559337615967\n",
      "Epoch 960, Loss: 3.382746934890747\n",
      "Epoch 980, Loss: 3.4220008850097656\n",
      "weights of  A04 :  [[array([[-0.634376  , -0.40847364, -0.01642635, -0.51298076],\n",
      "       [-0.14597714,  0.02006063,  0.29936066, -0.03579251],\n",
      "       [ 0.22455469,  0.14387321, -0.4479327 ,  0.5876981 ],\n",
      "       [ 0.04842022,  0.35104993,  1.0187668 ,  0.0316627 ],\n",
      "       [ 0.20620689, -0.18868996, -0.53192174, -0.27483904]],\n",
      "      dtype=float32), array([-0.26137006,  0.04204024,  0.448322  , -0.01350348], dtype=float32)]]\n",
      "Lowest loss of  A04 :  1.6978167\n",
      "Epoch 0, Loss: 3.420656681060791\n",
      "Epoch 20, Loss: 2.406435012817383\n",
      "Epoch 40, Loss: 2.05684494972229\n",
      "Epoch 60, Loss: 1.864230990409851\n",
      "Epoch 80, Loss: 1.8367729187011719\n",
      "Epoch 100, Loss: 1.8332959413528442\n",
      "Epoch 120, Loss: 1.8280363082885742\n",
      "Epoch 140, Loss: 1.8228013515472412\n",
      "Epoch 160, Loss: 1.8178908824920654\n",
      "Epoch 180, Loss: 1.8128330707550049\n",
      "Epoch 200, Loss: 1.8077976703643799\n",
      "Epoch 220, Loss: 1.8028473854064941\n",
      "Epoch 240, Loss: 1.7980287075042725\n",
      "Epoch 260, Loss: 1.7933897972106934\n",
      "Epoch 280, Loss: 1.7889719009399414\n",
      "Epoch 300, Loss: 1.7848145961761475\n",
      "Epoch 320, Loss: 1.780951738357544\n",
      "Epoch 340, Loss: 1.7774144411087036\n",
      "Epoch 360, Loss: 1.7742305994033813\n",
      "Epoch 380, Loss: 1.7714245319366455\n",
      "Epoch 400, Loss: 1.7690176963806152\n",
      "Epoch 420, Loss: 1.7670284509658813\n",
      "Epoch 440, Loss: 1.7654730081558228\n",
      "Epoch 460, Loss: 1.76436448097229\n",
      "Epoch 480, Loss: 1.763714075088501\n",
      "Epoch 500, Loss: 1.763529896736145\n",
      "Epoch 520, Loss: 1.7638188600540161\n",
      "Epoch 540, Loss: 1.7645854949951172\n",
      "Epoch 560, Loss: 1.765832543373108\n",
      "Epoch 580, Loss: 1.7675611972808838\n",
      "Epoch 600, Loss: 1.769770860671997\n",
      "Epoch 620, Loss: 1.7724592685699463\n",
      "Epoch 640, Loss: 1.7756233215332031\n",
      "Epoch 660, Loss: 1.779258131980896\n",
      "Epoch 680, Loss: 1.783358097076416\n",
      "Epoch 700, Loss: 1.7879163026809692\n",
      "Epoch 720, Loss: 1.7929253578186035\n",
      "Epoch 740, Loss: 1.7983758449554443\n",
      "Epoch 760, Loss: 1.8042594194412231\n",
      "Epoch 780, Loss: 1.8105652332305908\n",
      "Epoch 800, Loss: 1.817283034324646\n",
      "Epoch 820, Loss: 1.824401617050171\n",
      "Epoch 840, Loss: 1.831908941268921\n",
      "Epoch 860, Loss: 1.8397929668426514\n",
      "Epoch 880, Loss: 1.8480417728424072\n",
      "Epoch 900, Loss: 1.856642246246338\n",
      "Epoch 920, Loss: 1.8655810356140137\n",
      "Epoch 940, Loss: 1.8748457431793213\n",
      "Epoch 960, Loss: 1.8844225406646729\n",
      "Epoch 980, Loss: 1.8942980766296387\n",
      "weights of  A05 :  [[array([[ 0.00345724, -0.6915996 ,  0.5424453 , -0.5808582 ],\n",
      "       [-0.09716301,  0.5084171 ,  0.4732875 , -0.19002986],\n",
      "       [-0.23258154,  0.12224231, -0.94703144,  0.5265701 ],\n",
      "       [-0.5552658 , -0.58257705, -0.04552018,  0.2134401 ],\n",
      "       [-0.16204801, -0.05483522, -0.5639323 , -0.36035496]],\n",
      "      dtype=float32), array([-0.4463902 , -0.03094852,  0.38024807, -0.09314942], dtype=float32)]]\n",
      "Lowest loss of  A05 :  1.763527\n",
      "Epoch 0, Loss: 2.331671714782715\n",
      "Epoch 20, Loss: 1.9011106491088867\n",
      "Epoch 40, Loss: 1.9003016948699951\n",
      "Epoch 60, Loss: 1.8916923999786377\n",
      "Epoch 80, Loss: 1.8862714767456055\n",
      "Epoch 100, Loss: 1.8811628818511963\n",
      "Epoch 120, Loss: 1.8760883808135986\n",
      "Epoch 140, Loss: 1.871231198310852\n",
      "Epoch 160, Loss: 1.866712212562561\n",
      "Epoch 180, Loss: 1.8626680374145508\n",
      "Epoch 200, Loss: 1.8592069149017334\n",
      "Epoch 220, Loss: 1.8564234972000122\n",
      "Epoch 240, Loss: 1.8543988466262817\n",
      "Epoch 260, Loss: 1.8532017469406128\n",
      "Epoch 280, Loss: 1.8528902530670166\n",
      "Epoch 300, Loss: 1.8535137176513672\n",
      "Epoch 320, Loss: 1.8551125526428223\n",
      "Epoch 340, Loss: 1.857720136642456\n",
      "Epoch 360, Loss: 1.8613637685775757\n",
      "Epoch 380, Loss: 1.8660640716552734\n",
      "Epoch 400, Loss: 1.871837854385376\n",
      "Epoch 420, Loss: 1.8786969184875488\n",
      "Epoch 440, Loss: 1.8866479396820068\n",
      "Epoch 460, Loss: 1.895695447921753\n",
      "Epoch 480, Loss: 1.905838966369629\n",
      "Epoch 500, Loss: 1.9170753955841064\n",
      "Epoch 520, Loss: 1.929398536682129\n",
      "Epoch 540, Loss: 1.9427988529205322\n",
      "Epoch 560, Loss: 1.9572641849517822\n",
      "Epoch 580, Loss: 1.9727797508239746\n",
      "Epoch 600, Loss: 1.989328145980835\n",
      "Epoch 620, Loss: 2.006889820098877\n",
      "Epoch 640, Loss: 2.0254428386688232\n",
      "Epoch 660, Loss: 2.0449628829956055\n",
      "Epoch 680, Loss: 2.0654239654541016\n",
      "Epoch 700, Loss: 2.086798906326294\n",
      "Epoch 720, Loss: 2.109058141708374\n",
      "Epoch 740, Loss: 2.1321706771850586\n",
      "Epoch 760, Loss: 2.1561052799224854\n",
      "Epoch 780, Loss: 2.1808278560638428\n",
      "Epoch 800, Loss: 2.2063050270080566\n",
      "Epoch 820, Loss: 2.2325022220611572\n",
      "Epoch 840, Loss: 2.2593841552734375\n",
      "Epoch 860, Loss: 2.286914825439453\n",
      "Epoch 880, Loss: 2.315058469772339\n",
      "Epoch 900, Loss: 2.3437790870666504\n",
      "Epoch 920, Loss: 2.373040199279785\n",
      "Epoch 940, Loss: 2.402806043624878\n",
      "Epoch 960, Loss: 2.433040142059326\n",
      "Epoch 980, Loss: 2.4637084007263184\n",
      "weights of  A06 :  [[array([[ 0.25280634, -0.18899551, -0.42892876,  0.42263675],\n",
      "       [ 0.4932176 ,  0.13761768,  0.91856104, -0.10497551],\n",
      "       [ 0.12546828,  1.0229481 ,  0.7966253 ,  0.6125098 ],\n",
      "       [ 0.6221836 , -0.23779503,  0.5500156 ,  0.31370243],\n",
      "       [-0.06998672,  0.51558244, -0.16208772,  0.3585672 ]],\n",
      "      dtype=float32), array([-0.2163335 , -0.16211618, -0.00983346,  0.33080527], dtype=float32)]]\n",
      "Lowest loss of  A06 :  1.8528786\n",
      "Epoch 0, Loss: 3.5192489624023438\n",
      "Epoch 20, Loss: 2.2972984313964844\n",
      "Epoch 40, Loss: 1.756007432937622\n",
      "Epoch 60, Loss: 1.666656494140625\n",
      "Epoch 80, Loss: 1.661177158355713\n",
      "Epoch 100, Loss: 1.6595758199691772\n",
      "Epoch 120, Loss: 1.6585826873779297\n",
      "Epoch 140, Loss: 1.6577062606811523\n",
      "Epoch 160, Loss: 1.6571532487869263\n",
      "Epoch 180, Loss: 1.6570508480072021\n",
      "Epoch 200, Loss: 1.6574633121490479\n",
      "Epoch 220, Loss: 1.658431053161621\n",
      "Epoch 240, Loss: 1.6600106954574585\n",
      "Epoch 260, Loss: 1.6622503995895386\n",
      "Epoch 280, Loss: 1.6651909351348877\n",
      "Epoch 300, Loss: 1.6688671112060547\n",
      "Epoch 320, Loss: 1.6733089685440063\n",
      "Epoch 340, Loss: 1.6785414218902588\n",
      "Epoch 360, Loss: 1.684584379196167\n",
      "Epoch 380, Loss: 1.6914539337158203\n",
      "Epoch 400, Loss: 1.6991612911224365\n",
      "Epoch 420, Loss: 1.707714557647705\n",
      "Epoch 440, Loss: 1.7171177864074707\n",
      "Epoch 460, Loss: 1.727372407913208\n",
      "Epoch 480, Loss: 1.7384766340255737\n",
      "Epoch 500, Loss: 1.7504262924194336\n",
      "Epoch 520, Loss: 1.7632147073745728\n",
      "Epoch 540, Loss: 1.776833176612854\n",
      "Epoch 560, Loss: 1.7912715673446655\n",
      "Epoch 580, Loss: 1.8065180778503418\n",
      "Epoch 600, Loss: 1.8225595951080322\n",
      "Epoch 620, Loss: 1.839381217956543\n",
      "Epoch 640, Loss: 1.856968879699707\n",
      "Epoch 660, Loss: 1.8753058910369873\n",
      "Epoch 680, Loss: 1.894376277923584\n",
      "Epoch 700, Loss: 1.914163589477539\n",
      "Epoch 720, Loss: 1.9346504211425781\n",
      "Epoch 740, Loss: 1.955819845199585\n",
      "Epoch 760, Loss: 1.9776551723480225\n",
      "Epoch 780, Loss: 2.000138998031616\n",
      "Epoch 800, Loss: 2.023254871368408\n",
      "Epoch 820, Loss: 2.0469863414764404\n",
      "Epoch 840, Loss: 2.071317672729492\n",
      "Epoch 860, Loss: 2.0962328910827637\n",
      "Epoch 880, Loss: 2.1217164993286133\n",
      "Epoch 900, Loss: 2.147754430770874\n",
      "Epoch 920, Loss: 2.1743323802948\n",
      "Epoch 940, Loss: 2.2014362812042236\n",
      "Epoch 960, Loss: 2.229053497314453\n",
      "Epoch 980, Loss: 2.257171630859375\n",
      "weights of  A07 :  [[array([[ 0.42909068, -0.60953224, -0.21532722, -0.71842796],\n",
      "       [-0.41718945,  0.09260104,  0.59734964,  0.44016433],\n",
      "       [-0.31185088,  0.37094623, -0.20063415,  0.37580925],\n",
      "       [ 0.43150485, -0.09773399, -0.43418398, -0.06683868],\n",
      "       [ 0.02359663,  0.44669598,  0.11875761,  0.16648269]],\n",
      "      dtype=float32), array([ 0.14853632,  0.173596  , -0.14780731,  0.08365487], dtype=float32)]]\n",
      "Lowest loss of  A07 :  1.6570287\n",
      "Epoch 0, Loss: 2.123863458633423\n",
      "Epoch 20, Loss: 1.7748652696609497\n",
      "Epoch 40, Loss: 1.755090594291687\n",
      "Epoch 60, Loss: 1.7478134632110596\n",
      "Epoch 80, Loss: 1.7457194328308105\n",
      "Epoch 100, Loss: 1.7457106113433838\n",
      "Epoch 120, Loss: 1.7483556270599365\n",
      "Epoch 140, Loss: 1.7538172006607056\n",
      "Epoch 160, Loss: 1.7624726295471191\n",
      "Epoch 180, Loss: 1.7744827270507812\n",
      "Epoch 200, Loss: 1.7900099754333496\n",
      "Epoch 220, Loss: 1.8091416358947754\n",
      "Epoch 240, Loss: 1.8319326639175415\n",
      "Epoch 260, Loss: 1.8583974838256836\n",
      "Epoch 280, Loss: 1.888521671295166\n",
      "Epoch 300, Loss: 1.9222640991210938\n",
      "Epoch 320, Loss: 1.959561824798584\n",
      "Epoch 340, Loss: 2.000333786010742\n",
      "Epoch 360, Loss: 2.0444846153259277\n",
      "Epoch 380, Loss: 2.0919065475463867\n",
      "Epoch 400, Loss: 2.1424827575683594\n",
      "Epoch 420, Loss: 2.1960883140563965\n",
      "Epoch 440, Loss: 2.2525949478149414\n",
      "Epoch 460, Loss: 2.3118696212768555\n",
      "Epoch 480, Loss: 2.373777389526367\n",
      "Epoch 500, Loss: 2.4381816387176514\n",
      "Epoch 520, Loss: 2.5049474239349365\n",
      "Epoch 540, Loss: 2.573939800262451\n",
      "Epoch 560, Loss: 2.6450271606445312\n",
      "Epoch 580, Loss: 2.718078136444092\n",
      "Epoch 600, Loss: 2.7929656505584717\n",
      "Epoch 620, Loss: 2.8695662021636963\n",
      "Epoch 640, Loss: 2.9477598667144775\n",
      "Epoch 660, Loss: 3.0274295806884766\n",
      "Epoch 680, Loss: 3.108463764190674\n",
      "Epoch 700, Loss: 3.1907525062561035\n",
      "Epoch 720, Loss: 3.27419376373291\n",
      "Epoch 740, Loss: 3.358687400817871\n",
      "Epoch 760, Loss: 3.444136142730713\n",
      "Epoch 780, Loss: 3.5304512977600098\n",
      "Epoch 800, Loss: 3.617544412612915\n",
      "Epoch 820, Loss: 3.705331802368164\n",
      "Epoch 840, Loss: 3.7937355041503906\n",
      "Epoch 860, Loss: 3.882680892944336\n",
      "Epoch 880, Loss: 3.972095251083374\n",
      "Epoch 900, Loss: 4.061910629272461\n",
      "Epoch 920, Loss: 4.152063846588135\n",
      "Epoch 940, Loss: 4.242492198944092\n",
      "Epoch 960, Loss: 4.3331379890441895\n",
      "Epoch 980, Loss: 4.423947334289551\n",
      "weights of  A08 :  [[array([[ 0.23639777,  0.14825441,  0.2910862 ,  0.23608503],\n",
      "       [ 0.9705478 ,  0.22270222,  0.4373395 , -0.07035628],\n",
      "       [-0.849078  , -0.30187085, -0.60978365, -0.523466  ],\n",
      "       [ 0.159712  , -0.65123457, -0.2573622 , -0.07143638],\n",
      "       [-0.6112823 ,  0.2770108 ,  0.23845671,  0.3961544 ]],\n",
      "      dtype=float32), array([-0.24454853, -0.13716497,  0.23967125, -0.04455912], dtype=float32)]]\n",
      "Lowest loss of  A08 :  1.7453979\n",
      "Epoch 0, Loss: 3.557304859161377\n",
      "Epoch 20, Loss: 2.5340735912323\n",
      "Epoch 40, Loss: 2.045849323272705\n",
      "Epoch 60, Loss: 1.8598709106445312\n",
      "Epoch 80, Loss: 1.8379521369934082\n",
      "Epoch 100, Loss: 1.8196707963943481\n",
      "Epoch 120, Loss: 1.8017657995224\n",
      "Epoch 140, Loss: 1.7838311195373535\n",
      "Epoch 160, Loss: 1.7656548023223877\n",
      "Epoch 180, Loss: 1.7473571300506592\n",
      "Epoch 200, Loss: 1.729240894317627\n",
      "Epoch 220, Loss: 1.7115256786346436\n",
      "Epoch 240, Loss: 1.6943854093551636\n",
      "Epoch 260, Loss: 1.6779903173446655\n",
      "Epoch 280, Loss: 1.6624908447265625\n",
      "Epoch 300, Loss: 1.6480188369750977\n",
      "Epoch 320, Loss: 1.6346914768218994\n",
      "Epoch 340, Loss: 1.6226106882095337\n",
      "Epoch 360, Loss: 1.6118654012680054\n",
      "Epoch 380, Loss: 1.6025304794311523\n",
      "Epoch 400, Loss: 1.5946698188781738\n",
      "Epoch 420, Loss: 1.5883355140686035\n",
      "Epoch 440, Loss: 1.5835695266723633\n",
      "Epoch 460, Loss: 1.5804038047790527\n",
      "Epoch 480, Loss: 1.5788624286651611\n",
      "Epoch 500, Loss: 1.5789604187011719\n",
      "Epoch 520, Loss: 1.5807058811187744\n",
      "Epoch 540, Loss: 1.5840997695922852\n",
      "Epoch 560, Loss: 1.589137315750122\n",
      "Epoch 580, Loss: 1.5958080291748047\n",
      "Epoch 600, Loss: 1.6040968894958496\n",
      "Epoch 620, Loss: 1.6139838695526123\n",
      "Epoch 640, Loss: 1.6254452466964722\n",
      "Epoch 660, Loss: 1.6384538412094116\n",
      "Epoch 680, Loss: 1.6529791355133057\n",
      "Epoch 700, Loss: 1.668988585472107\n",
      "Epoch 720, Loss: 1.6864464282989502\n",
      "Epoch 740, Loss: 1.7053160667419434\n",
      "Epoch 760, Loss: 1.7255580425262451\n",
      "Epoch 780, Loss: 1.7471318244934082\n",
      "Epoch 800, Loss: 1.769996166229248\n",
      "Epoch 820, Loss: 1.7941083908081055\n",
      "Epoch 840, Loss: 1.819425344467163\n",
      "Epoch 860, Loss: 1.8459031581878662\n",
      "Epoch 880, Loss: 1.8734978437423706\n",
      "Epoch 900, Loss: 1.9021644592285156\n",
      "Epoch 920, Loss: 1.931859016418457\n",
      "Epoch 940, Loss: 1.9625370502471924\n",
      "Epoch 960, Loss: 1.9941539764404297\n",
      "Epoch 980, Loss: 2.0266661643981934\n",
      "weights of  A09 :  [[array([[ 0.1876853 ,  0.17872815, -0.36860827, -0.07918718],\n",
      "       [-0.16615349, -0.13212825,  0.6704992 ,  0.34625816],\n",
      "       [-0.21403532, -0.0596535 , -0.24071062, -0.0310413 ],\n",
      "       [-0.88023686, -0.45067844,  0.6215633 , -0.06266054],\n",
      "       [ 0.49724135, -0.04775345, -0.13219479, -0.5062452 ]],\n",
      "      dtype=float32), array([-0.12153833, -0.3066038 ,  0.5688572 , -0.32620966], dtype=float32)]]\n",
      "Lowest loss of  A09 :  1.578703\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Custom loss function\n",
    "class CustomLossLL1(tf.keras.losses.Loss):\n",
    "    def __init__(self, lambda_t, model):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        ws = self.get_weights_from_model()\n",
    "        reg_term = self.regularization_term(ws)\n",
    "        return ce_loss + self.lambda_t * reg_term\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        # return tf.concat([tf.reshape(w, [-1]) for w in model_weights], axis=0)\n",
    "        return model_weights\n",
    "\n",
    "    def regularization_term(self, ws):\n",
    "        reg_term = tf.pow(tf.norm(ws, ord='euclidean'),2)\n",
    "        return reg_term\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True) # Perform a forward pass and compute the predictions\n",
    "        loss = custom_loss(y, y_pred) # Compute the custom loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_weight_LL(X_train, y_train, lambd, num_tier=10, learning_rate = 0.01):\n",
    "    n_classes = np.unique(y_train).size\n",
    "    y_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=n_classes)\n",
    "\n",
    "    lambda_t = lambd  # Regularization parameter\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_classes, input_shape=(X_train.shape[1],), activation='softmax')  # Adjust input_shape to match the number of features in X\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate)\n",
    "    custom_loss = CustomLossLL1(lambda_t, model)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = num_tier\n",
    "    lowest_loss = float('inf')\n",
    "    best_weights = None\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X_train, y_one_hot, custom_loss)\n",
    "        loss_value = loss.numpy()\n",
    "        if loss_value < lowest_loss:\n",
    "            lowest_loss = loss_value\n",
    "            best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    # best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "    return best_weights, lowest_loss\n",
    "\n",
    "def build_clf_params(data, target_subjects ,condition):\n",
    "\n",
    "    for sub in data.keys():\n",
    "        if (sub  != target_subjects) and  (sub != target_data_0): #Don't apply weight to target subject\n",
    "            # Where the tranining data is stored\n",
    "            if condition == \"noEA\":\n",
    "                X = data[sub]['Raw_csp']\n",
    "                y = data[sub]['Raw_csp_label']\n",
    "                store_ws = 'ws_Raw'\n",
    "\n",
    "            else:\n",
    "                X = data[sub]['EA_csp']\n",
    "                y = data[sub]['EA_csp_label']\n",
    "                store_ws = 'ws_EA'\n",
    "\n",
    "            weights, loss = train_weight_LL(X_train=X, y_train=y, lambd= 0.1, num_tier=1000, learning_rate= 0.005)\n",
    "            print(\"weights of \", str(sub), \": \", weights)\n",
    "            print(\"Lowest loss of \", str(sub), \": \", loss)\n",
    "            data[sub][store_ws] = weights\n",
    "\n",
    "build_clf_params(CSP2D_Epoch, target_subjects= target_data ,condition = condition_wLTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the kl divergence\n",
    "def KL_div(P, Q):\n",
    "    # First convert to np array\n",
    "    P = np.array(P)\n",
    "    Q = np.array(Q)\n",
    "    \n",
    "    # Then compute their means, datain shape of samples x feat\n",
    "    mu_P = np.mean(P, axis=0)\n",
    "    mu_Q = np.mean(Q, axis=0)    \n",
    "\n",
    "    \n",
    "    # Compute their covariance\n",
    "    sigma_P = np.cov(P, rowvar=False)\n",
    "    sigma_Q = np.cov(Q, rowvar=False)  \n",
    "\n",
    "    diff = mu_Q - mu_P\n",
    "\n",
    "    inv_sigma_Q = np.linalg.inv(sigma_Q)\n",
    "    term1 = np.dot(np.dot(diff.T, inv_sigma_Q), diff)\n",
    "    \n",
    "    # Calculate the trace term trace(Sigma_Q^{-1} * Sigma_P)\n",
    "    term2 = np.trace(np.dot(inv_sigma_Q, sigma_P))\n",
    "    \n",
    "    # Calculate the determinant term ln(det(Sigma_P) / det(Sigma_Q))\n",
    "    det_sigma0 = np.linalg.det(sigma_P)\n",
    "    det_sigma1 = np.linalg.det(sigma_Q)\n",
    "\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    term3 = np.log((det_sigma0+epsilon) / (det_sigma1+epsilon))\n",
    "    \n",
    "    print(term3)\n",
    "    \n",
    "    # Dimensionality of the data\n",
    "    K = mu_P.shape[0]\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_div = 0.5 * (term1 + term2 - term3 - K)\n",
    "    \n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.013046335809121101\n",
      "-0.020345206185433222\n",
      "-0.03704890610853611\n",
      "-0.0036543318428413907\n",
      "-0.11547122673100808\n",
      "-0.12188058181832209\n",
      "-0.08019869394593408\n",
      "-0.15258691487612716\n",
      "-0.009117815331594123\n",
      "-0.03195924974387014\n",
      "-0.019888838834848314\n",
      "-0.03363407838011897\n",
      "-0.07766147716213788\n",
      "-0.1339580591356929\n",
      "-0.10716656578592094\n",
      "-0.2236419578727472\n",
      "-0.792516492604584\n",
      "-0.38985354537227546\n",
      "-0.3780520634350904\n",
      "-0.5243189236006929\n",
      "0.009963885156666026\n",
      "-0.020956145697663475\n",
      "-0.020027596217610966\n",
      "-0.010764495085184646\n",
      "-0.06390564747268009\n",
      "-0.0371600264329485\n",
      "-0.08077822045638713\n",
      "-0.07884308942420591\n",
      "-0.8778010591307759\n",
      "-0.5122005605274579\n",
      "-0.15656597153216567\n",
      "-0.6767878638826109\n"
     ]
    }
   ],
   "source": [
    "# Compute kl divergence of target subject to each source subject\n",
    "def compute_all_kl_div(data, target_subjects , condition):\n",
    "    '''\n",
    "    Parameter:\n",
    "    data, is the whole data containing target and source data\n",
    "    '''\n",
    "    kl_div_score = []\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        target_data = 'Raw_csp'\n",
    "        label_name = 'Raw_csp_label'\n",
    "\n",
    "    else:\n",
    "        target_data = 'EA_csp'\n",
    "        label_name = 'EA_csp_label'\n",
    "        \n",
    "    # cal P from target data\n",
    "    label_tgt =  data[target_subjects][label_name]\n",
    "    P_left =  data[target_subjects][target_data][np.where(label_tgt == 0)]\n",
    "    P_right = data[target_subjects][target_data][np.where(label_tgt == 1)]\n",
    "    P_non = data[target_subjects][target_data][np.where(label_tgt == 2)]\n",
    "    P_feet = data[target_subjects][target_data][np.where(label_tgt == 3)]\n",
    "\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "\n",
    "    #cal Q from each source subject\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            label_src =  data[sub][label_name]\n",
    "            Q_left =  data[sub][target_data][np.where(label_src == 0)]\n",
    "            Q_right = data[sub][target_data][np.where(label_src == 1)]\n",
    "            Q_non = data[sub][target_data][np.where(label_src == 2)]\n",
    "            Q_feet = data[sub][target_data][np.where(label_src == 3)]\n",
    "\n",
    "            kl_left = KL_div(P_left, Q_left)\n",
    "            kl_right = KL_div(P_right, Q_right)\n",
    "            kl_non = KL_div(P_non, Q_non)\n",
    "            kl_feet = KL_div(P_feet, Q_feet)\n",
    "\n",
    "            # kl_div = (kl_left + kl_right+ kl_non + kl_feet)/4\n",
    "\n",
    "            kl_div_temp = [kl_left, kl_right, kl_non, kl_feet]\n",
    "\n",
    "            kl_div_score.append(kl_div_temp)\n",
    "\n",
    "    data[target_subjects]['kl_div'] = kl_div_score\n",
    "\n",
    "\n",
    "compute_all_kl_div(CSP2D_Epoch, target_subjects=target_data_0 ,condition = condition_wLTL) #target_sub for cal KL is calibrate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.42320557,  2.08335446,  5.78887027,  1.66003968],\n",
       "       [ 1.23518178,  0.45930739,  0.35049121, -0.64477476],\n",
       "       [ 3.18623324,  1.07699435,  2.15712201, -0.08035444],\n",
       "       [ 2.3720294 ,  1.27986848,  3.56701029, -0.85198609],\n",
       "       [ 0.10415563,  0.26472214,  1.08903794, -0.85246588],\n",
       "       [ 5.00787935,  1.23511343,  2.01016668,  1.21894158],\n",
       "       [ 3.66177753,  0.78463994,  0.87388955, -0.17226742],\n",
       "       [-0.52804668, -0.10113967,  0.02055188, -0.70714614]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(CSP2D_Epoch[target_data_0]['kl_div'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0026122349312512195, 0.4294729870447311, 0.009701418086893959, 0.03158249952373265, 8464.510828289724, 0.0015898270541893347, 0.005561407760538303, 12.871813705717567]\n",
      "[0.05307181644730946, 22.449573326988762, 0.7429934991796989, 0.3725657280700145, 203.32128831847328, 0.4295680566394077, 2.636920109705572, 9594.719183163676]\n",
      "[[3.08124215e-07 5.40186270e-06 1.61967392e-10 5.21217005e-06]\n",
      " [5.06581645e-05 2.28500777e-03 1.20400634e-05 2.29210458e-04]\n",
      " [1.14432350e-06 7.56248635e-05 8.39950113e-09 9.54378837e-01]\n",
      " [3.72529008e-06 3.79212367e-05 1.12348355e-09 7.51745784e-05]\n",
      " [9.98425035e-01 2.06948576e-02 1.29271061e-07 7.50054593e-05]\n",
      " [1.87526860e-07 4.37231627e-05 1.11382809e-08 1.79276362e-05]\n",
      " [6.55991687e-07 2.68396323e-04 3.11751035e-07 4.50602138e-02]\n",
      " [1.51828515e-03 9.76589067e-01 9.99987498e-01 1.58418565e-04]]\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_weights(data, target_subjects):\n",
    "    kl = data[target_subjects]['kl_div']\n",
    "    KL_inv_left = []\n",
    "    KL_inv_right = []\n",
    "    KL_inv_non = []\n",
    "    KL_inv_feet = []\n",
    "\n",
    "    alpha_s = []\n",
    "    eps = 0.0001\n",
    "    \n",
    "    #equation (9)\n",
    "    for val in kl:\n",
    "        if val != 0: \n",
    "            KL_inv_left.append(1/((val[0] + eps)**4))\n",
    "            KL_inv_right.append(1/((val[1] + eps)**4))\n",
    "            KL_inv_non.append(1/((val[2] + eps)**4))\n",
    "            KL_inv_feet.append(1/((val[3] + eps)**4))\n",
    "\n",
    "    print(KL_inv_left)\n",
    "    print(KL_inv_right)\n",
    "    \n",
    "    for i in range(0,len(KL_inv_left)):\n",
    "        temp = [KL_inv_left[i]/sum(KL_inv_left), KL_inv_right[i]/sum(KL_inv_right), KL_inv_non[i]/sum(KL_inv_non), KL_inv_feet[i]/sum(KL_inv_feet)]\n",
    "        alpha_s.append(temp)\n",
    "\n",
    "    print(np.array(alpha_s))\n",
    "                \n",
    "    data[target_subjects]['alpha_s'] = alpha_s\n",
    "\n",
    "compute_similarity_weights(CSP2D_Epoch, target_subjects=target_data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25269806  0.17138827 -0.36860698 -0.47900679]\n",
      " [ 0.49221731 -0.1275219   0.67049592 -0.03716568]\n",
      " [ 0.12493296 -0.03578234 -0.24071086  0.53747992]\n",
      " [ 0.61984348 -0.44546507  0.6215602   0.02699506]\n",
      " [-0.06913421 -0.03574355 -0.13218919 -0.24457191]]\n",
      "[[ 3.22115305 -0.88961543 -1.02450094 -1.13448107  1.00520276]\n",
      " [-0.88961543  4.96680777 -0.80309866  5.44433701 -0.76217483]\n",
      " [-1.02450094 -0.80309866  2.57491708 -0.2796031  -0.76348048]\n",
      " [-1.13448107  5.44433701 -0.2796031   6.78515259 -0.80725858]\n",
      " [ 1.00520276 -0.76217483 -0.76348048 -0.80725858  0.59384656]]\n"
     ]
    }
   ],
   "source": [
    "def compute_ETL_and_mu_ws(data, target_subjects, condition):\n",
    "\n",
    "    mu_ws = 0\n",
    "    temp_ws = 0\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        ws_name = 'ws_Raw'\n",
    "    else:\n",
    "        ws_name = 'ws_EA'\n",
    "\n",
    "    alpha_s = np.array(data[target_subjects]['alpha_s'])\n",
    "\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "    index_count = 0\n",
    "\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            ws = data[sub][ws_name][0][0]\n",
    "            # mu_ws += ws @ alpha_s  #equation (10)\n",
    "            # mu_ws += np.dot(ws, np.transpose(alpha_s))\n",
    "            mu_ws += ws * alpha_s[index_count]\n",
    "            index_count += 1\n",
    "\n",
    "    print(np.array(mu_ws))\n",
    "\n",
    "    index_count = 0\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            ws = data[sub][ws_name][0][0]\n",
    "            # ws_min_mu = np.dot((np.dot(ws,np.transpose(alpha_s)) - mu_ws) , np.transpose((np.dot(ws,np.transpose(alpha_s)) - mu_ws)))\n",
    "            ws_min_mu = np.dot(((ws * alpha_s[index_count]) - mu_ws), np.transpose((ws * alpha_s[index_count]) - mu_ws))\n",
    "            temp_ws += ws_min_mu #equation (11)\n",
    "            index_count += 1\n",
    "\n",
    "    print(np.array(temp_ws))\n",
    "    \n",
    "    # den = np.diag(temp_ws) #get array in diagonal line\n",
    "\n",
    "    den = temp_ws\n",
    "    nom = np.trace(temp_ws) #Return the sum along diagonals of the array.\n",
    "    Sigma_TL = den/nom\n",
    "\n",
    "\n",
    "    data[target_subjects]['Sigma_TL'] = Sigma_TL\n",
    "    data[target_subjects]['mu_ws'] = mu_ws\n",
    "\n",
    "compute_ETL_and_mu_ws(CSP2D_Epoch, target_subjects = target_data_0, condition=condition_wLTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(CSP2D_Epoch[target_data_0]['Sigma_TL']).shape)\n",
    "print(np.array(CSP2D_Epoch[target_data_0]['mu_ws']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 87.7649154663086\n",
      "Epoch 20, Loss: 117.83604431152344\n",
      "Epoch 40, Loss: 107.62225341796875\n",
      "Epoch 60, Loss: 102.5091552734375\n",
      "Epoch 80, Loss: 101.28036499023438\n",
      "Epoch 100, Loss: 103.3765869140625\n",
      "Epoch 120, Loss: 106.49283599853516\n",
      "Epoch 140, Loss: 110.0731201171875\n",
      "Epoch 160, Loss: 113.44339752197266\n",
      "Epoch 180, Loss: 116.2527847290039\n",
      "Epoch 200, Loss: 118.3266372680664\n",
      "Epoch 220, Loss: 119.55570983886719\n",
      "Epoch 240, Loss: 119.89937591552734\n",
      "Epoch 260, Loss: 119.34516906738281\n",
      "Epoch 280, Loss: 117.894775390625\n",
      "Epoch 300, Loss: 115.55709075927734\n",
      "Epoch 320, Loss: 112.3413314819336\n",
      "Epoch 340, Loss: 108.2550048828125\n",
      "Epoch 360, Loss: 103.3022689819336\n",
      "Epoch 380, Loss: 97.48307800292969\n",
      "Epoch 400, Loss: 90.7938232421875\n",
      "Epoch 420, Loss: 83.2267837524414\n",
      "Epoch 440, Loss: 74.77128601074219\n",
      "Epoch 460, Loss: 65.4132080078125\n",
      "Epoch 480, Loss: 55.1358642578125\n",
      "Epoch 500, Loss: 43.92036056518555\n",
      "Epoch 520, Loss: 31.745458602905273\n",
      "Epoch 540, Loss: 18.588281631469727\n",
      "Epoch 560, Loss: 4.424511909484863\n",
      "Epoch 580, Loss: -10.771509170532227\n",
      "Epoch 600, Loss: -27.027149200439453\n",
      "Epoch 620, Loss: -44.37006759643555\n",
      "Epoch 640, Loss: -62.829246520996094\n",
      "Epoch 660, Loss: -82.4344711303711\n",
      "Epoch 680, Loss: -103.2156753540039\n",
      "Epoch 700, Loss: -125.2039566040039\n",
      "Epoch 720, Loss: -148.4308624267578\n",
      "Epoch 740, Loss: -172.9284210205078\n",
      "Epoch 760, Loss: -198.7288055419922\n",
      "Epoch 780, Loss: -225.86532592773438\n",
      "Epoch 800, Loss: -254.37100219726562\n",
      "Epoch 820, Loss: -284.27935791015625\n",
      "Epoch 840, Loss: -315.62408447265625\n",
      "Epoch 860, Loss: -348.4400939941406\n",
      "Epoch 880, Loss: -382.7616271972656\n",
      "Epoch 900, Loss: -418.6244201660156\n",
      "Epoch 920, Loss: -456.06292724609375\n",
      "Epoch 940, Loss: -495.11383056640625\n",
      "Epoch 960, Loss: -535.8129272460938\n",
      "Epoch 980, Loss: -578.19677734375\n",
      "Epoch 1000, Loss: -622.3021240234375\n",
      "Epoch 1020, Loss: -668.166015625\n",
      "Epoch 1040, Loss: -715.8272705078125\n",
      "Epoch 1060, Loss: -765.3234252929688\n",
      "Epoch 1080, Loss: -816.6923828125\n",
      "Epoch 1100, Loss: -869.9761962890625\n",
      "Epoch 1120, Loss: -925.2122802734375\n",
      "Epoch 1140, Loss: -982.4412231445312\n",
      "Epoch 1160, Loss: -1041.7041015625\n",
      "Epoch 1180, Loss: -1103.0428466796875\n",
      "Epoch 1200, Loss: -1166.4989013671875\n",
      "Epoch 1220, Loss: -1232.1134033203125\n",
      "Epoch 1240, Loss: -1299.931640625\n",
      "Epoch 1260, Loss: -1369.9967041015625\n",
      "Epoch 1280, Loss: -1442.350830078125\n",
      "Epoch 1300, Loss: -1517.042724609375\n",
      "Epoch 1320, Loss: -1594.115966796875\n",
      "Epoch 1340, Loss: -1673.6148681640625\n",
      "Epoch 1360, Loss: -1755.587158203125\n",
      "Epoch 1380, Loss: -1840.0836181640625\n",
      "Epoch 1400, Loss: -1927.150390625\n",
      "Epoch 1420, Loss: -2016.8333740234375\n",
      "Epoch 1440, Loss: -2109.18505859375\n",
      "Epoch 1460, Loss: -2204.25244140625\n",
      "Epoch 1480, Loss: -2302.088623046875\n",
      "Epoch 1500, Loss: -2402.744384765625\n",
      "Epoch 1520, Loss: -2506.26806640625\n",
      "Epoch 1540, Loss: -2612.716796875\n",
      "Epoch 1560, Loss: -2722.14306640625\n",
      "Epoch 1580, Loss: -2834.595947265625\n",
      "Epoch 1600, Loss: -2950.131591796875\n",
      "Epoch 1620, Loss: -3068.80419921875\n",
      "Epoch 1640, Loss: -3190.669921875\n",
      "Epoch 1660, Loss: -3315.778076171875\n",
      "Epoch 1680, Loss: -3444.19287109375\n",
      "Epoch 1700, Loss: -3575.96337890625\n",
      "Epoch 1720, Loss: -3711.15771484375\n",
      "Epoch 1740, Loss: -3849.82373046875\n",
      "Epoch 1760, Loss: -3992.016845703125\n",
      "Epoch 1780, Loss: -4137.8017578125\n",
      "Epoch 1800, Loss: -4287.2275390625\n",
      "Epoch 1820, Loss: -4440.35498046875\n",
      "Epoch 1840, Loss: -4597.24560546875\n",
      "Epoch 1860, Loss: -4757.95556640625\n",
      "Epoch 1880, Loss: -4922.54443359375\n",
      "Epoch 1900, Loss: -5091.07373046875\n",
      "Epoch 1920, Loss: -5263.60302734375\n",
      "Epoch 1940, Loss: -5440.1826171875\n",
      "Epoch 1960, Loss: -5620.8818359375\n",
      "Epoch 1980, Loss: -5805.755859375\n",
      "Epoch 2000, Loss: -5994.86669921875\n",
      "Epoch 2020, Loss: -6188.2734375\n",
      "Epoch 2040, Loss: -6386.0361328125\n",
      "Epoch 2060, Loss: -6588.21337890625\n",
      "Epoch 2080, Loss: -6794.86572265625\n",
      "Epoch 2100, Loss: -7006.046875\n",
      "Epoch 2120, Loss: -7221.82421875\n",
      "Epoch 2140, Loss: -7442.25146484375\n",
      "Epoch 2160, Loss: -7667.38525390625\n",
      "Epoch 2180, Loss: -7897.2861328125\n",
      "Epoch 2200, Loss: -8132.0146484375\n",
      "Epoch 2220, Loss: -8371.6416015625\n",
      "Epoch 2240, Loss: -8616.2060546875\n",
      "Epoch 2260, Loss: -8865.7744140625\n",
      "Epoch 2280, Loss: -9120.4052734375\n",
      "Epoch 2300, Loss: -9380.1513671875\n",
      "Epoch 2320, Loss: -9645.0703125\n",
      "Epoch 2340, Loss: -9915.228515625\n",
      "Epoch 2360, Loss: -10190.66796875\n",
      "Epoch 2380, Loss: -10471.4423828125\n",
      "Epoch 2400, Loss: -10757.6171875\n",
      "Epoch 2420, Loss: -11049.25\n",
      "Epoch 2440, Loss: -11346.373046875\n",
      "Epoch 2460, Loss: -11649.0654296875\n",
      "Epoch 2480, Loss: -11957.3623046875\n",
      "Epoch 2500, Loss: -12271.322265625\n",
      "Epoch 2520, Loss: -12591.0\n",
      "Epoch 2540, Loss: -12916.443359375\n",
      "Epoch 2560, Loss: -13247.6982421875\n",
      "Epoch 2580, Loss: -13584.818359375\n",
      "Epoch 2600, Loss: -13927.845703125\n",
      "Epoch 2620, Loss: -14276.837890625\n",
      "Epoch 2640, Loss: -14631.837890625\n",
      "Epoch 2660, Loss: -14992.884765625\n",
      "Epoch 2680, Loss: -15360.029296875\n",
      "Epoch 2700, Loss: -15733.3154296875\n",
      "Epoch 2720, Loss: -16112.7998046875\n",
      "Epoch 2740, Loss: -16498.5078125\n",
      "Epoch 2760, Loss: -16890.484375\n",
      "Epoch 2780, Loss: -17288.779296875\n",
      "Epoch 2800, Loss: -17693.41796875\n",
      "Epoch 2820, Loss: -18104.44140625\n",
      "Epoch 2840, Loss: -18521.89453125\n",
      "Epoch 2860, Loss: -18945.794921875\n",
      "Epoch 2880, Loss: -19376.20703125\n",
      "Epoch 2900, Loss: -19813.15625\n",
      "Epoch 2920, Loss: -20256.65625\n",
      "Epoch 2940, Loss: -20706.765625\n",
      "Epoch 2960, Loss: -21163.505859375\n",
      "Epoch 2980, Loss: -21626.90234375\n",
      "Epoch 3000, Loss: -22096.966796875\n",
      "Epoch 3020, Loss: -22573.755859375\n",
      "Epoch 3040, Loss: -23057.279296875\n",
      "Epoch 3060, Loss: -23547.56640625\n",
      "Epoch 3080, Loss: -24044.619140625\n",
      "Epoch 3100, Loss: -24548.484375\n",
      "Epoch 3120, Loss: -25059.171875\n",
      "Epoch 3140, Loss: -25576.71484375\n",
      "Epoch 3160, Loss: -26101.125\n",
      "Epoch 3180, Loss: -26632.431640625\n",
      "Epoch 3200, Loss: -27170.59765625\n",
      "Epoch 3220, Loss: -27715.6875\n",
      "Epoch 3240, Loss: -28267.681640625\n",
      "Epoch 3260, Loss: -28826.599609375\n",
      "Epoch 3280, Loss: -29392.46484375\n",
      "Epoch 3300, Loss: -29965.26953125\n",
      "Epoch 3320, Loss: -30545.03125\n",
      "Epoch 3340, Loss: -31131.740234375\n",
      "Epoch 3360, Loss: -31725.400390625\n",
      "Epoch 3380, Loss: -32326.009765625\n",
      "Epoch 3400, Loss: -32933.5859375\n",
      "Epoch 3420, Loss: -33548.09765625\n",
      "Epoch 3440, Loss: -34169.55859375\n",
      "Epoch 3460, Loss: -34797.9921875\n",
      "Epoch 3480, Loss: -35433.30859375\n",
      "Epoch 3500, Loss: -36075.5703125\n",
      "Epoch 3520, Loss: -36724.765625\n",
      "Epoch 3540, Loss: -37380.8203125\n",
      "Epoch 3560, Loss: -38043.7734375\n",
      "Epoch 3580, Loss: -38713.6171875\n",
      "Epoch 3600, Loss: -39390.25390625\n",
      "Epoch 3620, Loss: -40073.7265625\n",
      "Epoch 3640, Loss: -40764.015625\n",
      "Epoch 3660, Loss: -41461.1015625\n",
      "Epoch 3680, Loss: -42164.94921875\n",
      "Epoch 3700, Loss: -42875.5390625\n",
      "Epoch 3720, Loss: -43592.796875\n",
      "Epoch 3740, Loss: -44316.73046875\n",
      "Epoch 3760, Loss: -45047.28125\n",
      "Epoch 3780, Loss: -45784.4453125\n",
      "Epoch 3800, Loss: -46528.16015625\n",
      "Epoch 3820, Loss: -47278.40625\n",
      "Epoch 3840, Loss: -48035.13671875\n",
      "Epoch 3860, Loss: -48798.2890625\n",
      "Epoch 3880, Loss: -49567.8515625\n",
      "Epoch 3900, Loss: -50343.73046875\n",
      "Epoch 3920, Loss: -51125.88671875\n",
      "Epoch 3940, Loss: -51914.328125\n",
      "Epoch 3960, Loss: -52708.9375\n",
      "Epoch 3980, Loss: -53509.6953125\n",
      "Epoch 4000, Loss: -54316.54296875\n",
      "Epoch 4020, Loss: -55129.3984375\n",
      "Epoch 4040, Loss: -55948.15625\n",
      "Epoch 4060, Loss: -56772.8515625\n",
      "Epoch 4080, Loss: -57603.3828125\n",
      "Epoch 4100, Loss: -58439.67578125\n",
      "Epoch 4120, Loss: -59281.63671875\n",
      "Epoch 4140, Loss: -60129.2421875\n",
      "Epoch 4160, Loss: -60982.3984375\n",
      "Epoch 4180, Loss: -61841.0625\n",
      "Epoch 4200, Loss: -62705.08203125\n",
      "Epoch 4220, Loss: -63574.4140625\n",
      "Epoch 4240, Loss: -64448.9453125\n",
      "Epoch 4260, Loss: -65328.66796875\n",
      "Epoch 4280, Loss: -66213.421875\n",
      "Epoch 4300, Loss: -67103.125\n",
      "Epoch 4320, Loss: -67997.7421875\n",
      "Epoch 4340, Loss: -68897.1328125\n",
      "Epoch 4360, Loss: -69801.203125\n",
      "Epoch 4380, Loss: -70709.9140625\n",
      "Epoch 4400, Loss: -71623.078125\n",
      "Epoch 4420, Loss: -72540.640625\n",
      "Epoch 4440, Loss: -73462.484375\n",
      "Epoch 4460, Loss: -74388.5\n",
      "Epoch 4480, Loss: -75318.5859375\n",
      "Epoch 4500, Loss: -76252.6484375\n",
      "Epoch 4520, Loss: -77190.484375\n",
      "Epoch 4540, Loss: -78132.1171875\n",
      "Epoch 4560, Loss: -79077.3671875\n",
      "Epoch 4580, Loss: -80026.109375\n",
      "Epoch 4600, Loss: -80978.2109375\n",
      "Epoch 4620, Loss: -81933.5625\n",
      "Epoch 4640, Loss: -82891.984375\n",
      "Epoch 4660, Loss: -83853.4609375\n",
      "Epoch 4680, Loss: -84817.7734375\n",
      "Epoch 4700, Loss: -85784.8359375\n",
      "Epoch 4720, Loss: -86754.4296875\n",
      "Epoch 4740, Loss: -87726.5546875\n",
      "Epoch 4760, Loss: -88700.9609375\n",
      "Epoch 4780, Loss: -89677.5390625\n",
      "Epoch 4800, Loss: -90656.1796875\n",
      "Epoch 4820, Loss: -91636.703125\n",
      "Epoch 4840, Loss: -92618.953125\n",
      "Epoch 4860, Loss: -93602.796875\n",
      "Epoch 4880, Loss: -94588.0859375\n",
      "Epoch 4900, Loss: -95574.6640625\n",
      "Epoch 4920, Loss: -96562.3515625\n",
      "Epoch 4940, Loss: -97551.015625\n",
      "Epoch 4960, Loss: -98540.4765625\n",
      "Epoch 4980, Loss: -99530.5625\n",
      "weights of  A01 :  [[array([[ 0.73231167,  0.14069353, -0.3501956 , -0.4214986 ],\n",
      "       [-0.37709892, -1.4570383 ,  1.9496199 ,  0.26253167],\n",
      "       [-1.8728136 , -0.54187804,  2.347369  ,  0.04165108],\n",
      "       [-0.22085273, -0.7903529 ,  0.26773596,  0.6587716 ],\n",
      "       [ 1.3231485 ,  1.3937671 , -2.3328347 , -0.15834343]],\n",
      "      dtype=float32), array([-0.2592058, -1.7200282,  3.2696128,  0.5509717], dtype=float32)]]\n",
      "loss of  A01 :  0.024658203\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67        15\n",
      "           1       0.62      0.67      0.65        15\n",
      "           2       0.93      0.87      0.90        15\n",
      "           3       0.73      0.73      0.73        15\n",
      "\n",
      "    accuracy                           0.73        60\n",
      "   macro avg       0.74      0.73      0.74        60\n",
      "weighted avg       0.74      0.73      0.74        60\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[10  4  0  1]\n",
      " [ 4 10  0  1]\n",
      " [ 0  0 13  2]\n",
      " [ 1  2  1 11]]\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.60      0.58        57\n",
      "           1       0.64      0.68      0.66        57\n",
      "           2       0.60      0.49      0.54        57\n",
      "           3       0.53      0.56      0.55        57\n",
      "\n",
      "    accuracy                           0.58       228\n",
      "   macro avg       0.58      0.58      0.58       228\n",
      "weighted avg       0.58      0.58      0.58       228\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[34 19  0  4]\n",
      " [16 39  1  1]\n",
      " [ 6  0 28 23]\n",
      " [ 4  3 18 32]]\n"
     ]
    }
   ],
   "source": [
    "# Custom loss function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "class CustomLossLL2(tf.keras.losses.Loss):\n",
    "    def __init__(self, lambda_t, model, mu, sigma_TL):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "        self.mu = tf.convert_to_tensor(mu, dtype=tf.float32)\n",
    "        self.sigma_TL = tf.convert_to_tensor(sigma_TL, dtype=tf.float32)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        wt = self.get_weights_from_model()\n",
    "        reg_term = self.regularization_term(wt)\n",
    "\n",
    "        return ce_loss + (self.lambda_t * tf.linalg.matmul(reg_term, wt))\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        # return tf.concat([tf.reshape(w, [-1]) for w in model_weights], axis=0)\n",
    "        return model_weights\n",
    "\n",
    "    def regularization_term(self, wt):\n",
    "        diff = wt - self.mu\n",
    "        reg_term = 0.5 * tf.linalg.matmul(tf.linalg.matmul(tf.linalg.inv(self.sigma_TL), diff[0]), tf.transpose(diff[0]))\n",
    "        reg_term += 0.5 * tf.math.log(tf.linalg.det(self.sigma_TL))\n",
    "        return reg_term\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True) # Perform a forward pass and compute the predictions\n",
    "        loss = custom_loss(y, y_pred) # Compute the custom loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_weight_LL2(X_train, y_train, lambd, mu, sigma_TL, num_tier=10, learning_rate = 0.01):\n",
    "    n_classes = np.unique(y_train).size\n",
    "    y_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=n_classes)\n",
    "\n",
    "    lambda_t = lambd  # Regularization parameter\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_classes, input_shape=(X_train.shape[1],), activation='softmax')  # Adjust input_shape to match the number of features in X\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate)\n",
    "    custom_loss = CustomLossLL2(lambda_t, model, mu, sigma_TL)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = num_tier\n",
    "    lowest_loss = float('inf')\n",
    "    best_weights = None\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X_train, y_one_hot, custom_loss)\n",
    "        loss_value = loss.numpy()\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        if (abs(loss_value) < lowest_loss):\n",
    "            lowest_loss = abs(loss_value)\n",
    "            best_model = model\n",
    "            best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "    return best_model, best_weights, lowest_loss\n",
    "\n",
    "def GetConfusionMatrix(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_prob = model.predict(X_train)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "    print(\"Classification TRAIN DATA \\n=======================\")\n",
    "    print(classification_report(y_true= y_train, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true= y_train, y_pred=y_pred))\n",
    "\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    print(\"Classification TEST DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "\n",
    "def tgt_test_wLTL(data, target_subjects ,condition):\n",
    "        tgt_data = target_subjects + \"_test\"\n",
    "\n",
    "        if condition == \"noEA\":\n",
    "            X = data[target_subjects]['Raw_csp']\n",
    "            y = data[target_subjects]['Raw_csp_label']\n",
    "            X_test = data[tgt_data]['Raw_csp']\n",
    "            y_test = data[tgt_data]['Raw_csp_label']\n",
    "            store_ws = 'wt_Raw'\n",
    "\n",
    "        else:\n",
    "            X = data[target_subjects]['EA_csp']\n",
    "            y = data[target_subjects]['EA_csp_label']\n",
    "            X_test = data[tgt_data]['EA_csp']\n",
    "            y_test = data[tgt_data]['EA_csp_label']\n",
    "            store_ws = 'wt_EA'\n",
    "\n",
    "        mu = data[target_subjects]['mu_ws']\n",
    "        sigma_TL = data[target_subjects]['Sigma_TL']\n",
    "\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "        \n",
    "        model, weights, loss = train_weight_LL2(X_train=X_train, y_train=y_train, mu =mu, sigma_TL=sigma_TL, lambd= 0.1, num_tier=5000, learning_rate= 0.01)\n",
    "        print(\"weights of \", str(target_subjects), \": \", weights)\n",
    "        print(\"loss of \", str(target_subjects), \": \", loss)\n",
    "        data[target_subjects][store_ws] = weights\n",
    "\n",
    "        GetConfusionMatrix(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "tgt_test_wLTL(CSP2D_Epoch, target_subjects= target_data_0 ,condition = condition_wLTL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
