{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCI competition 2a\n",
    "\n",
    "C:\\Users\\pipo_\\Downloads\\desc_2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from mne.datasets import eegbci\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from mne.channels import make_standard_montage\n",
    "from scipy import signal\n",
    "from scipy.linalg import sqrtm, inv \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.utils import shuffle\n",
    "from mne.decoding import CSP\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit,StratifiedKFold ,cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pyriemann.utils.distance import distance_riemann\n",
    "from scipy.linalg import logm, expm\n",
    "\n",
    "\n",
    "target_data_0 = \"A06\"\n",
    "calibrate_size = 120\n",
    "alignmentMethod = \"EA\"\n",
    "\n",
    "condition_wLTL= \"EA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRawEDF(target_subjects=\"pipo\"):\n",
    "    EEG_data = {}\n",
    "\n",
    "    if target_subjects == \"all\":\n",
    "        target_subjects = [\"A01\",\"A02\",\"A03\",\"A04\",\"A05\",\"A06\",\"A07\",\"A08\",\"A09\"]\n",
    "\n",
    "    path = \"D:\\\\BCI_Competition_2a\\\\Training\\\\\"\n",
    "    list_dir = listdir(path)\n",
    "\n",
    "    for i in range (0,len(target_subjects)):\n",
    "\n",
    "        raw_gdf = mne.io.read_raw_gdf(path+list_dir[i],preload = False, verbose = False)\n",
    "\n",
    "        EEG_data[target_subjects[i]] = {\"Raw_data\": raw_gdf.copy()}\n",
    "\n",
    "    print(f\"Successful to create Data of {target_subjects}\")\n",
    "\n",
    "    return EEG_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful to create Data of ['A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07', 'A08', 'A09']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "c:\\Python311\\Lib\\contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    }
   ],
   "source": [
    "Raw_data = GetRawEDF(target_subjects=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut,highcut,fs,order):\n",
    "    nyq = 0.5*fs\n",
    "    low = lowcut/nyq\n",
    "    high = highcut/nyq\n",
    "    b,a = signal.butter(order,[low,high],'bandpass')\n",
    "    return b,a\n",
    "\n",
    "def butter_bandpass_filter(data,lowcut = 6,highcut = 30, order = 4):\n",
    "    b,a = butter_bandpass(lowcut,highcut,250,order)\n",
    "    y = signal.filtfilt(b,a,data,axis=2)\n",
    "    return y\n",
    "\n",
    "def GetEpoch(EEG_data, tmin=-2.0, tmax=6.0, crop=(0,2),baseline = (-0.5,0.0), trial_removal_th = 100):\n",
    "\n",
    "    EEG_epoch = {}\n",
    "\n",
    "    for key_subs in EEG_data:\n",
    "        raw_edf = EEG_data[key_subs][\"Raw_data\"]\n",
    "\n",
    "        events, event_dict = mne.events_from_annotations(raw_edf)\n",
    "\n",
    "        if key_subs == 'A04':\n",
    "            event_dict =  {'769': 5,\n",
    "            '770': 6,\n",
    "            '772': 8,\n",
    "            '771': 7}\n",
    "            mapping = {5: 0, 6: 1, 8: 2, 7: 3}\n",
    "            selected_events = events[np.isin(events[:, 2], [5, 6, 7, 8])]\n",
    "\n",
    "        else:\n",
    "            event_dict =  {'769': 7,\n",
    "            '770': 8,\n",
    "            '772': 10,\n",
    "            '771': 9}\n",
    "            mapping = {7: 0, 8: 1, 10: 2, 9: 3}\n",
    "            selected_events = events[np.isin(events[:, 2], [7, 8, 9, 10])]\n",
    "\n",
    "        Epochs = mne.Epochs(raw_edf, selected_events, \n",
    "            tmin= tmin,  \n",
    "            tmax= tmax,    \n",
    "            event_id=event_dict,\n",
    "            preload = True,\n",
    "            event_repeated='drop',\n",
    "            baseline=baseline,\n",
    "            verbose=False\n",
    "            )\n",
    "        \n",
    "        selected_ch = ['EEG-Fz', 'EEG-Cz', 'EEG-C3', 'EEG-C4', 'EEG-Pz']\n",
    "        \n",
    "        EEG_epoch[key_subs] =  {\"Raw_Epoch\": Epochs.copy().pick(selected_ch).crop(tmin= crop[0], tmax= crop[1])}\n",
    "\n",
    "        train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5 \n",
    "\n",
    "        labels = EEG_epoch[key_subs][\"Raw_Epoch\"].copy().events[:,-1]\n",
    "\n",
    "        labels = np.vectorize(mapping.get)(labels)\n",
    "\n",
    "        outlier_trial = []\n",
    "        for ii in range(0,train_data.shape[0]):\n",
    "            if train_data[ii].max() > trial_removal_th or train_data[ii].min() < -trial_removal_th:\n",
    "                outlier_trial.append(ii)\n",
    "                print(key_subs,train_data[ii].min(), ii)\n",
    "                print(key_subs,train_data[ii].max(), ii)\n",
    "\n",
    "        EEG_epoch[key_subs]['Raw_Epoch'] = np.delete(train_data, outlier_trial, axis = 0)\n",
    "        EEG_epoch[key_subs]['label'] = np.delete(labels, outlier_trial)\n",
    "\n",
    "        filtered_data = butter_bandpass_filter(EEG_epoch[key_subs]['Raw_Epoch'], lowcut= 6, highcut= 32)\n",
    "        EEG_epoch[key_subs]['Raw_Epoch'] = filtered_data\n",
    "\n",
    "    return EEG_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n",
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_14924\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_14924\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_14924\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_14924\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_14924\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_14924\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_14924\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_14924\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_14924\\1188733013.py:52: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    }
   ],
   "source": [
    "EEG_Epochs = GetEpoch(Raw_data ,tmin= -1.0, tmax= 4.0, crop = (0,4) ,baseline= (-0.5,0.0), trial_removal_th = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "LDA only Cross-validation scores: 0.37357142857142855\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.46      0.41        50\n",
      "           1       0.45      0.43      0.44        51\n",
      "           2       0.49      0.38      0.43        50\n",
      "           3       0.36      0.36      0.36        50\n",
      "\n",
      "    accuracy                           0.41       201\n",
      "   macro avg       0.42      0.41      0.41       201\n",
      "weighted avg       0.42      0.41      0.41       201\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[23 12  5 10]\n",
      " [12 22  6 11]\n",
      " [15  5 19 11]\n",
      " [13 10  9 18]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.36      0.36        22\n",
      "           1       0.38      0.48      0.43        21\n",
      "           2       0.32      0.27      0.29        22\n",
      "           3       0.53      0.45      0.49        22\n",
      "\n",
      "    accuracy                           0.39        87\n",
      "   macro avg       0.39      0.39      0.39        87\n",
      "weighted avg       0.39      0.39      0.39        87\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[ 8  6  4  4]\n",
      " [ 5 10  3  3]\n",
      " [ 7  7  6  2]\n",
      " [ 3  3  6 10]]\n"
     ]
    }
   ],
   "source": [
    "def GetConfusionMatrix(models, X_train, X_test, y_train, y_test):\n",
    "    y_pred = models.predict(X_train)\n",
    "    print(\"Classification TRAIN DATA \\n=======================\")\n",
    "    print(classification_report(y_true= y_train, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true= y_train, y_pred=y_pred))\n",
    "\n",
    "    y_pred = models.predict(X_test)\n",
    "    print(\"Classification TEST DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "    \n",
    "\n",
    "label_target = EEG_Epochs[target_data_0]['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(EEG_Epochs[target_data_0]['Raw_Epoch'], label_target, test_size=0.3, random_state = 42, stratify=label_target)\n",
    "\n",
    "csp = CSP(n_components = 5, reg=None, log=None, rank= 'info')\n",
    "csp.fit(x_train, y_train)   \n",
    "\n",
    "x_train = csp.transform(x_train)\n",
    "x_test = csp.transform(x_test)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "score = cross_val_score(lda, x_train, y_train, cv= 10)\n",
    "print(\"LDA only Cross-validation scores:\", np.mean(score))\n",
    "lda.fit(x_train, y_train)\n",
    "\n",
    "GetConfusionMatrix(lda, x_train, x_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BCIAllFunction import BCIFuntions\n",
    "\n",
    "calibrate_size = calibrate_size / EEG_Epochs[target_data_0]['Raw_Epoch'].shape[0]\n",
    "AllBCIClass = BCIFuntions(numclass = 4, frequency = 250, ch_pick = ['Fz','C3', 'Cz','C4','Pz'])\n",
    "\n",
    "\n",
    "if alignmentMethod == \"LA\":\n",
    "    AllBCIClass.ComputeLA(EEG_Epochs, target_subject= target_data_0, calibrate_size=calibrate_size)\n",
    "    if calibrate_size != 0:\n",
    "        target_data = target_data_0 + \"_test\"\n",
    "    count = 0\n",
    "\n",
    "    for index in range(len(EEG_Epochs[target_data]['KMediod_label'])):\n",
    "        if EEG_Epochs[target_data]['label'][index] == EEG_Epochs[target_data]['KMediod_label'][index]:\n",
    "            count += 1\n",
    "    print(count/len(EEG_Epochs[target_data]['KMediod_label']) * 100)\n",
    "    \n",
    "else:\n",
    "    AllBCIClass.GetRawSet_ComputeEA(EEG_Epochs, target_subject= target_data_0, calibrate_size=calibrate_size)\n",
    "    if calibrate_size != 0:\n",
    "        target_data = target_data_0 + \"_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Raw_Epoch', 'label', 'Raw_left', 'Raw_right', 'Raw_non', 'Raw_feet', 'EA_left', 'EA_right', 'EA_feet', 'EA_non', 'EA_Epoch'])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EEG_Epochs['A01'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "LDA only Cross-validation scores: 0.3878022650749923\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.47      0.42       606\n",
      "           1       0.40      0.51      0.45       606\n",
      "           2       0.40      0.29      0.33       606\n",
      "           3       0.41      0.31      0.35       606\n",
      "\n",
      "    accuracy                           0.39      2424\n",
      "   macro avg       0.40      0.39      0.39      2424\n",
      "weighted avg       0.40      0.39      0.39      2424\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[286 161  88  71]\n",
      " [152 307  74  73]\n",
      " [158 149 173 126]\n",
      " [166 152 100 188]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.10      0.16        42\n",
      "           1       0.71      0.24      0.36        42\n",
      "           2       0.41      0.31      0.35        42\n",
      "           3       0.25      0.67      0.36        42\n",
      "\n",
      "    accuracy                           0.33       168\n",
      "   macro avg       0.45      0.33      0.31       168\n",
      "weighted avg       0.45      0.33      0.31       168\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[ 4  0  3 35]\n",
      " [ 3 10  4 25]\n",
      " [ 1  3 13 25]\n",
      " [ 1  1 12 28]]\n"
     ]
    }
   ],
   "source": [
    "AllBCIClass.classifyCSP_LDA(EEG_Epochs, target_subjects= target_data, condition = \"noEA\") #target_sub is used for target_data otherwise are source_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "LDA only Cross-validation scores: 0.4740043532972826\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.57      0.54       606\n",
      "           1       0.51      0.48      0.49       606\n",
      "           2       0.50      0.54      0.52       606\n",
      "           3       0.39      0.32      0.35       606\n",
      "\n",
      "    accuracy                           0.48      2424\n",
      "   macro avg       0.48      0.48      0.48      2424\n",
      "weighted avg       0.48      0.48      0.48      2424\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[346 122  60  78]\n",
      " [134 292  77 103]\n",
      " [ 84  60 330 132]\n",
      " [116 104 190 196]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.24      0.28        42\n",
      "           1       0.44      0.48      0.46        42\n",
      "           2       0.31      0.38      0.34        42\n",
      "           3       0.26      0.26      0.26        42\n",
      "\n",
      "    accuracy                           0.34       168\n",
      "   macro avg       0.34      0.34      0.34       168\n",
      "weighted avg       0.34      0.34      0.34       168\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[10  9  7 16]\n",
      " [ 6 20 11  5]\n",
      " [ 9  6 16 11]\n",
      " [ 4 10 17 11]]\n"
     ]
    }
   ],
   "source": [
    "AllBCIClass.classifyCSP_LDA(EEG_Epochs, target_subjects= target_data, condition = \"EA\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WLTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "CSP2D_Epoch = AllBCIClass.computeCSPFeatures(EEG_Epochs, target_subject = target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.892957329750061\n",
      "Epoch 20, Loss: 1.7133259773254395\n",
      "Epoch 40, Loss: 1.6421326398849487\n",
      "Epoch 60, Loss: 1.5849602222442627\n",
      "Epoch 80, Loss: 1.5396479368209839\n",
      "Epoch 100, Loss: 1.5084420442581177\n",
      "Epoch 120, Loss: 1.4916439056396484\n",
      "Epoch 140, Loss: 1.4888875484466553\n",
      "Epoch 160, Loss: 1.4994882345199585\n",
      "Epoch 180, Loss: 1.5225889682769775\n",
      "Epoch 200, Loss: 1.557239055633545\n",
      "Epoch 220, Loss: 1.602473497390747\n",
      "Epoch 240, Loss: 1.6573600769042969\n",
      "Epoch 260, Loss: 1.7210259437561035\n",
      "Epoch 280, Loss: 1.7926750183105469\n",
      "Epoch 300, Loss: 1.8715918064117432\n",
      "Epoch 320, Loss: 1.957141399383545\n",
      "Epoch 340, Loss: 2.0487654209136963\n",
      "Epoch 360, Loss: 2.1459765434265137\n",
      "Epoch 380, Loss: 2.248350143432617\n",
      "Epoch 400, Loss: 2.355515480041504\n",
      "Epoch 420, Loss: 2.467151165008545\n",
      "Epoch 440, Loss: 2.5829758644104004\n",
      "Epoch 460, Loss: 2.702742338180542\n",
      "Epoch 480, Loss: 2.8262338638305664\n",
      "Epoch 500, Loss: 2.9532551765441895\n",
      "Epoch 520, Loss: 3.083634853363037\n",
      "Epoch 540, Loss: 3.217214822769165\n",
      "Epoch 560, Loss: 3.3538527488708496\n",
      "Epoch 580, Loss: 3.493417739868164\n",
      "Epoch 600, Loss: 3.635788679122925\n",
      "Epoch 620, Loss: 3.780850648880005\n",
      "Epoch 640, Loss: 3.9284982681274414\n",
      "Epoch 660, Loss: 4.078629016876221\n",
      "Epoch 680, Loss: 4.231148719787598\n",
      "Epoch 700, Loss: 4.385965347290039\n",
      "Epoch 720, Loss: 4.5429911613464355\n",
      "Epoch 740, Loss: 4.702141761779785\n",
      "Epoch 760, Loss: 4.863338947296143\n",
      "Epoch 780, Loss: 5.026503086090088\n",
      "Epoch 800, Loss: 5.191560745239258\n",
      "Epoch 820, Loss: 5.358438968658447\n",
      "Epoch 840, Loss: 5.527069091796875\n",
      "Epoch 860, Loss: 5.697385787963867\n",
      "Epoch 880, Loss: 5.869322776794434\n",
      "Epoch 900, Loss: 6.042819023132324\n",
      "Epoch 920, Loss: 6.217815399169922\n",
      "Epoch 940, Loss: 6.394253730773926\n",
      "Epoch 960, Loss: 6.572079181671143\n",
      "Epoch 980, Loss: 6.751238822937012\n",
      "weights of  A01 :  [[array([[ 0.37626874, -0.22602125,  0.40653178,  0.32383004],\n",
      "       [-0.00934737, -0.23462003,  0.603927  ,  0.06000739],\n",
      "       [-0.20211251,  0.5628897 ,  0.0115378 ,  0.4999704 ],\n",
      "       [ 0.52068985, -0.08083517,  0.282135  ,  0.3722841 ],\n",
      "       [ 0.11550748,  0.42685124, -0.03572606, -0.17228666]],\n",
      "      dtype=float32), array([-0.30541748, -0.20741422,  0.6483062 ,  0.4685055 ], dtype=float32)]]\n",
      "Lowest loss of  A01 :  1.4882821\n",
      "Epoch 0, Loss: 3.933681011199951\n",
      "Epoch 20, Loss: 2.735593318939209\n",
      "Epoch 40, Loss: 2.134392738342285\n",
      "Epoch 60, Loss: 1.9491147994995117\n",
      "Epoch 80, Loss: 1.934568166732788\n",
      "Epoch 100, Loss: 1.9361162185668945\n",
      "Epoch 120, Loss: 1.9350364208221436\n",
      "Epoch 140, Loss: 1.9349708557128906\n",
      "Epoch 160, Loss: 1.935298204421997\n",
      "Epoch 180, Loss: 1.9360053539276123\n",
      "Epoch 200, Loss: 1.9369783401489258\n",
      "Epoch 220, Loss: 1.9382625818252563\n",
      "Epoch 240, Loss: 1.9399303197860718\n",
      "Epoch 260, Loss: 1.9420068264007568\n",
      "Epoch 280, Loss: 1.9445264339447021\n",
      "Epoch 300, Loss: 1.9475210905075073\n",
      "Epoch 320, Loss: 1.9510231018066406\n",
      "Epoch 340, Loss: 1.9550622701644897\n",
      "Epoch 360, Loss: 1.9596678018569946\n",
      "Epoch 380, Loss: 1.9648668766021729\n",
      "Epoch 400, Loss: 1.9706854820251465\n",
      "Epoch 420, Loss: 1.9771482944488525\n",
      "Epoch 440, Loss: 1.9842785596847534\n",
      "Epoch 460, Loss: 1.992098093032837\n",
      "Epoch 480, Loss: 2.000627040863037\n",
      "Epoch 500, Loss: 2.0098841190338135\n",
      "Epoch 520, Loss: 2.0198864936828613\n",
      "Epoch 540, Loss: 2.0306508541107178\n",
      "Epoch 560, Loss: 2.0421905517578125\n",
      "Epoch 580, Loss: 2.0545194149017334\n",
      "Epoch 600, Loss: 2.067648410797119\n",
      "Epoch 620, Loss: 2.0815885066986084\n",
      "Epoch 640, Loss: 2.0963478088378906\n",
      "Epoch 660, Loss: 2.1119344234466553\n",
      "Epoch 680, Loss: 2.12835431098938\n",
      "Epoch 700, Loss: 2.1456127166748047\n",
      "Epoch 720, Loss: 2.163712739944458\n",
      "Epoch 740, Loss: 2.1826577186584473\n",
      "Epoch 760, Loss: 2.202449321746826\n",
      "Epoch 780, Loss: 2.2230873107910156\n",
      "Epoch 800, Loss: 2.244570732116699\n",
      "Epoch 820, Loss: 2.2668983936309814\n",
      "Epoch 840, Loss: 2.2900679111480713\n",
      "Epoch 860, Loss: 2.314075231552124\n",
      "Epoch 880, Loss: 2.338916301727295\n",
      "Epoch 900, Loss: 2.364586114883423\n",
      "Epoch 920, Loss: 2.3910775184631348\n",
      "Epoch 940, Loss: 2.4183850288391113\n",
      "Epoch 960, Loss: 2.446500778198242\n",
      "Epoch 980, Loss: 2.475416660308838\n",
      "weights of  A02 :  [[array([[ 0.4152579 , -0.83192927, -0.6349174 , -0.17578167],\n",
      "       [ 0.12533943,  0.6864161 ,  0.37579003, -0.4783218 ],\n",
      "       [-0.99229974,  0.3430387 , -0.26928917, -0.4623012 ],\n",
      "       [-0.17576128, -0.83932036, -0.87724805,  0.1266783 ],\n",
      "       [-0.15118615, -0.7605656 ,  0.082582  , -0.01620996]],\n",
      "      dtype=float32), array([ 0.29862288,  0.05995681,  0.21971053, -0.23111756], dtype=float32)]]\n",
      "Lowest loss of  A02 :  1.9343584\n",
      "Epoch 0, Loss: 1.922200322151184\n",
      "Epoch 20, Loss: 1.781599760055542\n",
      "Epoch 40, Loss: 1.7597906589508057\n",
      "Epoch 60, Loss: 1.743093729019165\n",
      "Epoch 80, Loss: 1.7321887016296387\n",
      "Epoch 100, Loss: 1.7269428968429565\n",
      "Epoch 120, Loss: 1.728541612625122\n",
      "Epoch 140, Loss: 1.7372959852218628\n",
      "Epoch 160, Loss: 1.7534159421920776\n",
      "Epoch 180, Loss: 1.7769473791122437\n",
      "Epoch 200, Loss: 1.8078334331512451\n",
      "Epoch 220, Loss: 1.8459386825561523\n",
      "Epoch 240, Loss: 1.8910678625106812\n",
      "Epoch 260, Loss: 1.9429829120635986\n",
      "Epoch 280, Loss: 2.0014166831970215\n",
      "Epoch 300, Loss: 2.066082239151001\n",
      "Epoch 320, Loss: 2.13667893409729\n",
      "Epoch 340, Loss: 2.212900400161743\n",
      "Epoch 360, Loss: 2.2944376468658447\n",
      "Epoch 380, Loss: 2.380983829498291\n",
      "Epoch 400, Loss: 2.472235918045044\n",
      "Epoch 420, Loss: 2.5678975582122803\n",
      "Epoch 440, Loss: 2.6676812171936035\n",
      "Epoch 460, Loss: 2.7713069915771484\n",
      "Epoch 480, Loss: 2.878504753112793\n",
      "Epoch 500, Loss: 2.989017963409424\n",
      "Epoch 520, Loss: 3.1025969982147217\n",
      "Epoch 540, Loss: 3.2190041542053223\n",
      "Epoch 560, Loss: 3.3380136489868164\n",
      "Epoch 580, Loss: 3.459409236907959\n",
      "Epoch 600, Loss: 3.5829856395721436\n",
      "Epoch 620, Loss: 3.708547830581665\n",
      "Epoch 640, Loss: 3.8359103202819824\n",
      "Epoch 660, Loss: 3.9648971557617188\n",
      "Epoch 680, Loss: 4.095341205596924\n",
      "Epoch 700, Loss: 4.227087497711182\n",
      "Epoch 720, Loss: 4.3599853515625\n",
      "Epoch 740, Loss: 4.493893146514893\n",
      "Epoch 760, Loss: 4.628680229187012\n",
      "Epoch 780, Loss: 4.764219760894775\n",
      "Epoch 800, Loss: 4.900393486022949\n",
      "Epoch 820, Loss: 5.0370917320251465\n",
      "Epoch 840, Loss: 5.1742072105407715\n",
      "Epoch 860, Loss: 5.311643123626709\n",
      "Epoch 880, Loss: 5.449305534362793\n",
      "Epoch 900, Loss: 5.587106704711914\n",
      "Epoch 920, Loss: 5.724967002868652\n",
      "Epoch 940, Loss: 5.86280632019043\n",
      "Epoch 960, Loss: 6.000555038452148\n",
      "Epoch 980, Loss: 6.138144016265869\n",
      "weights of  A03 :  [[array([[ 0.18277845, -0.7918089 ,  0.4495992 , -0.2550702 ],\n",
      "       [-0.21611334,  0.89068717,  0.36532634,  0.15576744],\n",
      "       [-0.6505711 , -0.48753625, -0.10520098, -0.25348657],\n",
      "       [-0.07094152,  0.37984884, -0.28496572,  0.6651259 ],\n",
      "       [ 0.02386622, -0.5296035 , -0.93358517, -0.5935927 ]],\n",
      "      dtype=float32), array([-0.3242018 , -0.34914488,  0.27624193, -0.02829062], dtype=float32)]]\n",
      "Lowest loss of  A03 :  1.7267022\n",
      "Epoch 0, Loss: 1.9528114795684814\n",
      "Epoch 20, Loss: 1.6526154279708862\n",
      "Epoch 40, Loss: 1.6100919246673584\n",
      "Epoch 60, Loss: 1.5778032541275024\n",
      "Epoch 80, Loss: 1.5606595277786255\n",
      "Epoch 100, Loss: 1.5553854703903198\n",
      "Epoch 120, Loss: 1.56134831905365\n",
      "Epoch 140, Loss: 1.5776495933532715\n",
      "Epoch 160, Loss: 1.603432059288025\n",
      "Epoch 180, Loss: 1.637774109840393\n",
      "Epoch 200, Loss: 1.6798447370529175\n",
      "Epoch 220, Loss: 1.7288696765899658\n",
      "Epoch 240, Loss: 1.7841483354568481\n",
      "Epoch 260, Loss: 1.8450393676757812\n",
      "Epoch 280, Loss: 1.9109528064727783\n",
      "Epoch 300, Loss: 1.981339931488037\n",
      "Epoch 320, Loss: 2.055687189102173\n",
      "Epoch 340, Loss: 2.1335105895996094\n",
      "Epoch 360, Loss: 2.214353322982788\n",
      "Epoch 380, Loss: 2.2977828979492188\n",
      "Epoch 400, Loss: 2.383392810821533\n",
      "Epoch 420, Loss: 2.4707984924316406\n",
      "Epoch 440, Loss: 2.5596401691436768\n",
      "Epoch 460, Loss: 2.649582862854004\n",
      "Epoch 480, Loss: 2.7403125762939453\n",
      "Epoch 500, Loss: 2.8315415382385254\n",
      "Epoch 520, Loss: 2.9230031967163086\n",
      "Epoch 540, Loss: 3.014451503753662\n",
      "Epoch 560, Loss: 3.1056652069091797\n",
      "Epoch 580, Loss: 3.196441411972046\n",
      "Epoch 600, Loss: 3.286599636077881\n",
      "Epoch 620, Loss: 3.3759753704071045\n",
      "Epoch 640, Loss: 3.464423656463623\n",
      "Epoch 660, Loss: 3.5518176555633545\n",
      "Epoch 680, Loss: 3.638044834136963\n",
      "Epoch 700, Loss: 3.723008632659912\n",
      "Epoch 720, Loss: 3.806626558303833\n",
      "Epoch 740, Loss: 3.888828754425049\n",
      "Epoch 760, Loss: 3.969557285308838\n",
      "Epoch 780, Loss: 4.048766136169434\n",
      "Epoch 800, Loss: 4.126418590545654\n",
      "Epoch 820, Loss: 4.202486515045166\n",
      "Epoch 840, Loss: 4.276952743530273\n",
      "Epoch 860, Loss: 4.349803924560547\n",
      "Epoch 880, Loss: 4.421035289764404\n",
      "Epoch 900, Loss: 4.49064826965332\n",
      "Epoch 920, Loss: 4.558649063110352\n",
      "Epoch 940, Loss: 4.625048637390137\n",
      "Epoch 960, Loss: 4.6898603439331055\n",
      "Epoch 980, Loss: 4.7531023025512695\n",
      "weights of  A04 :  [[array([[ 0.2196549 , -0.26340497, -0.12310302,  0.19957028],\n",
      "       [-0.08472478,  0.41479564, -0.5075074 ,  0.00660634],\n",
      "       [-0.15877895, -0.41314203, -0.12555996, -0.2190524 ],\n",
      "       [-0.48694766,  0.4041711 ,  0.51286876, -0.28734714],\n",
      "       [ 0.02831826, -0.01602317,  0.50218064,  0.28267062]],\n",
      "      dtype=float32), array([-0.40068677, -0.12473945,  0.15034702,  0.25972694], dtype=float32)]]\n",
      "Lowest loss of  A04 :  1.5553744\n",
      "Epoch 0, Loss: 5.278694152832031\n",
      "Epoch 20, Loss: 3.832581043243408\n",
      "Epoch 40, Loss: 2.853247880935669\n",
      "Epoch 60, Loss: 2.186370849609375\n",
      "Epoch 80, Loss: 1.875193476676941\n",
      "Epoch 100, Loss: 1.7959997653961182\n",
      "Epoch 120, Loss: 1.7843366861343384\n",
      "Epoch 140, Loss: 1.7794373035430908\n",
      "Epoch 160, Loss: 1.7743000984191895\n",
      "Epoch 180, Loss: 1.768803358078003\n",
      "Epoch 200, Loss: 1.7630618810653687\n",
      "Epoch 220, Loss: 1.7571775913238525\n",
      "Epoch 240, Loss: 1.7511540651321411\n",
      "Epoch 260, Loss: 1.7450143098831177\n",
      "Epoch 280, Loss: 1.7387901544570923\n",
      "Epoch 300, Loss: 1.7325077056884766\n",
      "Epoch 320, Loss: 1.7261908054351807\n",
      "Epoch 340, Loss: 1.719863772392273\n",
      "Epoch 360, Loss: 1.7135485410690308\n",
      "Epoch 380, Loss: 1.7072664499282837\n",
      "Epoch 400, Loss: 1.7010383605957031\n",
      "Epoch 420, Loss: 1.6948833465576172\n",
      "Epoch 440, Loss: 1.6888200044631958\n",
      "Epoch 460, Loss: 1.6828657388687134\n",
      "Epoch 480, Loss: 1.6770377159118652\n",
      "Epoch 500, Loss: 1.671351671218872\n",
      "Epoch 520, Loss: 1.6658226251602173\n",
      "Epoch 540, Loss: 1.6604652404785156\n",
      "Epoch 560, Loss: 1.6552927494049072\n",
      "Epoch 580, Loss: 1.6503181457519531\n",
      "Epoch 600, Loss: 1.6455533504486084\n",
      "Epoch 620, Loss: 1.6410096883773804\n",
      "Epoch 640, Loss: 1.63669753074646\n",
      "Epoch 660, Loss: 1.632627010345459\n",
      "Epoch 680, Loss: 1.6288068294525146\n",
      "Epoch 700, Loss: 1.6252455711364746\n",
      "Epoch 720, Loss: 1.6219509840011597\n",
      "Epoch 740, Loss: 1.6189299821853638\n",
      "Epoch 760, Loss: 1.6161887645721436\n",
      "Epoch 780, Loss: 1.613733172416687\n",
      "Epoch 800, Loss: 1.6115682125091553\n",
      "Epoch 820, Loss: 1.6096981763839722\n",
      "Epoch 840, Loss: 1.6081271171569824\n",
      "Epoch 860, Loss: 1.6068578958511353\n",
      "Epoch 880, Loss: 1.6058931350708008\n",
      "Epoch 900, Loss: 1.6052353382110596\n",
      "Epoch 920, Loss: 1.6048853397369385\n",
      "Epoch 940, Loss: 1.604844331741333\n",
      "Epoch 960, Loss: 1.6051130294799805\n",
      "Epoch 980, Loss: 1.6056907176971436\n",
      "weights of  A05 :  [[array([[-0.4553381 , -0.21919282,  0.31415272, -0.3067025 ],\n",
      "       [-0.02923135,  0.09324401, -0.5247931 ,  0.15248927],\n",
      "       [-0.45435798,  0.05803283,  0.21590088,  0.21829401],\n",
      "       [-0.19899702, -0.55760175,  0.04243505, -0.2715658 ],\n",
      "       [ 0.8184252 ,  0.3370763 ,  0.19672474,  0.45644018]],\n",
      "      dtype=float32), array([-0.5461696 , -0.04094781,  0.52396756,  0.7835269 ], dtype=float32)]]\n",
      "Lowest loss of  A05 :  1.6048236\n",
      "Epoch 0, Loss: 2.6267783641815186\n",
      "Epoch 20, Loss: 1.9920833110809326\n",
      "Epoch 40, Loss: 1.905681848526001\n",
      "Epoch 60, Loss: 1.9128402471542358\n",
      "Epoch 80, Loss: 1.9232447147369385\n",
      "Epoch 100, Loss: 1.9379602670669556\n",
      "Epoch 120, Loss: 1.956194281578064\n",
      "Epoch 140, Loss: 1.9783270359039307\n",
      "Epoch 160, Loss: 2.003772497177124\n",
      "Epoch 180, Loss: 2.0321693420410156\n",
      "Epoch 200, Loss: 2.0632078647613525\n",
      "Epoch 220, Loss: 2.0965380668640137\n",
      "Epoch 240, Loss: 2.1318557262420654\n",
      "Epoch 260, Loss: 2.1688919067382812\n",
      "Epoch 280, Loss: 2.20741868019104\n",
      "Epoch 300, Loss: 2.247248888015747\n",
      "Epoch 320, Loss: 2.288235664367676\n",
      "Epoch 340, Loss: 2.3302688598632812\n",
      "Epoch 360, Loss: 2.373270273208618\n",
      "Epoch 380, Loss: 2.4171876907348633\n",
      "Epoch 400, Loss: 2.461991310119629\n",
      "Epoch 420, Loss: 2.5076677799224854\n",
      "Epoch 440, Loss: 2.5542149543762207\n",
      "Epoch 460, Loss: 2.6016392707824707\n",
      "Epoch 480, Loss: 2.6499500274658203\n",
      "Epoch 500, Loss: 2.6991586685180664\n",
      "Epoch 520, Loss: 2.7492752075195312\n",
      "Epoch 540, Loss: 2.800307273864746\n",
      "Epoch 560, Loss: 2.85225772857666\n",
      "Epoch 580, Loss: 2.905125141143799\n",
      "Epoch 600, Loss: 2.958902359008789\n",
      "Epoch 620, Loss: 3.013577938079834\n",
      "Epoch 640, Loss: 3.069133758544922\n",
      "Epoch 660, Loss: 3.12554931640625\n",
      "Epoch 680, Loss: 3.1827969551086426\n",
      "Epoch 700, Loss: 3.2408456802368164\n",
      "Epoch 720, Loss: 3.2996625900268555\n",
      "Epoch 740, Loss: 3.3592095375061035\n",
      "Epoch 760, Loss: 3.4194488525390625\n",
      "Epoch 780, Loss: 3.480339527130127\n",
      "Epoch 800, Loss: 3.541839599609375\n",
      "Epoch 820, Loss: 3.6039059162139893\n",
      "Epoch 840, Loss: 3.6664953231811523\n",
      "Epoch 860, Loss: 3.7295665740966797\n",
      "Epoch 880, Loss: 3.793076753616333\n",
      "Epoch 900, Loss: 3.856985330581665\n",
      "Epoch 920, Loss: 3.9212512969970703\n",
      "Epoch 940, Loss: 3.9858360290527344\n",
      "Epoch 960, Loss: 4.050703048706055\n",
      "Epoch 980, Loss: 4.115816593170166\n",
      "weights of  A07 :  [[array([[-0.29140663,  0.67470145,  0.6500353 ,  0.800375  ],\n",
      "       [ 0.2923447 ,  0.6224894 ,  0.48237187, -0.30229   ],\n",
      "       [ 0.572491  ,  0.33503884,  0.29576713,  0.40970394],\n",
      "       [ 0.55223554, -0.3156848 ,  0.42697084, -0.8556546 ],\n",
      "       [ 0.09947059, -0.42489094, -0.6181583 ,  0.5363407 ]],\n",
      "      dtype=float32), array([ 0.15391374,  0.11859433, -0.14305794, -0.00981561], dtype=float32)]]\n",
      "Lowest loss of  A07 :  1.9055507\n",
      "Epoch 0, Loss: 3.366853952407837\n",
      "Epoch 20, Loss: 2.2663698196411133\n",
      "Epoch 40, Loss: 1.8465595245361328\n",
      "Epoch 60, Loss: 1.6925371885299683\n",
      "Epoch 80, Loss: 1.669463872909546\n",
      "Epoch 100, Loss: 1.655576229095459\n",
      "Epoch 120, Loss: 1.6417958736419678\n",
      "Epoch 140, Loss: 1.628355622291565\n",
      "Epoch 160, Loss: 1.615332007408142\n",
      "Epoch 180, Loss: 1.6029376983642578\n",
      "Epoch 200, Loss: 1.5914227962493896\n",
      "Epoch 220, Loss: 1.5810091495513916\n",
      "Epoch 240, Loss: 1.5718903541564941\n",
      "Epoch 260, Loss: 1.5642346143722534\n",
      "Epoch 280, Loss: 1.5581903457641602\n",
      "Epoch 300, Loss: 1.5538861751556396\n",
      "Epoch 320, Loss: 1.5514318943023682\n",
      "Epoch 340, Loss: 1.5509206056594849\n",
      "Epoch 360, Loss: 1.5524286031723022\n",
      "Epoch 380, Loss: 1.5560176372528076\n",
      "Epoch 400, Loss: 1.561734676361084\n",
      "Epoch 420, Loss: 1.5696145296096802\n",
      "Epoch 440, Loss: 1.5796796083450317\n",
      "Epoch 460, Loss: 1.5919413566589355\n",
      "Epoch 480, Loss: 1.6064012050628662\n",
      "Epoch 500, Loss: 1.6230523586273193\n",
      "Epoch 520, Loss: 1.6418787240982056\n",
      "Epoch 540, Loss: 1.662858247756958\n",
      "Epoch 560, Loss: 1.685962200164795\n",
      "Epoch 580, Loss: 1.7111564874649048\n",
      "Epoch 600, Loss: 1.7384021282196045\n",
      "Epoch 620, Loss: 1.7676568031311035\n",
      "Epoch 640, Loss: 1.7988743782043457\n",
      "Epoch 660, Loss: 1.8320062160491943\n",
      "Epoch 680, Loss: 1.8670017719268799\n",
      "Epoch 700, Loss: 1.90380859375\n",
      "Epoch 720, Loss: 1.9423739910125732\n",
      "Epoch 740, Loss: 1.9826431274414062\n",
      "Epoch 760, Loss: 2.024561882019043\n",
      "Epoch 780, Loss: 2.06807541847229\n",
      "Epoch 800, Loss: 2.1131296157836914\n",
      "Epoch 820, Loss: 2.159670352935791\n",
      "Epoch 840, Loss: 2.20764422416687\n",
      "Epoch 860, Loss: 2.256998062133789\n",
      "Epoch 880, Loss: 2.307681083679199\n",
      "Epoch 900, Loss: 2.3596417903900146\n",
      "Epoch 920, Loss: 2.412830114364624\n",
      "Epoch 940, Loss: 2.467198371887207\n",
      "Epoch 960, Loss: 2.5226988792419434\n",
      "Epoch 980, Loss: 2.579285144805908\n",
      "weights of  A08 :  [[array([[ 0.65079087, -0.5796477 ,  0.40378594,  0.22966626],\n",
      "       [-0.43383738,  0.4133242 , -0.09401742,  0.18248111],\n",
      "       [-0.38264668,  0.26962453,  0.4634497 , -0.03954704],\n",
      "       [ 0.11736418, -0.02258536, -0.09325153, -0.94398606],\n",
      "       [ 0.00151536, -0.28396288, -0.22047876,  0.20665766]],\n",
      "      dtype=float32), array([-0.31418544, -0.17556582,  0.99946666,  0.16071182], dtype=float32)]]\n",
      "Lowest loss of  A08 :  1.5508618\n",
      "Epoch 0, Loss: 3.515143632888794\n",
      "Epoch 20, Loss: 2.5132064819335938\n",
      "Epoch 40, Loss: 2.13362455368042\n",
      "Epoch 60, Loss: 1.9574946165084839\n",
      "Epoch 80, Loss: 1.9094130992889404\n",
      "Epoch 100, Loss: 1.88987398147583\n",
      "Epoch 120, Loss: 1.8690876960754395\n",
      "Epoch 140, Loss: 1.8475055694580078\n",
      "Epoch 160, Loss: 1.8259738683700562\n",
      "Epoch 180, Loss: 1.8045902252197266\n",
      "Epoch 200, Loss: 1.7835981845855713\n",
      "Epoch 220, Loss: 1.763274908065796\n",
      "Epoch 240, Loss: 1.7438126802444458\n",
      "Epoch 260, Loss: 1.725395917892456\n",
      "Epoch 280, Loss: 1.708180546760559\n",
      "Epoch 300, Loss: 1.6922998428344727\n",
      "Epoch 320, Loss: 1.6778686046600342\n",
      "Epoch 340, Loss: 1.6649833917617798\n",
      "Epoch 360, Loss: 1.6537237167358398\n",
      "Epoch 380, Loss: 1.644155740737915\n",
      "Epoch 400, Loss: 1.6363319158554077\n",
      "Epoch 420, Loss: 1.6302919387817383\n",
      "Epoch 440, Loss: 1.6260658502578735\n",
      "Epoch 460, Loss: 1.6236732006072998\n",
      "Epoch 480, Loss: 1.6231250762939453\n",
      "Epoch 500, Loss: 1.6244239807128906\n",
      "Epoch 520, Loss: 1.627565860748291\n",
      "Epoch 540, Loss: 1.632539987564087\n",
      "Epoch 560, Loss: 1.6393296718597412\n",
      "Epoch 580, Loss: 1.6479130983352661\n",
      "Epoch 600, Loss: 1.658263921737671\n",
      "Epoch 620, Loss: 1.6703517436981201\n",
      "Epoch 640, Loss: 1.684141755104065\n",
      "Epoch 660, Loss: 1.6995971202850342\n",
      "Epoch 680, Loss: 1.716676950454712\n",
      "Epoch 700, Loss: 1.7353386878967285\n",
      "Epoch 720, Loss: 1.7555369138717651\n",
      "Epoch 740, Loss: 1.7772252559661865\n",
      "Epoch 760, Loss: 1.8003551959991455\n",
      "Epoch 780, Loss: 1.8248770236968994\n",
      "Epoch 800, Loss: 1.85073983669281\n",
      "Epoch 820, Loss: 1.8778923749923706\n",
      "Epoch 840, Loss: 1.9062823057174683\n",
      "Epoch 860, Loss: 1.9358570575714111\n",
      "Epoch 880, Loss: 1.9665641784667969\n",
      "Epoch 900, Loss: 1.9983505010604858\n",
      "Epoch 920, Loss: 2.0311636924743652\n",
      "Epoch 940, Loss: 2.0649514198303223\n",
      "Epoch 960, Loss: 2.099661111831665\n",
      "Epoch 980, Loss: 2.1352415084838867\n",
      "weights of  A09 :  [[array([[-0.2599893 , -0.3356414 , -0.16589354,  0.18938355],\n",
      "       [-0.65429044, -0.2859282 , -0.18579462,  0.16886541],\n",
      "       [ 0.33088633,  0.6816145 ,  0.504634  , -0.17811577],\n",
      "       [-0.45228726, -1.0783038 ,  0.06168412,  0.02488419],\n",
      "       [ 0.08712289, -0.13548514, -0.06726345, -0.6947671 ]],\n",
      "      dtype=float32), array([-0.49914804, -0.27214476,  1.021153  , -0.32544863], dtype=float32)]]\n",
      "Lowest loss of  A09 :  1.6230869\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Custom loss function\n",
    "class CustomLossLL1(tf.keras.losses.Loss):\n",
    "    def __init__(self, lambda_t, model):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        ws = self.get_weights_from_model()\n",
    "        reg_term = self.regularization_term(ws)\n",
    "        return ce_loss + self.lambda_t * reg_term\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        # return tf.concat([tf.reshape(w, [-1]) for w in model_weights], axis=0)\n",
    "        return model_weights\n",
    "\n",
    "    def regularization_term(self, ws):\n",
    "        reg_term = tf.pow(tf.norm(ws, ord='euclidean'),2)\n",
    "        return reg_term\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True) # Perform a forward pass and compute the predictions\n",
    "        loss = custom_loss(y, y_pred) # Compute the custom loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_weight_LL(X_train, y_train, lambd, num_tier=10, learning_rate = 0.01):\n",
    "    n_classes = np.unique(y_train).size\n",
    "    y_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=n_classes)\n",
    "\n",
    "    lambda_t = lambd  # Regularization parameter\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_classes, input_shape=(X_train.shape[1],), activation='softmax')  # Adjust input_shape to match the number of features in X\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate)\n",
    "    custom_loss = CustomLossLL1(lambda_t, model)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = num_tier\n",
    "    lowest_loss = float('inf')\n",
    "    best_weights = None\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X_train, y_one_hot, custom_loss)\n",
    "        loss_value = loss.numpy()\n",
    "        if loss_value < lowest_loss:\n",
    "            lowest_loss = loss_value\n",
    "            best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    # best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "    return best_weights, lowest_loss\n",
    "\n",
    "def build_clf_params(data, target_subjects ,condition):\n",
    "\n",
    "    for sub in data.keys():\n",
    "        if (sub  != target_subjects) and  (sub != target_data_0): #Don't apply weight to target subject\n",
    "            # Where the tranining data is stored\n",
    "            if condition == \"noEA\":\n",
    "                X = data[sub]['Raw_csp']\n",
    "                y = data[sub]['Raw_csp_label']\n",
    "                store_ws = 'ws_Raw'\n",
    "\n",
    "            else:\n",
    "                X = data[sub]['EA_csp']\n",
    "                y = data[sub]['EA_csp_label']\n",
    "                store_ws = 'ws_EA'\n",
    "\n",
    "            weights, loss = train_weight_LL(X_train=X, y_train=y, lambd= 0.1, num_tier=1000, learning_rate= 0.005)\n",
    "            print(\"weights of \", str(sub), \": \", weights)\n",
    "            print(\"Lowest loss of \", str(sub), \": \", loss)\n",
    "            data[sub][store_ws] = weights\n",
    "\n",
    "build_clf_params(CSP2D_Epoch, target_subjects= target_data ,condition = condition_wLTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the kl divergence\n",
    "def KL_div(P, Q):\n",
    "    # First convert to np array\n",
    "    P = np.array(P)\n",
    "    Q = np.array(Q)\n",
    "    \n",
    "    # Then compute their means, datain shape of samples x feat\n",
    "    mu_P = np.mean(P, axis=0)\n",
    "    mu_Q = np.mean(Q, axis=0)    \n",
    "\n",
    "    \n",
    "    # Compute their covariance\n",
    "    sigma_P = np.cov(P, rowvar=False)\n",
    "    sigma_Q = np.cov(Q, rowvar=False)  \n",
    "\n",
    "    diff = mu_Q - mu_P\n",
    "\n",
    "    inv_sigma_Q = np.linalg.inv(sigma_Q)\n",
    "    term1 = np.dot(np.dot(diff.T, inv_sigma_Q), diff)\n",
    "    \n",
    "    # Calculate the trace term trace(Sigma_Q^{-1} * Sigma_P)\n",
    "    term2 = np.trace(np.dot(inv_sigma_Q, sigma_P))\n",
    "    \n",
    "    # Calculate the determinant term ln(det(Sigma_P) / det(Sigma_Q))\n",
    "    det_sigma0 = np.linalg.det(sigma_P)\n",
    "    det_sigma1 = np.linalg.det(sigma_Q)\n",
    "\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    term3 = np.log((det_sigma0+epsilon) / (det_sigma1+epsilon))\n",
    "    \n",
    "    print(term3)\n",
    "    \n",
    "    # Dimensionality of the data\n",
    "    K = mu_P.shape[0]\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_div = 0.5 * (term1 + term2 - term3 - K)\n",
    "    \n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2892522866877802\n",
      "0.05778548245622659\n",
      "0.3926780743664433\n",
      "0.24505163115428116\n",
      "1.2848244695027986\n",
      "0.04431032919710483\n",
      "0.3856292491889666\n",
      "0.26316216040562906\n",
      "1.1926898355483762\n",
      "-0.049909494344152525\n",
      "0.3411886338992855\n",
      "0.13360733501237046\n",
      "1.2877201694764768\n",
      "0.03290675367735027\n",
      "0.39838635976921644\n",
      "0.23620018618345692\n",
      "1.2241706568707633\n",
      "-0.0597029223176751\n",
      "0.3180565662575529\n",
      "0.06747849473863131\n",
      "1.3046792075410272\n",
      "0.04622015135688045\n",
      "0.39826033362990176\n",
      "0.25616323212059744\n",
      "1.2339710594846274\n",
      "0.027608500205255973\n",
      "0.3420750465000283\n",
      "0.19390946200963297\n",
      "0.48762253693713664\n",
      "-0.38487243535726917\n",
      "0.27448644508107145\n",
      "-0.3203919916575563\n"
     ]
    }
   ],
   "source": [
    "# Compute kl divergence of target subject to each source subject\n",
    "def compute_all_kl_div(data, target_subjects , condition):\n",
    "    '''\n",
    "    Parameter:\n",
    "    data, is the whole data containing target and source data\n",
    "    '''\n",
    "    kl_div_score = []\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        target_data = 'Raw_csp'\n",
    "        label_name = 'Raw_csp_label'\n",
    "\n",
    "    else:\n",
    "        target_data = 'EA_csp'\n",
    "        label_name = 'EA_csp_label'\n",
    "        \n",
    "    # cal P from target data\n",
    "    label_tgt =  data[target_subjects][label_name]\n",
    "    P_left =  data[target_subjects][target_data][np.where(label_tgt == 0)]\n",
    "    P_right = data[target_subjects][target_data][np.where(label_tgt == 1)]\n",
    "    P_non = data[target_subjects][target_data][np.where(label_tgt == 2)]\n",
    "    P_feet = data[target_subjects][target_data][np.where(label_tgt == 3)]\n",
    "\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "\n",
    "    #cal Q from each source subject\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            label_src =  data[sub][label_name]\n",
    "            Q_left =  data[sub][target_data][np.where(label_src == 0)]\n",
    "            Q_right = data[sub][target_data][np.where(label_src == 1)]\n",
    "            Q_non = data[sub][target_data][np.where(label_src == 2)]\n",
    "            Q_feet = data[sub][target_data][np.where(label_src == 3)]\n",
    "\n",
    "            kl_left = KL_div(P_left, Q_left)\n",
    "            kl_right = KL_div(P_right, Q_right)\n",
    "            kl_non = KL_div(P_non, Q_non)\n",
    "            kl_feet = KL_div(P_feet, Q_feet)\n",
    "\n",
    "            # kl_div = (kl_left + kl_right+ kl_non + kl_feet)/4\n",
    "\n",
    "            kl_div_temp = [kl_left, kl_right, kl_non, kl_feet]\n",
    "\n",
    "            kl_div_score.append(kl_div_temp)\n",
    "\n",
    "    data[target_subjects]['kl_div'] = kl_div_score\n",
    "\n",
    "\n",
    "compute_all_kl_div(CSP2D_Epoch, target_subjects=target_data_0 ,condition = condition_wLTL) #target_sub for cal KL is calibrate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.49981156,  4.99347434,  8.24152365,  3.63779696],\n",
       "       [ 5.32767393,  1.52156024,  3.52612037,  4.59705701],\n",
       "       [ 4.18652534,  1.93403677,  3.28925458,  2.26193964],\n",
       "       [ 4.41365757,  1.14857994,  3.83123545,  2.80574815],\n",
       "       [ 3.21629018,  0.69981306,  2.18983865,  1.65530914],\n",
       "       [ 7.32939934,  2.0707374 ,  4.30802669,  5.05504452],\n",
       "       [ 3.83900779,  1.32869118,  3.5925581 ,  2.76258951],\n",
       "       [ 0.63642639, -0.12422276,  2.53530803,  1.2867409 ]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(CSP2D_Epoch[target_data_0]['kl_div'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0010928918246028447, 0.0012411287777504341, 0.0032549446544421416, 0.0026349119038919524, 0.009343833514665369, 0.000346499617420143, 0.004603400644880859, 6.091641629339231]\n",
      "[0.0016082513323604115, 0.1865216416611863, 0.07145806805388064, 0.5743860049155737, 4.167001043407393, 0.05437709463402768, 0.32075471527696975, 4213.027555134919]\n",
      "[[1.78747687e-04 3.81246429e-07 2.24720475e-03 9.86427686e-03]\n",
      " [2.02992550e-04 4.42161672e-05 6.70588284e-02 3.86820500e-03]\n",
      " [5.32361773e-04 1.69395994e-05 8.85627292e-02 6.59881539e-02]\n",
      " [4.30952450e-04 1.36161935e-04 4.81163914e-02 2.78746980e-02]\n",
      " [1.52822868e-03 9.87814675e-04 4.50781380e-01 2.30062289e-01]\n",
      " [5.66716704e-05 1.28904436e-05 3.00981563e-02 2.64565081e-03]\n",
      " [7.52908203e-04 7.60369896e-05 6.22343567e-02 2.96577713e-02]\n",
      " [9.96317137e-01 9.98725559e-01 2.50900953e-01 6.30038955e-01]]\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_weights(data, target_subjects):\n",
    "    kl = data[target_subjects]['kl_div']\n",
    "    KL_inv_left = []\n",
    "    KL_inv_right = []\n",
    "    KL_inv_non = []\n",
    "    KL_inv_feet = []\n",
    "\n",
    "    alpha_s = []\n",
    "    eps = 0.0001\n",
    "    \n",
    "    #equation (9)\n",
    "    for val in kl:\n",
    "        if val != 0: \n",
    "            KL_inv_left.append(1/((val[0] + eps)**4))\n",
    "            KL_inv_right.append(1/((val[1] + eps)**4))\n",
    "            KL_inv_non.append(1/((val[2] + eps)**4))\n",
    "            KL_inv_feet.append(1/((val[3] + eps)**4))\n",
    "\n",
    "    print(KL_inv_left)\n",
    "    print(KL_inv_right)\n",
    "    \n",
    "    for i in range(0,len(KL_inv_left)):\n",
    "        temp = [KL_inv_left[i]/sum(KL_inv_left), KL_inv_right[i]/sum(KL_inv_right), KL_inv_non[i]/sum(KL_inv_non), KL_inv_feet[i]/sum(KL_inv_feet)]\n",
    "        alpha_s.append(temp)\n",
    "\n",
    "    print(np.array(alpha_s))\n",
    "                \n",
    "    data[target_subjects]['alpha_s'] = alpha_s\n",
    "\n",
    "compute_similarity_weights(CSP2D_Epoch, target_subjects=target_data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25891066 -0.33555171  0.13691678  0.04893298]\n",
      " [-0.65236331 -0.2853304  -0.24002355  0.15529069]\n",
      " [ 0.32806539  0.68077886  0.22829113 -0.08177716]\n",
      " [-0.45099622 -1.07746185 -0.01710004 -0.03701628]\n",
      " [ 0.08807441 -0.13505119 -0.01358355 -0.35822562]]\n",
      "[[ 1.42991548e+00  1.67990036e+00 -2.03714417e+00  3.33151916e+00\n",
      "  -8.39628582e-03]\n",
      " [ 1.67990036e+00  4.11215369e+00 -3.30378126e+00  4.19314689e+00\n",
      "  -5.12454972e-01]\n",
      " [-2.03714417e+00 -3.30378126e+00  4.39376903e+00 -6.17636576e+00\n",
      "  -2.23165618e-01]\n",
      " [ 3.33151916e+00  4.19314689e+00 -6.17636576e+00  9.57119659e+00\n",
      "   8.10555002e-01]\n",
      " [-8.39628582e-03 -5.12454972e-01 -2.23165618e-01  8.10555002e-01\n",
      "   1.17328969e+00]]\n"
     ]
    }
   ],
   "source": [
    "def compute_ETL_and_mu_ws(data, target_subjects, condition):\n",
    "\n",
    "    mu_ws = 0\n",
    "    temp_ws = 0\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        ws_name = 'ws_Raw'\n",
    "    else:\n",
    "        ws_name = 'ws_EA'\n",
    "\n",
    "    alpha_s = np.array(data[target_subjects]['alpha_s'])\n",
    "\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "    index_count = 0\n",
    "\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            ws = data[sub][ws_name][0][0]\n",
    "            # mu_ws += ws @ alpha_s  #equation (10)\n",
    "            # mu_ws += np.dot(ws, np.transpose(alpha_s))\n",
    "            mu_ws += ws * alpha_s[index_count]\n",
    "            index_count += 1\n",
    "\n",
    "    print(np.array(mu_ws))\n",
    "\n",
    "    index_count = 0\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            ws = data[sub][ws_name][0][0]\n",
    "            # ws_min_mu = np.dot((np.dot(ws,np.transpose(alpha_s)) - mu_ws) , np.transpose((np.dot(ws,np.transpose(alpha_s)) - mu_ws)))\n",
    "            ws_min_mu = np.dot(((ws * alpha_s[index_count]) - mu_ws), np.transpose((ws * alpha_s[index_count]) - mu_ws))\n",
    "            temp_ws += ws_min_mu #equation (11)\n",
    "            index_count += 1\n",
    "\n",
    "    print(np.array(temp_ws))\n",
    "    \n",
    "    # den = np.diag(temp_ws) #get array in diagonal line\n",
    "\n",
    "    den = temp_ws\n",
    "    nom = np.trace(temp_ws) #Return the sum along diagonals of the array.\n",
    "    Sigma_TL = den/nom\n",
    "\n",
    "\n",
    "    data[target_subjects]['Sigma_TL'] = Sigma_TL\n",
    "    data[target_subjects]['mu_ws'] = mu_ws\n",
    "\n",
    "compute_ETL_and_mu_ws(CSP2D_Epoch, target_subjects = target_data_0, condition=condition_wLTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(CSP2D_Epoch[target_data_0]['Sigma_TL']).shape)\n",
    "print(np.array(CSP2D_Epoch[target_data_0]['mu_ws']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: -2.2522635459899902\n",
      "Epoch 20, Loss: -2.8747618198394775\n",
      "Epoch 40, Loss: -3.5457820892333984\n",
      "Epoch 60, Loss: -4.41274356842041\n",
      "Epoch 80, Loss: -5.433821201324463\n",
      "Epoch 100, Loss: -6.564009666442871\n",
      "Epoch 120, Loss: -7.735199928283691\n",
      "Epoch 140, Loss: -8.889394760131836\n",
      "Epoch 160, Loss: -9.982175827026367\n",
      "Epoch 180, Loss: -10.9820556640625\n",
      "Epoch 200, Loss: -11.869194030761719\n",
      "Epoch 220, Loss: -12.633204460144043\n",
      "Epoch 240, Loss: -13.270782470703125\n",
      "Epoch 260, Loss: -13.783778190612793\n",
      "Epoch 280, Loss: -14.17738151550293\n",
      "Epoch 300, Loss: -14.45861530303955\n",
      "Epoch 320, Loss: -14.635427474975586\n",
      "Epoch 340, Loss: -14.715901374816895\n",
      "Epoch 360, Loss: -14.707903861999512\n",
      "Epoch 380, Loss: -14.618825912475586\n",
      "Epoch 400, Loss: -14.455578804016113\n",
      "Epoch 420, Loss: -14.2245512008667\n",
      "Epoch 440, Loss: -13.931619644165039\n",
      "Epoch 460, Loss: -13.582201957702637\n",
      "Epoch 480, Loss: -13.181341171264648\n",
      "Epoch 500, Loss: -12.733694076538086\n",
      "Epoch 520, Loss: -12.243658065795898\n",
      "Epoch 540, Loss: -11.715130805969238\n",
      "Epoch 560, Loss: -11.151742935180664\n",
      "Epoch 580, Loss: -10.556988716125488\n",
      "Epoch 600, Loss: -9.933764457702637\n",
      "Epoch 620, Loss: -9.284875869750977\n",
      "Epoch 640, Loss: -8.6127347946167\n",
      "Epoch 660, Loss: -7.919604301452637\n",
      "Epoch 680, Loss: -7.207342624664307\n",
      "Epoch 700, Loss: -6.47772216796875\n",
      "Epoch 720, Loss: -5.732306003570557\n",
      "Epoch 740, Loss: -4.972381591796875\n",
      "Epoch 760, Loss: -4.199236869812012\n",
      "Epoch 780, Loss: -3.4139404296875\n",
      "Epoch 800, Loss: -2.617584228515625\n",
      "Epoch 820, Loss: -1.81109619140625\n",
      "Epoch 840, Loss: -0.995269775390625\n",
      "Epoch 860, Loss: -0.17106322944164276\n",
      "Epoch 880, Loss: 0.660656750202179\n",
      "Epoch 900, Loss: 1.499182105064392\n",
      "Epoch 920, Loss: 2.343579053878784\n",
      "Epoch 940, Loss: 3.1931214332580566\n",
      "Epoch 960, Loss: 4.046972751617432\n",
      "Epoch 980, Loss: 4.904192924499512\n",
      "Epoch 1000, Loss: 5.763970851898193\n",
      "Epoch 1020, Loss: 6.625482082366943\n",
      "Epoch 1040, Loss: 7.487860202789307\n",
      "Epoch 1060, Loss: 8.350110054016113\n",
      "Epoch 1080, Loss: 9.211438179016113\n",
      "Epoch 1100, Loss: 10.0709228515625\n",
      "Epoch 1120, Loss: 10.927721977233887\n",
      "Epoch 1140, Loss: 11.780858993530273\n",
      "Epoch 1160, Loss: 12.629565238952637\n",
      "Epoch 1180, Loss: 13.472821235656738\n",
      "Epoch 1200, Loss: 14.310003280639648\n",
      "Epoch 1220, Loss: 15.140050888061523\n",
      "Epoch 1240, Loss: 15.962389945983887\n",
      "Epoch 1260, Loss: 16.776073455810547\n",
      "Epoch 1280, Loss: 17.580297470092773\n",
      "Epoch 1300, Loss: 18.374608993530273\n",
      "Epoch 1320, Loss: 19.1580753326416\n",
      "Epoch 1340, Loss: 19.930164337158203\n",
      "Epoch 1360, Loss: 20.690221786499023\n",
      "Epoch 1380, Loss: 21.43780517578125\n",
      "Epoch 1400, Loss: 22.172210693359375\n",
      "Epoch 1420, Loss: 22.893041610717773\n",
      "Epoch 1440, Loss: 23.600000381469727\n",
      "Epoch 1460, Loss: 24.292247772216797\n",
      "Epoch 1480, Loss: 24.969751358032227\n",
      "Epoch 1500, Loss: 25.632152557373047\n",
      "Epoch 1520, Loss: 26.279077529907227\n",
      "Epoch 1540, Loss: 26.910247802734375\n",
      "Epoch 1560, Loss: 27.525548934936523\n",
      "Epoch 1580, Loss: 28.12481689453125\n",
      "Epoch 1600, Loss: 28.7078857421875\n",
      "Epoch 1620, Loss: 29.274572372436523\n",
      "Epoch 1640, Loss: 29.8249568939209\n",
      "Epoch 1660, Loss: 30.3590087890625\n",
      "Epoch 1680, Loss: 30.876733779907227\n",
      "Epoch 1700, Loss: 31.378040313720703\n",
      "Epoch 1720, Loss: 31.863208770751953\n",
      "Epoch 1740, Loss: 32.33220291137695\n",
      "Epoch 1760, Loss: 32.785072326660156\n",
      "Epoch 1780, Loss: 33.2220573425293\n",
      "Epoch 1800, Loss: 33.643463134765625\n",
      "Epoch 1820, Loss: 34.0493049621582\n",
      "Epoch 1840, Loss: 34.4398193359375\n",
      "Epoch 1860, Loss: 34.81535720825195\n",
      "Epoch 1880, Loss: 35.17615509033203\n",
      "Epoch 1900, Loss: 35.522239685058594\n",
      "Epoch 1920, Loss: 35.85432052612305\n",
      "Epoch 1940, Loss: 36.17235565185547\n",
      "Epoch 1960, Loss: 36.47675323486328\n",
      "Epoch 1980, Loss: 36.76789474487305\n",
      "Epoch 2000, Loss: 37.046112060546875\n",
      "Epoch 2020, Loss: 37.3116340637207\n",
      "Epoch 2040, Loss: 37.564918518066406\n",
      "Epoch 2060, Loss: 37.80610275268555\n",
      "Epoch 2080, Loss: 38.03596878051758\n",
      "Epoch 2100, Loss: 38.25443649291992\n",
      "Epoch 2120, Loss: 38.46210861206055\n",
      "Epoch 2140, Loss: 38.65924835205078\n",
      "Epoch 2160, Loss: 38.84625244140625\n",
      "Epoch 2180, Loss: 39.0234489440918\n",
      "Epoch 2200, Loss: 39.19135665893555\n",
      "Epoch 2220, Loss: 39.349952697753906\n",
      "Epoch 2240, Loss: 39.499961853027344\n",
      "Epoch 2260, Loss: 39.641502380371094\n",
      "Epoch 2280, Loss: 39.77508544921875\n",
      "Epoch 2300, Loss: 39.90087127685547\n",
      "Epoch 2320, Loss: 40.019378662109375\n",
      "Epoch 2340, Loss: 40.130760192871094\n",
      "Epoch 2360, Loss: 40.23546600341797\n",
      "Epoch 2380, Loss: 40.3337516784668\n",
      "Epoch 2400, Loss: 40.425941467285156\n",
      "Epoch 2420, Loss: 40.51220703125\n",
      "Epoch 2440, Loss: 40.593017578125\n",
      "Epoch 2460, Loss: 40.668418884277344\n",
      "Epoch 2480, Loss: 40.73895263671875\n",
      "Epoch 2500, Loss: 40.804649353027344\n",
      "Epoch 2520, Loss: 40.86604690551758\n",
      "Epoch 2540, Loss: 40.92303466796875\n",
      "Epoch 2560, Loss: 40.976104736328125\n",
      "Epoch 2580, Loss: 41.025390625\n",
      "Epoch 2600, Loss: 41.0711784362793\n",
      "Epoch 2620, Loss: 41.113502502441406\n",
      "Epoch 2640, Loss: 41.15275955200195\n",
      "Epoch 2660, Loss: 41.188941955566406\n",
      "Epoch 2680, Loss: 41.22239303588867\n",
      "Epoch 2700, Loss: 41.2533073425293\n",
      "Epoch 2720, Loss: 41.2817497253418\n",
      "Epoch 2740, Loss: 41.30790328979492\n",
      "Epoch 2760, Loss: 41.332054138183594\n",
      "Epoch 2780, Loss: 41.3540153503418\n",
      "Epoch 2800, Loss: 41.37431716918945\n",
      "Epoch 2820, Loss: 41.3927116394043\n",
      "Epoch 2840, Loss: 41.40973663330078\n",
      "Epoch 2860, Loss: 41.42506790161133\n",
      "Epoch 2880, Loss: 41.43928146362305\n",
      "Epoch 2900, Loss: 41.452125549316406\n",
      "Epoch 2920, Loss: 41.463905334472656\n",
      "Epoch 2940, Loss: 41.47450637817383\n",
      "Epoch 2960, Loss: 41.4840087890625\n",
      "Epoch 2980, Loss: 41.492759704589844\n",
      "Epoch 3000, Loss: 41.50048828125\n",
      "Epoch 3020, Loss: 41.50767135620117\n",
      "Epoch 3040, Loss: 41.514060974121094\n",
      "Epoch 3060, Loss: 41.519676208496094\n",
      "Epoch 3080, Loss: 41.52485275268555\n",
      "Epoch 3100, Loss: 41.529396057128906\n",
      "Epoch 3120, Loss: 41.5335578918457\n",
      "Epoch 3140, Loss: 41.537208557128906\n",
      "Epoch 3160, Loss: 41.5405387878418\n",
      "Epoch 3180, Loss: 41.543373107910156\n",
      "Epoch 3200, Loss: 41.5458869934082\n",
      "Epoch 3220, Loss: 41.54740524291992\n",
      "Epoch 3240, Loss: 41.5415153503418\n",
      "Epoch 3260, Loss: 41.546119689941406\n",
      "Epoch 3280, Loss: 41.555564880371094\n",
      "Epoch 3300, Loss: 41.55461502075195\n",
      "Epoch 3320, Loss: 41.555931091308594\n",
      "Epoch 3340, Loss: 41.55681228637695\n",
      "Epoch 3360, Loss: 41.55786895751953\n",
      "Epoch 3380, Loss: 41.558433532714844\n",
      "Epoch 3400, Loss: 41.559104919433594\n",
      "Epoch 3420, Loss: 41.559715270996094\n",
      "Epoch 3440, Loss: 41.560157775878906\n",
      "Epoch 3460, Loss: 41.56110763549805\n",
      "Epoch 3480, Loss: 41.580848693847656\n",
      "Epoch 3500, Loss: 41.5673942565918\n",
      "Epoch 3520, Loss: 41.56168746948242\n",
      "Epoch 3540, Loss: 41.560752868652344\n",
      "Epoch 3560, Loss: 41.561912536621094\n",
      "Epoch 3580, Loss: 41.56201171875\n",
      "Epoch 3600, Loss: 41.56211471557617\n",
      "Epoch 3620, Loss: 41.56212615966797\n",
      "Epoch 3640, Loss: 41.562232971191406\n",
      "Epoch 3660, Loss: 41.56227493286133\n",
      "Epoch 3680, Loss: 41.5622673034668\n",
      "Epoch 3700, Loss: 41.562294006347656\n",
      "Epoch 3720, Loss: 41.562400817871094\n",
      "Epoch 3740, Loss: 41.56580352783203\n",
      "Epoch 3760, Loss: 41.569602966308594\n",
      "Epoch 3780, Loss: 41.56329345703125\n",
      "Epoch 3800, Loss: 41.56201171875\n",
      "Epoch 3820, Loss: 41.561744689941406\n",
      "Epoch 3840, Loss: 41.5622444152832\n",
      "Epoch 3860, Loss: 41.5623664855957\n",
      "Epoch 3880, Loss: 41.56235885620117\n",
      "Epoch 3900, Loss: 41.56242752075195\n",
      "Epoch 3920, Loss: 41.56241989135742\n",
      "Epoch 3940, Loss: 41.562400817871094\n",
      "Epoch 3960, Loss: 41.56232833862305\n",
      "Epoch 3980, Loss: 41.562278747558594\n",
      "Epoch 4000, Loss: 41.56217575073242\n",
      "Epoch 4020, Loss: 41.562042236328125\n",
      "Epoch 4040, Loss: 41.562217712402344\n",
      "Epoch 4060, Loss: 41.59407424926758\n",
      "Epoch 4080, Loss: 41.567161560058594\n",
      "Epoch 4100, Loss: 41.559669494628906\n",
      "Epoch 4120, Loss: 41.5629997253418\n",
      "Epoch 4140, Loss: 41.562416076660156\n",
      "Epoch 4160, Loss: 41.5621452331543\n",
      "Epoch 4180, Loss: 41.56208419799805\n",
      "Epoch 4200, Loss: 41.5621452331543\n",
      "Epoch 4220, Loss: 41.5621337890625\n",
      "Epoch 4240, Loss: 41.5621452331543\n",
      "Epoch 4260, Loss: 41.56201171875\n",
      "Epoch 4280, Loss: 41.56208419799805\n",
      "Epoch 4300, Loss: 41.56206130981445\n",
      "Epoch 4320, Loss: 41.56200408935547\n",
      "Epoch 4340, Loss: 41.56195831298828\n",
      "Epoch 4360, Loss: 41.5620002746582\n",
      "Epoch 4380, Loss: 41.56189727783203\n",
      "Epoch 4400, Loss: 41.56193923950195\n",
      "Epoch 4420, Loss: 41.561378479003906\n",
      "Epoch 4440, Loss: 41.55354690551758\n",
      "Epoch 4460, Loss: 41.5637321472168\n",
      "Epoch 4480, Loss: 41.562400817871094\n",
      "Epoch 4500, Loss: 41.5619010925293\n",
      "Epoch 4520, Loss: 41.56181716918945\n",
      "Epoch 4540, Loss: 41.56196975708008\n",
      "Epoch 4560, Loss: 41.5618782043457\n",
      "Epoch 4580, Loss: 41.561805725097656\n",
      "Epoch 4600, Loss: 41.56183624267578\n",
      "Epoch 4620, Loss: 41.56184005737305\n",
      "Epoch 4640, Loss: 41.561912536621094\n",
      "Epoch 4660, Loss: 41.57368087768555\n",
      "Epoch 4680, Loss: 41.557884216308594\n",
      "Epoch 4700, Loss: 41.56101608276367\n",
      "Epoch 4720, Loss: 41.562164306640625\n",
      "Epoch 4740, Loss: 41.56230545043945\n",
      "Epoch 4760, Loss: 41.561927795410156\n",
      "Epoch 4780, Loss: 41.561988830566406\n",
      "Epoch 4800, Loss: 41.56196975708008\n",
      "Epoch 4820, Loss: 41.56193161010742\n",
      "Epoch 4840, Loss: 41.5618896484375\n",
      "Epoch 4860, Loss: 41.561851501464844\n",
      "Epoch 4880, Loss: 41.56180953979492\n",
      "Epoch 4900, Loss: 41.561866760253906\n",
      "Epoch 4920, Loss: 41.5613899230957\n",
      "Epoch 4940, Loss: 41.565513610839844\n",
      "Epoch 4960, Loss: 41.55952835083008\n",
      "Epoch 4980, Loss: 41.56206130981445\n",
      "weights of  A06 :  [[array([[ 1.1669552 , -1.519531  ,  1.990146  ,  0.07156592],\n",
      "       [-2.179983  ,  2.228068  , -0.16027331, -0.42740396],\n",
      "       [-0.46960938, -0.718482  , -1.5688875 ,  0.47861618],\n",
      "       [-0.84104383, -1.6472894 , -0.89388555,  4.186083  ],\n",
      "       [ 0.6681313 ,  0.32503676,  0.13078588, -2.9661841 ]],\n",
      "      dtype=float32), array([-0.7177974,  0.3384355,  1.0273576,  0.8028956], dtype=float32)]]\n",
      "loss of  A06 :  0.0053710938\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.47      0.47        30\n",
      "           1       0.40      0.47      0.43        30\n",
      "           2       0.41      0.37      0.39        30\n",
      "           3       0.59      0.57      0.58        30\n",
      "\n",
      "    accuracy                           0.47       120\n",
      "   macro avg       0.47      0.47      0.47       120\n",
      "weighted avg       0.47      0.47      0.47       120\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[14  9  5  2]\n",
      " [ 6 14  6  4]\n",
      " [ 7  6 11  6]\n",
      " [ 2  6  5 17]]\n",
      "6/6 [==============================] - 0s 403us/step\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.31      0.37        42\n",
      "           1       0.39      0.50      0.44        42\n",
      "           2       0.34      0.33      0.34        42\n",
      "           3       0.39      0.40      0.40        42\n",
      "\n",
      "    accuracy                           0.39       168\n",
      "   macro avg       0.39      0.39      0.38       168\n",
      "weighted avg       0.39      0.39      0.38       168\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[13 11 13  5]\n",
      " [ 5 21  6 10]\n",
      " [ 7  9 14 12]\n",
      " [ 4 13  8 17]]\n"
     ]
    }
   ],
   "source": [
    "# Custom loss function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "class CustomLossLL2(tf.keras.losses.Loss):\n",
    "    def __init__(self, lambda_t, model, mu, sigma_TL):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "        self.mu = tf.convert_to_tensor(mu, dtype=tf.float32)\n",
    "        self.sigma_TL = tf.convert_to_tensor(sigma_TL, dtype=tf.float32)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        wt = self.get_weights_from_model()\n",
    "        reg_term = self.regularization_term(wt)\n",
    "\n",
    "        return ce_loss + (self.lambda_t * tf.linalg.matmul(reg_term, wt))\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        # return tf.concat([tf.reshape(w, [-1]) for w in model_weights], axis=0)\n",
    "        return model_weights\n",
    "\n",
    "    def regularization_term(self, wt):\n",
    "        diff = wt - self.mu\n",
    "        reg_term = 0.5 * tf.linalg.matmul(tf.linalg.matmul(tf.linalg.inv(self.sigma_TL), diff[0]), tf.transpose(diff[0]))\n",
    "        reg_term += 0.5 * tf.math.log(tf.linalg.det(self.sigma_TL))\n",
    "        return reg_term\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True) # Perform a forward pass and compute the predictions\n",
    "        loss = custom_loss(y, y_pred) # Compute the custom loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_weight_LL2(X_train, y_train, lambd, mu, sigma_TL, num_tier=10, learning_rate = 0.01):\n",
    "    n_classes = np.unique(y_train).size\n",
    "    y_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=n_classes)\n",
    "\n",
    "    lambda_t = lambd  # Regularization parameter\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_classes, input_shape=(X_train.shape[1],), activation='softmax')  # Adjust input_shape to match the number of features in X\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate)\n",
    "    custom_loss = CustomLossLL2(lambda_t, model, mu, sigma_TL)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = num_tier\n",
    "    lowest_loss = float('inf')\n",
    "    best_weights = None\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X_train, y_one_hot, custom_loss)\n",
    "        loss_value = loss.numpy()\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        if (abs(loss_value) < lowest_loss):\n",
    "            lowest_loss = abs(loss_value)\n",
    "            best_model = model\n",
    "            best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "    return best_model, best_weights, lowest_loss\n",
    "\n",
    "def GetConfusionMatrix(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_prob = model.predict(X_train)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "    print(\"Classification TRAIN DATA \\n=======================\")\n",
    "    print(classification_report(y_true= y_train, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true= y_train, y_pred=y_pred))\n",
    "\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    print(\"Classification TEST DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "\n",
    "def tgt_test_wLTL(data, target_subjects ,condition):\n",
    "        tgt_data = target_subjects + \"_test\"\n",
    "\n",
    "        if condition == \"noEA\":\n",
    "            X = data[target_subjects]['Raw_csp']\n",
    "            y = data[target_subjects]['Raw_csp_label']\n",
    "            X_test = data[tgt_data]['Raw_csp']\n",
    "            y_test = data[tgt_data]['Raw_csp_label']\n",
    "            store_ws = 'wt_Raw'\n",
    "\n",
    "        else:\n",
    "            X = data[target_subjects]['EA_csp']\n",
    "            y = data[target_subjects]['EA_csp_label']\n",
    "            X_test = data[tgt_data]['EA_csp']\n",
    "            y_test = data[tgt_data]['EA_csp_label']\n",
    "            store_ws = 'wt_EA'\n",
    "\n",
    "        mu = data[target_subjects]['mu_ws']\n",
    "        sigma_TL = data[target_subjects]['Sigma_TL']\n",
    "\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "        \n",
    "        model, weights, loss = train_weight_LL2(X_train=X_train, y_train=y_train, mu =mu, sigma_TL=sigma_TL, lambd= 0.1, num_tier=5000, learning_rate= 0.01)\n",
    "        print(\"weights of \", str(target_subjects), \": \", weights)\n",
    "        print(\"loss of \", str(target_subjects), \": \", loss)\n",
    "        data[target_subjects][store_ws] = weights\n",
    "\n",
    "        GetConfusionMatrix(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "tgt_test_wLTL(CSP2D_Epoch, target_subjects= target_data_0 ,condition = condition_wLTL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
