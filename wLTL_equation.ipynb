{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand Weight Logistic Regression-Based Learning Algorithm\n",
    "# (wLTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference paper : https://ieeexplore.ieee.org/abstract/document/8737742/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Proposed Logistic Regression-Based Learning Algorithm (LTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*} P(y_{s}^{i}\\!\\!=\\!\\!1\\vert \\mathbf {x}_{s}^{i};\\mathbf {w}_{s}) = \\frac {1}{1+exp^{-(\\mathbf {w}_{s}^{T}\\mathbf {x}_{s}^{i})}},\\tag{1}\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula (1) describes the logistic regression model used to make probabilistic predictions for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The set of labeled EEG trials from session s can be presented as $ d_{s}\\!=\\!(\\mathbf {x}_{s}^{i},y_{s}^{i})_{i_{=1}}^{n_{s}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $ \\mathbf {x}_{s}^{i} $ = the feature vector \n",
    "- $ y_{s}^{i} $ = the class label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where s denotes the session s , and  $ \\mathbf {w}_{s}\\!\\in \\!\\mathbb {R}^{v\\times 1} $\n",
    " refers to the classification parameters being used to predict the class labels of the trials Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposed LTL algorithm consists of two main steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, for every previously recorded session $\\forall d_{s}\\!\\in \\!\\{d_{1},d_{2},\\ldots ,d_{m}\\} $ \n",
    "the classification parameters, $ w_{s}$ , are calculated using the following objective function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*} \\text L_{1}(\\mathbf {w}_{s})= \\min _{\\mathbf {w}_{s}} \\quad \\left({\\sum _{i=1}^{n_{s}} \\text H(\\mathbf {w}_{s};y_{s}^{i},\\mathbf {x}_{s}^{i}) +\\lambda _{s} \\vert \\vert \\mathbf {w}_{s}\\vert \\vert _{2}^{2}}\\right),\\tag{2}\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where H and $ \\vert \\vert.\\vert \\vert _{2} $ denote the cross-entropy and 2-norm functions respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, in $ \\text L_{1}(\\mathbf {w}_{s}) $ , the cross entropy aims at finding ws that minimizes the error rate while the 2-norm penalizes large values of ws to reduce the risk of over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subject-specific regularization parameter $\\lambda _{s}$ is used to control the degree of penalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Cross entropy function H is also called **negative log-likelihood** where its minimization is equivalent to maximizing the log likelihood (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}&\\hspace {-0.5pc}\\text H(\\mathbf {w}_{s}; \\mathbf {x}_{s}^{i}, y_{s}^{i}) =-y_{s}^{i} \\log P(y_{s}^{i}\\!\\!=\\!\\!1\\vert \\mathbf {x}_{s}^{i};\\mathbf {w}_{s})- (1 - y_{s}^{i}) \\\\&\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad {{{\\log (1 - P(y_{s}^{i}\\!\\!=\\!\\!1\\vert \\mathbf {x}_{s}^{i};\\mathbf {w}_{s})),} }}\\tag{3}\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $ P(y_{s}^{i}\\!\\!=\\!\\!1\\vert \\mathbf {x}_{s}^{i};\\mathbf {w}_{s}) $ is calculated using (1). The objective of function $ \\text L_{1}(\\mathbf {w}_{s}) $ does not have a closed form solution. However, it has a unique minimum that can be found using simple and effective iterative approaches such as the gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite being sufficiently effective for sessions with large training data sizes, the objective function $ \\text L_{1}(\\mathbf {w}_{s}) $ may fail in estimating the classification parameters of the new subject since few available subject-specific trials typically are not able to accurately represent the distributions of the features. \n",
    "\n",
    "In other words, in addition to the discriminative parameters, we are interested in parameters that are similar to the classification parameters of the other sessions with this assumption that there is some common information across the sessions performing the same mental tasks (i.e. motor imagery)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above-mentioned assumption, after calculating the classification parameters of the previously recorded sessions using (2), in the second step, the classification parameters of the **new target subject, $W_{t}$** , is calculated using the following objective function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*} \\text L_{2}(\\mathbf {w}_{t})\\!=\\!\\min _{\\mathbf {w}_{t}} \\quad \\left({\\sum _{i=1}^{n_{t}} \\text H(\\mathbf {w}_{t};y_{t}^{i},\\mathbf {x}_{t}^{i}) + \\lambda _{t} \\text R_{TL}(\\mathbf {w}_{t})}\\right),\\tag{4}\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\text R_{TL}$ is the regularization term penalizing dissimilarities between $W_{t}$ and the previously calculated $W_{s}$ (source_subjects), $ \\forall d_{s}\\!\\in \\!\\{d_{1},d_{2},\\ldots ,d_{m}\\} $ \n",
    "\n",
    "The regularization parameter $\\lambda _{s}$ is making a trade-off between minimizing the error and dissimilarities between the new target subject and previous sessions in terms of the classification parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term $\\text R_{TL}$ is calculated by taking into account the prior distribution of the existing classification parameters and comparing them with $w_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*} \\text R_{TL}(\\mathbf {w}_{t})= 0.5[(\\mathbf {w}_{t}-\\boldsymbol {\\mu })^{T}\\boldsymbol {\\Sigma }_{TL}^{-1}(\\mathbf {w}_{t}-\\boldsymbol {\\mu })+\\log (\\vert \\boldsymbol {\\Sigma }_{TL}\\vert)],\\tag{5}\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where Î¼ and $ \\boldsymbol {\\Sigma }_{TL} $ are respectively calculated as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} &\\qquad \\qquad \\quad \\boldsymbol {\\mu } = (1/m)\\sum _{s=1}^{m} \\mathbf {w}_{s},\\tag{6}\\\\ &\\boldsymbol {\\Sigma }_{TL}=\\frac {\\text {diag}\\left({\\sum _{s=1}^{m} (\\mathbf {w}_{s} - \\boldsymbol {\\mu })(\\mathbf {w}_{s} - \\boldsymbol {\\mu })^{T}}\\right)}{\\text {trace} \\left({\\sum _{s=1}^{m}(\\mathbf {w}_{s} - \\boldsymbol {\\mu })(\\mathbf {w}_{s} -\\boldsymbol { \\mu })^{T}}\\right)}.\\tag{7}\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in (7), $\\boldsymbol {\\Sigma }_{TL} \\!\\in \\!\\mathbb {R}^{v\\times v }$ only includes the normalized diagonal elements of the covariance matrix, where diag and trace give the diagonal elements and the sum of the diagonal elements of a matrix respectively.\n",
    "\n",
    "Indeed, in this study, only diagonal elements are used to reduce the optimization complexity.\n",
    "Subsequently, in (5), $\\boldsymbol {\\Sigma }_{TL}$ is used to normalize the divergence of each parameter of $w_{t}$ from the average of the corresponding parameters of the other classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you example code from https://github.com/orvindemsy/EA-wLTL/tree/master\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "\n",
    "class LogReg_TL(BaseEstimator):\n",
    "    def __init__(self, learningRate=1e-5, num_iter=100, penalty=None, intercept = True,\\\n",
    "                 lambd=1, ETL=np.array([[0, 0],[0, 0]]), mu=0):\n",
    "        \n",
    "        self.learningRate = learningRate\n",
    "        self.num_iter = num_iter\n",
    "        self.penalty = penalty\n",
    "        self.intercept = intercept\n",
    "        self.ETL = ETL\n",
    "        self.lambd = lambd\n",
    "        self.mu = mu\n",
    "        \n",
    "    def __sigmoid(self, z): #We need to change to softmax function \n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def __logLL(self, z, y): \n",
    "        return -1 * np.sum((y * np.log10(self.__sigmoid(z))) + ((1 - y) * np.log10(1 - self.__sigmoid(z))))\n",
    "\n",
    "    def __reg_logLL1(self, z, y, weights): # cal L1 weight (source_subjects) sum of negative log-likelihood (2)\n",
    "        reg = 0.5 * self.lambd * np.sum(np.dot(weights, weights))\n",
    "\n",
    "        return (-1 * np.sum((y * np.log10(self.__sigmoid(z))) + ((1 - y) * np.log10(1 - self.__sigmoid(z)))) ) + reg\n",
    "    \n",
    "    def __reg_logLL2(self, z, y, weights): # cal L2 weight (target_subjects) (4)\n",
    "        ETL_det = np.log10(np.linalg.det(self.ETL))\n",
    "        \n",
    "        reg = 0.5 * self.lambd * np.sum( ((weights-self.mu)**2)@self.ETL) + ETL_det \n",
    "\n",
    "        return (-1 * np.sum((y * np.log10(self.__sigmoid(z))) + ((1 - y) * np.log10(1 - self.__sigmoid(z)))) ) + reg\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.weights = np.zeros(np.shape(X_train)[1] + 1) \n",
    "        \n",
    "        if self.intercept:\n",
    "            X_train = np.c_[np.ones([np.shape(X_train)[0], 1]), X_train]\n",
    "        \n",
    "        self.costs = []\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X_train, self.weights)\n",
    "            err = -y_train + self.__sigmoid(z)\n",
    "            \n",
    "            if self.penalty == 'L1':\n",
    "                if i == 0:\n",
    "                    print(self.penalty)\n",
    "                    \n",
    "                delta_w = np.dot(err, X_train)\n",
    "\n",
    "                # weight update\n",
    "                self.weights += -self.learningRate * delta_w\n",
    "                self.weights[1:] += -self.learningRate * self.lambd * self.weights[1:]\n",
    "                \n",
    "                # costs\n",
    "                self.costs.append(self.__reg_logLL1(z, y_train, self.weights))\n",
    "                \n",
    "            elif (self.penalty == 'L2') and (np.all(self.mu)) :\n",
    "                if i == 0:\n",
    "                    print(self.penalty)\n",
    "                \n",
    "                delta_w = np.dot(err, X_train)\n",
    "                \n",
    "                # weight update\n",
    "                self.weights += -self.learningRate * delta_w\n",
    "                self.weights[1:] += -self.learningRate * self.lambd * ((self.weights - self.mu)@(np.linalg.inv(self.ETL)) )[1:]\n",
    "            \n",
    "                # cost\n",
    "                self.costs.append(self.__reg_logLL2(z, y_train, self.weights))\n",
    "                \n",
    "            else:\n",
    "                if i == 0:\n",
    "                    print(self.penalty)\n",
    "                    \n",
    "                delta_w = -self.learningRate * np.dot(err, X_train)\n",
    "                \n",
    "                # weight update\n",
    "                self.weights += delta_w    \n",
    "                \n",
    "                # cost\n",
    "                self.costs.append(self.__logLL(z, y_train))\n",
    "                \n",
    "            if i % 30000 == 0:\n",
    "                print('weights: ',np.round(self.weights, 3))           \n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.__sigmoid((X @ self.weights[1:]) + self.weights[0])\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.round(self.predict_proba(X))\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        scores = ((y==y_pred)*1).mean()\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clf_params(data):\n",
    "    # Where the tranining data is stored\n",
    "    X = data['feat_train']\n",
    "    y = data['y']\n",
    "    \n",
    "    # Use this model when training subject as source \n",
    "    model_L1 = LogReg_TL(learningRate=0.001, num_iter=30000, penalty='L1', lambd=1)\n",
    "    \n",
    "    # Fit model and store weight\n",
    "    model_L1.fit(X, y)\n",
    "    data['ws'] = model_L1.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all source subject to compute weights\n",
    "for subj in TL_data['src'].keys():\n",
    "    print('Processing weight subject ', subj)\n",
    "    build_clf_params(TL_data['src'][subj]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Proposed Algorithm of this paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposed LTL algorithm attempts to improve the estimation of the classification parameters of a new subject by incorporating the data from previously recorded sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it treats **different feature spaces from the previous sessions similarly**, whereas the distribution of EEG signals can be different from session to session and from subject to subject, leading to different subject-specific CSP feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To address this issue, in the proposed weighted logistic regression-based transfer learning algorithm different weights **are allocated to the previously recorded sessions to represent similarities between these sessions and the new subject** in terms of distributions of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kullback-Leibler (KL) divergence is frequently used in the literature to calculate similarities between two sets of EEG features.\n",
    "\n",
    "Since in MI-based BCIs the features are typically normalized log-power of CSP filtered EEG data, they are commonly assumed normally distributed.\n",
    "\n",
    "Thus, in this paper, the KL divergence between two normal distributions are used to **measure divergence between EEG features**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two normal distributions presented as $\\mathcal {N}_{0}(\\boldsymbol {\\mu }_{0},\\boldsymbol {\\Sigma }_{0})$ and $\\mathcal {N}_{1}(\\boldsymbol {\\mu }_{1},\\boldsymbol {\\Sigma }_{1})$ , the KL divergence has the following closed form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} \\text {KL}[N_{0}\\vert \\vert N_{1}]=&0.5\\Biggl [(\\boldsymbol {\\mu }_{1} - \\boldsymbol {\\mu }_{0})^{T}\\boldsymbol {\\Sigma }_{1}^{-1}(\\boldsymbol {\\mu }_{1} - \\boldsymbol {\\mu }_{0}) \\\\&+\\,\\text {trace}(\\boldsymbol {\\Sigma }_{1}^{-1}\\boldsymbol {\\Sigma }_{0})-\\ln \\left({\\frac {\\text det(\\boldsymbol {\\Sigma }_{0})}{\\text det(\\boldsymbol {\\Sigma }_{1})}}\\right)-K \\Biggr ],\\tag{8}\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where det, T and K denote the determinant function, transpose of the matrix, and the dimension of the data, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the supervised case, the total divergence is calculated by averaging the KL divergences calculated for each class separately. \n",
    " On the other hand, in the unsupervised case, the total divergence equals to the KL divergence between the two sessions without considering the class labels. \n",
    "\n",
    " Subsequently, the similarity weight $\\alpha _{s}$ between the feature sets of the target subject $d_{t}$ and the feature sets of each of the previous sessions/subjects $d_{s}$ , is calculated as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*} \\alpha _{s} = \\frac {(1/\\bar {(\\text {KL}}[d_{t},d_{s}]+\\epsilon)^{4})}{\\sum \\limits _{i=1}^{m}(1/\\bar {\\text {(KL}}[d_{t},d_{i}]+\\epsilon)^{4})},\\tag{9}\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon =0.0001$ is used to ensure the stability of the equation when $\\bar {\\text {KL}}[d_{t},d_{s}]$ gets equal to zero due to having two compared distributions completely similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The power of 4 is applied to the inverse of KL between the distribution of two feature sets to give larger weights to more similar distributions and smaller weights to less similar distributions. This results in an increased sparsity in the similarity weights $\\alpha _{s}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposed weighted logistic regression-based transfer learning algorithm has the same steps as the proposed LTL. However, instead of equal weights, different weights are assigned to the previously recorded sessions using (9). Accordingly, the new weighted Î¼ is obtained as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*} \\boldsymbol {\\mu }_{w} = \\sum _{s=1}^{m} \\alpha _{s} \\mathbf {w}_{s}.\\tag{10}\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, the weighted $\\boldsymbol {\\Sigma }_{TL}$ is calculated as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*} \\boldsymbol {\\Sigma }_{TL_{w}}\\!=\\frac {\\text {diag}\\left({\\sum _{s=1}^{m} (\\alpha _{s} \\mathbf {w}_{s} - \\boldsymbol {\\mu }_{w})(\\alpha _{s} \\mathbf {w}_{s} - \\boldsymbol {\\mu }_{w})^{T}}\\right)}{\\text {trace} \\left({\\sum _{s=1}^{m}(\\alpha _{s} \\mathbf {w}_{s} - \\boldsymbol {\\mu }_{w})(\\alpha _{s} \\mathbf {w}_{s} - \\boldsymbol {\\mu }_{w})^{T}}\\right)}.\\tag{11}\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, $\\text R_{TL}$ in (5) is calculated by replacing $\\boldsymbol {\\mu }$ and $\\boldsymbol {\\Sigma }_{TL}$ with $\\boldsymbol {\\mu }_{w}$ and $\\boldsymbol {\\Sigma }_{TL_{w}}$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code of KL divergence of each target to all source subjects thank again to https://github.com/orvindemsy/EA-wLTL/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the kl divergence\n",
    "def KL_div(P, Q):\n",
    "    # First convert to np array\n",
    "    P = np.array(P)\n",
    "    Q = np.array(Q)\n",
    "    \n",
    "    # Then compute their means, datain shape of samples x feat\n",
    "    mu_P = np.mean(P, axis=0)\n",
    "    mu_Q = np.mean(Q, axis=0)    \n",
    "    \n",
    "    # Compute their covariance\n",
    "    cov_P = np.cov(P, rowvar=False)\n",
    "    cov_Q = np.cov(Q, rowvar=False)    \n",
    "        \n",
    "    cov_Q_inv = np.linalg.inv(cov_Q)\n",
    "    \n",
    "    # Compute KL divergence\n",
    "    KL_div = np.log(np.linalg.det(cov_Q)/np.linalg.det(cov_P)) - mu_P.shape[0] + np.trace(cov_Q_inv@cov_P) + \\\n",
    "                (mu_P - mu_Q).T@cov_Q_inv@(mu_P - mu_Q)\n",
    "    \n",
    "    KL_div = 0.5 * KL_div\n",
    "    \n",
    "    return KL_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute kl divergence of target subject to each source subject\n",
    "def compute_all_kl_div(data):\n",
    "    '''\n",
    "    Parameter:\n",
    "    data, is the whole data containing target and source data\n",
    "    '''\n",
    "    \n",
    "    # Iterate over all subject\n",
    "    for tgt_subj in data['tgt'].keys():\n",
    "        kl_div_score = []\n",
    "        \n",
    "        print('=== Current target subject %02d === ' %tgt_subj)\n",
    "        # Separate into left and right class\n",
    "        yP = data['tgt'][tgt_subj]['ytr']\n",
    "        P_left = data['tgt'][tgt_subj]['feat_train'][yP==0]\n",
    "        P_right = data['tgt'][tgt_subj]['feat_train'][yP==1]\n",
    "        \n",
    "        \n",
    "        for src_subj in data['src'].keys():    \n",
    "            print('KL div with respect to source subject', src_subj)\n",
    "            \n",
    "            # Separate into left and right class\n",
    "            yQ = data['src'][src_subj]['y']\n",
    "            Q_left = data['src'][src_subj]['feat_train'][yQ==0]\n",
    "            Q_right = data['src'][src_subj]['feat_train'][yQ==1]\n",
    "            \n",
    "            # Compute kl div of each class, average, then append them\n",
    "            kl_left = KL_div(P_left, Q_left)\n",
    "            kl_right = KL_div(P_right, Q_right)\n",
    "            kl_div = (kl_left + kl_right)/2\n",
    "            kl_div_score.append(kl_div)\n",
    "            \n",
    "        # Zeroing source subject that acts as curetn target\n",
    "        kl_div_score[tgt_subj-1] = 0    \n",
    "            \n",
    "        # Store them back into current target subject\n",
    "        data['tgt'][tgt_subj]['kl_div'] = kl_div_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_all_kl_div(TL_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity weights $\\alpha_{s}$ Equation (9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_weights(kl):\n",
    "    KL_inv =[]\n",
    "    eps = 1e-2\n",
    "    \n",
    "    for val in kl:\n",
    "        if val != 0: \n",
    "            KL_inv.append(1/(val + eps**4))\n",
    "        \n",
    "    a_s = []\n",
    "    \n",
    "    for inv_val in KL_inv:\n",
    "        temp = inv_val/sum(KL_inv)    \n",
    "        a_s.append(temp)\n",
    "                \n",
    "    return a_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity weighst of all target subject\n",
    "for tgt_subj in TL_data['tgt'].keys():\n",
    "    kl = TL_data['tgt'][tgt_subj]['kl_div']\n",
    "    TL_data['tgt'][tgt_subj]['a_s'] = compute_similarity_weights(kl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol {\\Sigma }_{TL}$ Algorithm Equation (10), (11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define algorithm to compute ETL  \n",
    "\n",
    "def compute_ETL_and_mu_ws(data):\n",
    "    '''\n",
    "    NOTE THAT THIS CODE IS FOR WEIGHTED LTL, THIS USE a_s, weights of similarity from kl_divergence\n",
    "    Parameter:\n",
    "    data: contains data of all target and source subjects\n",
    "    '''\n",
    "    print('===== Computing ETL and mu_ws =======')\n",
    "    # Will compute the ETL of each target subject\n",
    "    for tgt_subj in data['tgt'].keys():\n",
    "        print('=== Target subject %d ====' %tgt_subj)\n",
    "        src_subj = [i for i in data['src'].keys() if i != tgt_subj]\n",
    "            \n",
    "        # First compute the mean of all source weights 'mu_ws' over all subjects\n",
    "        all_ws = []\n",
    "        \n",
    "        # similarity weights a_s of current target\n",
    "        a_s = data['tgt'][tgt_subj]['a_s']\n",
    "        \n",
    "        for a, s_subj in zip(a_s, src_subj):\n",
    "            print('Gathering weights from source subject', s_subj)\n",
    "            ws = TL_data['src'][s_subj]['ws']\n",
    "            ws = a * ws\n",
    "            all_ws.append(ws)\n",
    "\n",
    "        # Average of ws over all subjects, axis=0\n",
    "        mu_ws = np.mean(np.array(all_ws), axis=0)\n",
    "        \n",
    "        # This will add up all ws - mu_ws, to compute ETL\n",
    "        temp_ws = 0\n",
    "        for a, s_subj in zip(a_s, src_subj):\n",
    "            ws = TL_data['src'][s_subj]['ws']\n",
    "            ws_min_mu = (a*ws - mu_ws)[:, None] @ (a*ws - mu_ws)[None, :]\n",
    "            temp_ws += ws_min_mu\n",
    "\n",
    "        # Compute ETL, only contain diagonal element with zero elements to rest of elements\n",
    "        den = np.diag(temp_ws)\n",
    "        nom = np.trace(temp_ws)\n",
    "        ETL = np.diag(den/nom)\n",
    "\n",
    "        data['tgt'][tgt_subj]['ETL'] = ETL\n",
    "        data['tgt'][tgt_subj]['mu_ws'] = mu_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example training model wLTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_weighted_LTL(data):\n",
    "    '''\n",
    "    data: data containing one target subject, currently data = TL_data['tgt'][tgt_subj_no]\n",
    "    \n",
    "    '''\n",
    "    # Create stratified KFold instance\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Grab the train data of current target subject\n",
    "    Xtr = data['feat_train']\n",
    "    ytr = data['ytr']\n",
    "    Xte = data['feat_test']\n",
    "    yte = data['yte']\n",
    "\n",
    "    #print(Xtr.shape)\n",
    "    #print(ytr.shape)\n",
    "\n",
    "    # Then also defines the ETL and mu_ws\n",
    "    ETL = data['ETL']\n",
    "    mu_ws = data['mu_ws']\n",
    "\n",
    "    score_tr = []\n",
    "    best_score = 0\n",
    "\n",
    "    # Do k-fold cv using stratified kfold, will train on 10 samples:\n",
    "    for i, (idx_te, idx_tr) in enumerate(skf.split(Xtr, ytr)):\n",
    "        print('processing cv-', i+1)\n",
    "        \n",
    "        # Make sure there are 10 train index\n",
    "        assert len(idx_tr) == 10\n",
    "        assert (ytr[idx_tr] == 1).sum() == 5\n",
    "\n",
    "        # Define new model each iteration, so that weights is newly initialized\n",
    "        model_L2 = LogReg_TL(learningRate=0.001, num_iter=30000, penalty='L2', lambd=1, ETL=ETL, mu=mu_ws)\n",
    "\n",
    "        # Fit model into ten training samples samples\n",
    "        _ = model_L2.fit(Xtr[idx_tr], ytr[idx_tr])\n",
    "\n",
    "        # Evaluate on n-1 folds of training samples\n",
    "        curr_score = model_L2.score(Xtr[idx_te], ytr[idx_te])\n",
    "        score_tr.append(curr_score)\n",
    "        \n",
    "        print('best score:', best_score)\n",
    "        print('curr score:', curr_score)\n",
    "        \n",
    "        if best_score < curr_score:\n",
    "            print('found best')\n",
    "            best_score = curr_score\n",
    "            best_idx_tr = idx_tr\n",
    "\n",
    "    # Fit model using the best train index, obtain test score\n",
    "    model_L2 = LogReg_TL(learningRate=0.001, num_iter=30000, penalty='L2', lambd=1, ETL=ETL, mu=mu_ws)\n",
    "\n",
    "    _ = model_L2.fit(Xtr[best_idx_tr], ytr[best_idx_tr])\n",
    "    score_te = model_L2.score(Xte, yte)   \n",
    "\n",
    "    return np.mean(np.array(score_tr)), np.std(np.array(score_tr)), score_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subj in TL_data.keys():\n",
    "for tgt_subj in TL_data['tgt'].keys():\n",
    "    print('=== Processing target subject %02d ====' %tgt_subj)\n",
    "    temp_tgt = TL_data['tgt'][tgt_subj]\n",
    "    temp_tgt['score_tr_wLTL'], temp_tgt['score_std_wLTL'], temp_tgt['score_te_wLTL'] = model_weighted_LTL(TL_data['tgt'][tgt_subj])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
