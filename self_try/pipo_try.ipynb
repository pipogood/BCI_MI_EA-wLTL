{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All try code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\pipo\\notch_EDF\\sess1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\pipo\\notch_EDF\\sess2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\pipo\\notch_EDF\\sess3.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\NutF8\\notch_EDF\\sess1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\NutF8\\notch_EDF\\sess2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\NutF8\\notch_EDF\\sess3.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\AJpang\\notch_EDF\\sess1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\AJpang\\notch_EDF\\sess2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\AJpang\\notch_EDF\\sess3.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\Aoomim\\notch_EDF\\sess1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\Aoomim\\notch_EDF\\sess2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\Aoomim\\notch_EDF\\sess3.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\voen\\notch_EDF\\sess1.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\voen\\notch_EDF\\sess2.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\git\\Senior_Thesis\\DataSet\\Offline_Experiment\\voen\\notch_EDF\\sess3.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Successful to create Data of ['pipo', 'NutF8', 'AJpang', 'Aoomim', 'voen']\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from mne.datasets import eegbci\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from mne.channels import make_standard_montage\n",
    "from scipy import signal\n",
    "from scipy.linalg import sqrtm, inv \n",
    "\n",
    "def GetRawEDF(target_subjects= \"all\", condition=\"offline\"):\n",
    "\n",
    "    EEG_data = {}\n",
    "\n",
    "    if condition == \"offline\":\n",
    "        condition = \"Offline_Experiment\"\n",
    "    elif condition == \"online\":\n",
    "        condition = \"Online_Experiment\"\n",
    "\n",
    "    if target_subjects == \"all\":\n",
    "        target_subjects = [\"pipo\",\"NutF8\",\"AJpang\",\"Aoomim\",\"voen\"]\n",
    "\n",
    "    for i in range (0,len(target_subjects)):\n",
    "\n",
    "        path = \"C:\\\\git\\Senior_Thesis\\\\DataSet\\\\\"+condition+\"\\\\\"+ target_subjects[i] +\"\\\\notch_EDF\\\\\"\n",
    "        list_dir = listdir(path)\n",
    "        raw_each = [0] * len(list_dir)\n",
    "        for j in range(len(list_dir)):\n",
    "            raw_each[j] = mne.io.read_raw_edf(path+list_dir[j],preload = False)\n",
    "            \n",
    "        raw_edf = mne.concatenate_raws(raw_each)\n",
    "\n",
    "        eegbci.standardize(raw_edf)  # set channel names\n",
    "        montage = make_standard_montage(\"standard_1005\")\n",
    "        raw_edf.set_montage(montage)\n",
    "\n",
    "        EEG_data[target_subjects[i]] = {\"Raw_data\": raw_edf.copy()}\n",
    "\n",
    "    print(f\"Successful to create Data of {target_subjects}\")\n",
    "\n",
    "    return EEG_data\n",
    "\n",
    "EEG_data = GetRawEDF(target_subjects = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipo\n",
      "nutF8\n",
      "AJpang\n",
      "Aoomim\n"
     ]
    }
   ],
   "source": [
    "target_sub = [\"pipo\",\"nutF8\",\"AJpang\",\"Aoomim\"]\n",
    "\n",
    "a = [0,0,0,0]\n",
    "\n",
    "EEG_data = {}\n",
    "\n",
    "for i in target_sub:\n",
    "    EEG_data[i] = {\"Raw_data\": a.copy()}\n",
    "\n",
    "nor = [5,5,5]\n",
    "\n",
    "for key in EEG_data:\n",
    "    print(key)\n",
    "    EEG_data[key][\"EA_data\"] = nor\n",
    "\n",
    "\n",
    "# for sub_key in EEG_data:\n",
    "#     for data_key in EEG_data[sub_key]:\n",
    "#         print(sub_key,data_key)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pipo': {'data': [0, 0, 0, 0], 'EA_data': [5, 5, 5]},\n",
       " 'nutF8': {'data': [0, 0, 0, 0], 'EA_data': [5, 5, 5]},\n",
       " 'AJpang': {'data': [0, 0, 0, 0], 'EA_data': [5, 5, 5]},\n",
       " 'Aoomim': {'data': [0, 0, 0, 0], 'EA_data': [5, 5, 5]}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EEG_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try CSP with multiple subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from mne.decoding import CSP\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit,StratifiedKFold ,cross_val_score, cross_val_predict, KFold\n",
    "\n",
    "def computeCSPFeatures(data, target_subjects,condition):\n",
    "\n",
    "    all_data = None\n",
    "    label = None\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        query = \"Raw_Epoch\"\n",
    "    else:\n",
    "        query = \"EA_Epoch\"\n",
    "\n",
    "    for sub in target_subjects:\n",
    "        if all_data is None:\n",
    "            all_data = data[sub]['Raw_Epoch']\n",
    "        else:\n",
    "            all_data = np.concatenate((all_data, data[sub][query]), axis=0)\n",
    "\n",
    "        if label is None:\n",
    "            label = data[sub]['label']\n",
    "        else:\n",
    "            label = np.concatenate((label, data[sub]['label']), axis=0)\n",
    "\n",
    "\n",
    "    print(np.array(all_data).shape, np.array(label).shape)\n",
    "    \n",
    "    csp = CSP(n_components=8, reg=None, log=None, rank= 'info')\n",
    "\n",
    "    train_data_csp, label = shuffle(all_data, label, random_state = 0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_data_csp, label, test_size= 0.3, random_state = 0, stratify=label)\n",
    "    csp.fit(X_train, y_train)\n",
    "\n",
    "    X_train = csp.transform(X_train)\n",
    "    X_test  = csp.transform(X_test)\n",
    "\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    score = cross_val_score(lda, X_train, y_train, cv= 10)\n",
    "    print(\"LDA only Cross-validation scores:\", np.mean(score))\n",
    "    lda.fit(X_train, y_train)\n",
    "        \n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "    y_pred = lda.predict(X_train)\n",
    "\n",
    "    print(\"Classification TRAIN DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_train, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_train, y_pred=y_pred))\n",
    "\n",
    "    y_pred = lda.predict(X_test)\n",
    "\n",
    "    print(\"Classification TEST DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "computeCSPFeatures(EEG_Epochs, target_subjects = [\"pipo\",\"voen\",\"AJpang\"] ,condition = \"noEA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class LogReg_TL(BaseEstimator):\n",
    "\n",
    "    def __init__(self, learningRate=1e-5, num_iter=100, penalty=None, intercept = True,\\\n",
    "                 lambd=1, Sigma_TL=np.array([[0, 0],[0, 0]]), mu=0):\n",
    "        \n",
    "        self.learningRate = learningRate\n",
    "        self.num_iter = num_iter\n",
    "        self.penalty = penalty\n",
    "        self.intercept = intercept\n",
    "        self.Sigma_TL = Sigma_TL\n",
    "        self.lambd = lambd\n",
    "        self.mu = mu\n",
    "\n",
    "    def __softmax(self,z): #Change from sigmoid to softmax for multi-classification\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def __reg_logLL1(self,z, y, weights): # cal sum of negative log-likelihood (2)\n",
    "        reg = self.lambd * np.linalg.norm(weights)**2\n",
    "        return (-1 * np.sum((y * np.log10(self.__softmax(z))) + ((1 - y) * np.log10(1 - self.__softmax(z)))) ) + reg\n",
    "    \n",
    "    def __reg_logLL2(self, z, y, weights): # cal L2 weight (target_subjects) (4)\n",
    "        Sigma_TL_det = np.log10(np.linalg.det(self.Sigma_TL))\n",
    "        reg = 0.5 * self.lambd * np.sum( ((weights-self.mu)**2)@self.Sigma_TL) + Sigma_TL_det \n",
    "\n",
    "        return (-1 * np.sum((y * np.log10(self.__softmax(z))) + ((1 - y) * np.log10(1 - self.__softmax(z)))) ) + reg\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.costs = []\n",
    "\n",
    "        n_classes = np.unique(y_train).size\n",
    "\n",
    "        if self.intercept:\n",
    "            X_train = np.c_[np.ones([np.shape(X_train)[0], 1]), X_train]\n",
    "\n",
    "        self.weights = np.zeros((np.shape(X_train)[1], n_classes))\n",
    "\n",
    "        y_train_onehot = np.eye(n_classes)[y_train]\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X_train, self.weights)\n",
    "            err = self.__softmax(z) - y_train_onehot\n",
    "            # print(self.__softmax(z))\n",
    "\n",
    "            if self.penalty == 'L1':\n",
    "                \n",
    "                # weight update\n",
    "                delta_w = np.dot(X_train.T, err)\n",
    "                self.weights += -self.learningRate * delta_w\n",
    "                self.weights[1:] += -self.learningRate *self.lambd * self.weights[1:]\n",
    "                \n",
    "                # costs\n",
    "                self.costs.append(self.__reg_logLL1(z, y_train_onehot, self.weights))\n",
    "\n",
    "            elif self.penalty == 'L2':\n",
    "\n",
    "                # weight update\n",
    "                delta_w = np.dot(X_train.T, err)\n",
    "                self.weights += -self.learningRate * delta_w\n",
    "                self.weights[1:] += -self.learningRate *self.lambd * ((self.weights - self.mu)@(np.linalg.inv(self.Sigma_TL)))[1:]\n",
    "\n",
    "                self.costs.append(self.__reg_logLL2(z, y_train_onehot, self.weights))\n",
    "                \n",
    "\n",
    "        return self\n",
    "\n",
    "def build_clf_params(data, target_subjects ,condition):\n",
    "\n",
    "    for sub in data.keys():\n",
    "\n",
    "        if sub  == target_subjects: #Don't apply weight to target subject\n",
    "            pass \n",
    "\n",
    "        else:\n",
    "            # Where the tranining data is stored\n",
    "            if condition == \"noEA\":\n",
    "                X = data[sub]['Raw_csp']\n",
    "                y = data[sub]['Raw_csp_label']\n",
    "                store_ws = 'ws_Raw'\n",
    "\n",
    "            else:\n",
    "                X = data[sub]['EA_csp']\n",
    "                y = data[sub]['EA_csp_label']\n",
    "                store_ws = 'ws_EA'\n",
    "            \n",
    "            # Use this model when training subject as source \n",
    "            model_L1 = LogReg_TL(learningRate=0.001, num_iter=30000, penalty='L1', lambd=1)\n",
    "            \n",
    "            # Fit model and store weight\n",
    "            model_L1.fit(X, y)\n",
    "            print(\"weights of \", str(sub), \": \", model_L1.weights)\n",
    "            print(\"costs of \", str(sub), \": \", model_L1.costs[len(model_L1.costs)-1])\n",
    "            data[sub][store_ws] = model_L1.weights\n",
    "\n",
    "# X_train = np.random.rand(120, 5)\n",
    "# y_train = np.random.choice([0, 1, 2, 3], size=120)\n",
    "\n",
    "build_clf_params(CSP2D_Epoch, target_subjects= target_data ,condition = \"EA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class LogReg_TL(BaseEstimator):\n",
    "\n",
    "    def __init__(self, learningRate=1e-5, num_iter=100, penalty=None, intercept = True,\\\n",
    "                 lambd=1, Sigma_TL=np.array([[0, 0],[0, 0]]), mu=0):\n",
    "        \n",
    "        self.learningRate = learningRate\n",
    "        self.num_iter = num_iter\n",
    "        self.penalty = penalty\n",
    "        self.intercept = intercept\n",
    "        self.Sigma_TL = Sigma_TL\n",
    "        self.lambd = lambd\n",
    "        self.mu = mu\n",
    "\n",
    "    def softmax(self,z): #Change from sigmoid to softmax for multi-classification\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def reg_logLL1(self, ws, X, y): # cal sum of negative log-likelihood (2)\n",
    "        # Compute predictions\n",
    "        logits = np.dot(X, ws)\n",
    "        predictions = self.softmax(logits)\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        error = -np.sum(y * np.log(predictions))\n",
    "        cost = error / m + (self.lambd / 2) * np.sum(ws**2)  # Regularization term\n",
    "\n",
    "        # Compute gradient\n",
    "        gradient = np.dot(X.T, (predictions - y)) / m + self.lambd * ws\n",
    "        return cost, gradient\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.costs = 0.0\n",
    "        n_classes = np.unique(y_train).size\n",
    "        self.weights = np.zeros((np.shape(X_train)[1], n_classes))\n",
    "        y_train_onehot = np.eye(n_classes)[y_train]\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "\n",
    "            if self.penalty == 'L1':\n",
    "                \n",
    "                self.costs, gradient = self.reg_logLL1(self.weights, X_train, y_train_onehot)\n",
    "                self.weights -= self.learningRate * gradient\n",
    "                \n",
    "        return self\n",
    "\n",
    "def build_clf_params(data, target_subjects ,condition):\n",
    "\n",
    "    for sub in data.keys():\n",
    "\n",
    "        if sub  == target_subjects: #Don't apply weight to target subject\n",
    "            pass \n",
    "\n",
    "        else:\n",
    "            # Where the tranining data is stored\n",
    "            if condition == \"noEA\":\n",
    "                X = data[sub]['Raw_csp']\n",
    "                y = data[sub]['Raw_csp_label']\n",
    "                store_ws = 'ws_Raw'\n",
    "\n",
    "            else:\n",
    "                X = data[sub]['EA_csp']\n",
    "                y = data[sub]['EA_csp_label']\n",
    "                store_ws = 'ws_EA'\n",
    "            \n",
    "            # Use this model when training subject as source \n",
    "            model_L1 = LogReg_TL(learningRate=0.001, num_iter=30000, penalty='L1', lambd=1)\n",
    "            \n",
    "            # Fit model and store weight\n",
    "            model_L1.fit(X, y)\n",
    "            print(\"weights of \", str(sub), \": \", model_L1.weights)\n",
    "            print(\"costs of \", str(sub), \": \", model_L1.costs)\n",
    "            data[sub][store_ws] = model_L1.weights\n",
    "\n",
    "build_clf_params(CSP2D_Epoch, target_subjects= target_data ,condition = \"EA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_14 (Conv1D)          (None, 511, 32)           512       \n",
      "                                                                 \n",
      " max_pooling1d_14 (MaxPooli  (None, 255, 32)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 255, 32)           0         \n",
      "                                                                 \n",
      " conv1d_15 (Conv1D)          (None, 253, 64)           6208      \n",
      "                                                                 \n",
      " max_pooling1d_15 (MaxPooli  (None, 126, 64)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 126, 64)           0         \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, 126, 5)            1065      \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDi  (None, 126, 5)            30        \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " global_average_pooling1d_7  (None, 5)                 0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 4)                 24        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7839 (30.62 KB)\n",
      "Trainable params: 7839 (30.62 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GRU, Dense, GlobalAveragePooling1D, TimeDistributed, Dropout\n",
    "\n",
    "# Define the number of output classes\n",
    "n_classes = 4  # Assuming binary classification\n",
    "\n",
    "# Define the model using Sequential\n",
    "model = Sequential([\n",
    "    # 1D Convolution Layer\n",
    "    Conv1D(32, kernel_size=3, strides=1, activation='relu', input_shape=(513, 5)),\n",
    "    \n",
    "    # Max Pooling Layer\n",
    "    MaxPooling1D(pool_size=3, strides=2),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Second 1D Convolution Layer\n",
    "    Conv1D(64, kernel_size=3, strides=1, activation='relu'),\n",
    "    \n",
    "    # Second Max Pooling Layer\n",
    "    MaxPooling1D(pool_size=3, strides=2),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # GRU Layer\n",
    "    GRU(5, return_sequences=True),\n",
    "    \n",
    "    # Time Distributed Dense Layer\n",
    "    TimeDistributed(Dense(5, activation='relu')),\n",
    "    \n",
    "    # Global Average Pooling Layer\n",
    "    GlobalAveragePooling1D(),\n",
    "    \n",
    "    # Dense Layer for output (adjusted for binary classification)\n",
    "    Dense(n_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
