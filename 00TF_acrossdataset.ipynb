{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_utilize import Unicorn\n",
    "from OtherData_utilize import Physionet, BCIcompet2a\n",
    "\n",
    "target_class = [\"Left\", \"Right\"] \n",
    "num_subject_physionet = 109\n",
    "condition_wLTL = \"noEA\"\n",
    "\n",
    "load_wLTL_weight = False  #saved weight of 109 physionet subject\n",
    "EA_wLTL_weight = \"EA_wLTL_LRN.pkl\"\n",
    "noEA_wLTL_weight = \"noEA_wLTL_4class.pkl\"\n",
    "\n",
    "\n",
    "AllBCIClass = Unicorn(selectclass = target_class, desired_fz = 128, ch_pick = ['Fz','C3','Cz','C4','Pz'])\n",
    "Class_compet = BCIcompet2a(selectclass = target_class, desired_fz = 128, ch_pick = ['EEG-Fz', 'EEG-Cz', 'EEG-C3', 'EEG-C4', 'EEG-Pz'])\n",
    "Class_Physio = Physionet(selectclass = target_class, desired_fz = 128, ch_pick =  ['Fz..','C3..', 'Cz..','C4..','Pz..'])\n",
    "\n",
    "target_data_0 = \"pipo\" #this subject will be test_set otherwise are train_set\n",
    "calibrate_size = 20 # (trial)\n",
    "train_svm = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicorn hybrid black dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful to create Data of ['pipo']\n",
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n"
     ]
    }
   ],
   "source": [
    "subjectForTargetData = [target_data_0]\n",
    "# subjectForTargetData = \"all\"\n",
    "EEG_data = AllBCIClass.GetRawEDF(target_subjects= subjectForTargetData, condition=\"Offline_Experiment\")\n",
    "Unicorn_Epochs = AllBCIClass.GetEpoch(EEG_data ,tmin= -2.0, tmax= 6.0, crop = (0,4) ,baseline= (-0.5,0.0), band_pass=(6,32),trial_removal_th = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCI Compettition 2a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n",
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n",
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:258: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:258: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '32766', '768', '769', '770', '771', '772']\n",
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:258: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:258: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n",
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:258: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:258: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n",
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:258: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:258: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:258: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    }
   ],
   "source": [
    "EEG_data = Class_compet.GetRaw(target_subjects=\"all\", preload=True)\n",
    "EEG_compet_Epochs = Class_compet.GetEpoch(EEG_data ,tmin= -1.0, tmax= 4.0, crop = (0,4) ,baseline=(-0.5,0.0),band_pass=(6,32),trial_removal_th = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physionet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing subject number:  1\n",
      "processing subject number:  2\n",
      "processing subject number:  3\n",
      "processing subject number:  4\n",
      "processing subject number:  5\n",
      "processing subject number:  6\n",
      "processing subject number:  7\n",
      "processing subject number:  8\n",
      "processing subject number:  9\n",
      "processing subject number:  10\n",
      "processing subject number:  11\n",
      "processing subject number:  12\n",
      "processing subject number:  13\n",
      "processing subject number:  14\n",
      "processing subject number:  15\n",
      "processing subject number:  16\n",
      "processing subject number:  17\n",
      "processing subject number:  18\n",
      "processing subject number:  19\n",
      "processing subject number:  20\n",
      "processing subject number:  21\n",
      "processing subject number:  22\n",
      "processing subject number:  23\n",
      "processing subject number:  24\n",
      "processing subject number:  25\n",
      "processing subject number:  26\n",
      "processing subject number:  27\n",
      "processing subject number:  28\n",
      "processing subject number:  29\n",
      "processing subject number:  30\n",
      "processing subject number:  31\n",
      "processing subject number:  32\n",
      "processing subject number:  33\n",
      "processing subject number:  34\n",
      "processing subject number:  35\n",
      "processing subject number:  36\n",
      "processing subject number:  37\n",
      "processing subject number:  38\n",
      "processing subject number:  39\n",
      "processing subject number:  40\n",
      "processing subject number:  41\n",
      "processing subject number:  42\n",
      "processing subject number:  43\n",
      "processing subject number:  44\n",
      "processing subject number:  45\n",
      "processing subject number:  46\n",
      "processing subject number:  47\n",
      "processing subject number:  48\n",
      "processing subject number:  49\n",
      "processing subject number:  50\n",
      "processing subject number:  51\n",
      "processing subject number:  52\n",
      "processing subject number:  53\n",
      "processing subject number:  54\n",
      "processing subject number:  55\n",
      "processing subject number:  56\n",
      "processing subject number:  57\n",
      "processing subject number:  58\n",
      "processing subject number:  59\n",
      "processing subject number:  60\n",
      "processing subject number:  61\n",
      "processing subject number:  62\n",
      "processing subject number:  63\n",
      "processing subject number:  64\n",
      "processing subject number:  65\n",
      "processing subject number:  66\n",
      "processing subject number:  67\n",
      "processing subject number:  68\n",
      "processing subject number:  69\n",
      "processing subject number:  70\n",
      "processing subject number:  71\n",
      "processing subject number:  72\n",
      "processing subject number:  73\n",
      "processing subject number:  74\n",
      "processing subject number:  75\n",
      "processing subject number:  76\n",
      "processing subject number:  77\n",
      "processing subject number:  78\n",
      "processing subject number:  79\n",
      "processing subject number:  80\n",
      "processing subject number:  81\n",
      "processing subject number:  82\n",
      "processing subject number:  83\n",
      "processing subject number:  84\n",
      "processing subject number:  85\n",
      "processing subject number:  86\n",
      "processing subject number:  87\n",
      "processing subject number:  88\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "processing subject number:  89\n",
      "processing subject number:  90\n",
      "processing subject number:  91\n",
      "processing subject number:  92\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "processing subject number:  93\n",
      "processing subject number:  94\n",
      "processing subject number:  95\n",
      "processing subject number:  96\n",
      "processing subject number:  97\n",
      "processing subject number:  98\n",
      "processing subject number:  99\n",
      "processing subject number:  100\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "processing subject number:  101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:65: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw_RorL1 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R04.edf\",preload = True, verbose=False)\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:66: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw_RorL2 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R08.edf\",preload = True, verbose=False)\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:67: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw_RorL3 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R12.edf\",preload = True, verbose=False)\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:78: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw_Both1 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R06.edf\",preload = True, verbose=False)\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:79: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw_Both2 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R10.edf\",preload = True, verbose=False)\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:80: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw_Both3 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R14.edf\",preload = True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing subject number:  102\n",
      "processing subject number:  103\n",
      "processing subject number:  104\n",
      "processing subject number:  105\n",
      "processing subject number:  106\n",
      "processing subject number:  107\n",
      "processing subject number:  108\n",
      "processing subject number:  109\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "148 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "148 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "P059 -318.4516872532656 15\n",
      "P059 1038.9442350925492 15\n",
      "P059 -1074.055918632143 16\n",
      "P059 299.1623359364581 16\n",
      "P059 -1196.6689362296906 20\n",
      "P059 274.9148497256546 20\n",
      "P059 -1147.5470456986693 70\n",
      "P059 178.30133611108454 70\n",
      "P059 -1104.3980624949231 97\n",
      "P059 81.10133720571451 97\n",
      "P059 -1091.8251635226216 131\n",
      "P059 20.68441134035783 131\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "148 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "147 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "148 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "147 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "P084 -1019.3821703246222 141\n",
      "P084 774.6617274743861 141\n",
      "P084 -1070.1889576620583 142\n",
      "P084 1020.7164674663519 142\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "188 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P095 -142.26261475457747 118\n",
      "P095 1047.8370281991654 118\n",
      "P095 -148.75865329411093 119\n",
      "P095 1006.5569166385028 119\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "120 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "148 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "147 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    }
   ],
   "source": [
    "RAW_data_RorL, RAW_data_Both = Class_Physio.GetRaw(num_subject= num_subject_physionet)\n",
    "EEG_physio_Epochs = Class_Physio.Get_epoch(RAW_data_RorL, RAW_data_Both, tmin=-2.0, tmax=4.0, crop=(0,4),baseline = (-0.5,0.0),band_pass= (6,32),trial_removal_th = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all dictionaries into a single dictionary\n",
    "combined_epochs = {**Unicorn_Epochs, **EEG_compet_Epochs, **EEG_physio_Epochs}\n",
    "calibrate_size = calibrate_size / combined_epochs[target_data_0]['Raw_Epoch'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear Memory\n",
    "del Unicorn_Epochs\n",
    "del EEG_compet_Epochs\n",
    "del EEG_physio_Epochs\n",
    "del RAW_data_RorL\n",
    "del RAW_data_Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllBCIClass.ComputeEA(combined_epochs, target_subject= target_data_0, calibrate_size=calibrate_size)\n",
    "if calibrate_size != 0:\n",
    "    target_data = target_data_0 + \"_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.58      0.61      0.60      3123\n",
      "       Right       0.59      0.55      0.57      3082\n",
      "\n",
      "    accuracy                           0.58      6205\n",
      "   macro avg       0.58      0.58      0.58      6205\n",
      "weighted avg       0.58      0.58      0.58      6205\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[1914 1209]\n",
      " [1375 1707]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.67      1.00      0.80        20\n",
      "       Right       1.00      0.50      0.67        20\n",
      "\n",
      "    accuracy                           0.75        40\n",
      "   macro avg       0.83      0.75      0.73        40\n",
      "weighted avg       0.83      0.75      0.73        40\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[20  0]\n",
      " [10 10]]\n"
     ]
    }
   ],
   "source": [
    "AllBCIClass.classifyCSP_LDA(combined_epochs, target_subjects= target_data, condition = \"noEA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.63      0.64      0.64      3123\n",
      "       Right       0.63      0.62      0.63      3082\n",
      "\n",
      "    accuracy                           0.63      6205\n",
      "   macro avg       0.63      0.63      0.63      6205\n",
      "weighted avg       0.63      0.63      0.63      6205\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[2008 1115]\n",
      " [1159 1923]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.86      0.90      0.88        20\n",
      "       Right       0.89      0.85      0.87        20\n",
      "\n",
      "    accuracy                           0.88        40\n",
      "   macro avg       0.88      0.88      0.87        40\n",
      "weighted avg       0.88      0.88      0.87        40\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[18  2]\n",
      " [ 3 17]]\n"
     ]
    }
   ],
   "source": [
    "AllBCIClass.classifyCSP_LDA(combined_epochs, target_subjects= target_data, condition = \"EA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_svm == True:\n",
    "    AllBCIClass.classifyCSP_SVM(combined_epochs, target_subjects= target_data, condition = \"noEA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_svm == True:\n",
    "    AllBCIClass.classifyCSP_SVM(combined_epochs, target_subjects= target_data, condition = \"EA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "CSP2D_Epoch = AllBCIClass.computeCSPFeatures(combined_epochs, target_subject = target_data , target_subject_0= target_data_0) #For wLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train wLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't make the python class yet because I need to check the correction of this purpose later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.169079065322876\n",
      "Epoch 20, Loss: 1.0500757694244385\n",
      "Epoch 40, Loss: 0.9507477879524231\n",
      "Epoch 60, Loss: 0.9390997886657715\n",
      "Epoch 80, Loss: 0.9359259605407715\n",
      "Epoch 100, Loss: 0.9321447610855103\n",
      "Epoch 120, Loss: 0.9284084439277649\n",
      "Epoch 140, Loss: 0.9245835542678833\n",
      "Epoch 160, Loss: 0.9207533001899719\n",
      "Epoch 180, Loss: 0.916989266872406\n",
      "Epoch 200, Loss: 0.9133560657501221\n",
      "Epoch 220, Loss: 0.9099137783050537\n",
      "Epoch 240, Loss: 0.906717836856842\n",
      "Epoch 260, Loss: 0.9038194417953491\n",
      "Epoch 280, Loss: 0.9012657403945923\n",
      "Epoch 300, Loss: 0.8990998268127441\n",
      "Epoch 320, Loss: 0.8973612189292908\n",
      "Epoch 340, Loss: 0.8960857391357422\n",
      "Epoch 360, Loss: 0.8953057527542114\n",
      "Epoch 380, Loss: 0.8950496315956116\n",
      "Epoch 400, Loss: 0.8953428864479065\n",
      "Epoch 420, Loss: 0.8962073922157288\n",
      "Epoch 440, Loss: 0.8976619243621826\n",
      "Epoch 460, Loss: 0.8997220993041992\n",
      "Epoch 480, Loss: 0.9024006128311157\n",
      "weights of  A01 :  [[array([[-0.4028435 , -0.05904644],\n",
      "       [ 0.13910949,  0.12767859],\n",
      "       [ 0.7166452 , -0.05034139],\n",
      "       [-0.3873018 ,  0.606939  ],\n",
      "       [-0.2748409 , -0.9644816 ]], dtype=float32), array([-0.01943904,  0.01943904], dtype=float32)]]\n",
      "Lowest loss of  A01 :  0.89504963\n",
      "Epoch 0, Loss: 4.880281925201416\n",
      "Epoch 20, Loss: 1.3446226119995117\n",
      "Epoch 40, Loss: 1.0195003747940063\n",
      "Epoch 60, Loss: 0.9347875714302063\n",
      "Epoch 80, Loss: 0.922761082649231\n",
      "Epoch 100, Loss: 0.920944094657898\n",
      "Epoch 120, Loss: 0.920113205909729\n",
      "Epoch 140, Loss: 0.9192240238189697\n",
      "Epoch 160, Loss: 0.9182800054550171\n",
      "Epoch 180, Loss: 0.917300820350647\n",
      "Epoch 200, Loss: 0.916287362575531\n",
      "Epoch 220, Loss: 0.9152455925941467\n",
      "Epoch 240, Loss: 0.9141819477081299\n",
      "Epoch 260, Loss: 0.9131020307540894\n",
      "Epoch 280, Loss: 0.9120110273361206\n",
      "Epoch 300, Loss: 0.91091388463974\n",
      "Epoch 320, Loss: 0.9098153710365295\n",
      "Epoch 340, Loss: 0.9087197184562683\n",
      "Epoch 360, Loss: 0.9076313972473145\n",
      "Epoch 380, Loss: 0.9065542221069336\n",
      "Epoch 400, Loss: 0.9054921269416809\n",
      "Epoch 420, Loss: 0.9044486284255981\n",
      "Epoch 440, Loss: 0.9034271836280823\n",
      "Epoch 460, Loss: 0.9024308919906616\n",
      "Epoch 480, Loss: 0.9014627933502197\n",
      "weights of  A02 :  [[array([[-0.02972008,  0.3726919 ],\n",
      "       [ 0.55604833, -0.26178125],\n",
      "       [-0.35675862, -0.32691705],\n",
      "       [-0.19316241, -0.6215748 ],\n",
      "       [-0.92866343, -0.11076054]], dtype=float32), array([ 0.25749615, -0.25749615], dtype=float32)]]\n",
      "Lowest loss of  A02 :  0.90057194\n",
      "Epoch 0, Loss: 4.8723464012146\n",
      "Epoch 20, Loss: 1.6654632091522217\n",
      "Epoch 40, Loss: 1.1403477191925049\n",
      "Epoch 60, Loss: 1.0230287313461304\n",
      "Epoch 80, Loss: 0.9881157875061035\n",
      "Epoch 100, Loss: 0.9634924530982971\n",
      "Epoch 120, Loss: 0.9390208125114441\n",
      "Epoch 140, Loss: 0.9148687720298767\n",
      "Epoch 160, Loss: 0.8911769986152649\n",
      "Epoch 180, Loss: 0.8683961629867554\n",
      "Epoch 200, Loss: 0.8468924760818481\n",
      "Epoch 220, Loss: 0.8269526362419128\n",
      "Epoch 240, Loss: 0.8088067173957825\n",
      "Epoch 260, Loss: 0.7926272749900818\n",
      "Epoch 280, Loss: 0.7785353660583496\n",
      "Epoch 300, Loss: 0.7666075229644775\n",
      "Epoch 320, Loss: 0.7568814754486084\n",
      "Epoch 340, Loss: 0.7493627071380615\n",
      "Epoch 360, Loss: 0.7440295219421387\n",
      "Epoch 380, Loss: 0.7408386468887329\n",
      "Epoch 400, Loss: 0.7397302985191345\n",
      "Epoch 420, Loss: 0.7406319379806519\n",
      "Epoch 440, Loss: 0.7434622049331665\n",
      "Epoch 460, Loss: 0.7481340169906616\n",
      "Epoch 480, Loss: 0.7545570135116577\n",
      "weights of  A03 :  [[array([[ 0.27000058,  0.33697778],\n",
      "       [-0.9074531 , -0.29142675],\n",
      "       [ 0.5762945 , -0.4578985 ],\n",
      "       [-0.23520248,  0.439497  ],\n",
      "       [ 0.43681985, -0.01081352]], dtype=float32), array([-0.27298135,  0.2729813 ], dtype=float32)]]\n",
      "Lowest loss of  A03 :  0.7397283\n",
      "Epoch 0, Loss: 1.9280415773391724\n",
      "Epoch 20, Loss: 1.0454250574111938\n",
      "Epoch 40, Loss: 0.9901299476623535\n",
      "Epoch 60, Loss: 0.9916994571685791\n",
      "Epoch 80, Loss: 0.9905886650085449\n",
      "Epoch 100, Loss: 0.9911004900932312\n",
      "Epoch 120, Loss: 0.991805911064148\n",
      "Epoch 140, Loss: 0.9927250146865845\n",
      "Epoch 160, Loss: 0.9938806891441345\n",
      "Epoch 180, Loss: 0.9953018426895142\n",
      "Epoch 200, Loss: 0.9970161318778992\n",
      "Epoch 220, Loss: 0.9990495443344116\n",
      "Epoch 240, Loss: 1.0014266967773438\n",
      "Epoch 260, Loss: 1.0041708946228027\n",
      "Epoch 280, Loss: 1.0073034763336182\n",
      "Epoch 300, Loss: 1.010845422744751\n",
      "Epoch 320, Loss: 1.0148154497146606\n",
      "Epoch 340, Loss: 1.019230842590332\n",
      "Epoch 360, Loss: 1.024107813835144\n",
      "Epoch 380, Loss: 1.0294603109359741\n",
      "Epoch 400, Loss: 1.0353014469146729\n",
      "Epoch 420, Loss: 1.041642189025879\n",
      "Epoch 440, Loss: 1.0484927892684937\n",
      "Epoch 460, Loss: 1.055860996246338\n",
      "Epoch 480, Loss: 1.0637540817260742\n",
      "weights of  A04 :  [[array([[-0.8704426 , -0.05793263],\n",
      "       [-0.43080372,  0.6446025 ],\n",
      "       [ 0.04110013, -0.539778  ],\n",
      "       [-0.39927647, -0.5243126 ],\n",
      "       [ 0.77113545, -0.77375335]], dtype=float32), array([ 0.08454917, -0.08454916], dtype=float32)]]\n",
      "Lowest loss of  A04 :  0.98999035\n",
      "Epoch 0, Loss: 1.3758846521377563\n",
      "Epoch 20, Loss: 1.0380682945251465\n",
      "Epoch 40, Loss: 1.015573501586914\n",
      "Epoch 60, Loss: 1.0093355178833008\n",
      "Epoch 80, Loss: 1.002792477607727\n",
      "Epoch 100, Loss: 0.9955779314041138\n",
      "Epoch 120, Loss: 0.9878189563751221\n",
      "Epoch 140, Loss: 0.9795904159545898\n",
      "Epoch 160, Loss: 0.9710038304328918\n",
      "Epoch 180, Loss: 0.9621469974517822\n",
      "Epoch 200, Loss: 0.9530931115150452\n",
      "Epoch 220, Loss: 0.9439119100570679\n",
      "Epoch 240, Loss: 0.9346645474433899\n",
      "Epoch 260, Loss: 0.9254063963890076\n",
      "Epoch 280, Loss: 0.9161872267723083\n",
      "Epoch 300, Loss: 0.9070518016815186\n",
      "Epoch 320, Loss: 0.8980405926704407\n",
      "Epoch 340, Loss: 0.8891894817352295\n",
      "Epoch 360, Loss: 0.8805307149887085\n",
      "Epoch 380, Loss: 0.8720926642417908\n",
      "Epoch 400, Loss: 0.8639001846313477\n",
      "Epoch 420, Loss: 0.8559752702713013\n",
      "Epoch 440, Loss: 0.8483368158340454\n",
      "Epoch 460, Loss: 0.8410011529922485\n",
      "Epoch 480, Loss: 0.8339817523956299\n",
      "weights of  A05 :  [[array([[-0.32990056,  0.14977744],\n",
      "       [ 0.4346707 , -0.33225456],\n",
      "       [-0.12348773,  0.7680427 ],\n",
      "       [-0.31687176, -0.04532288],\n",
      "       [ 0.193904  , -0.2949766 ]], dtype=float32), array([-0.236032  ,  0.23603193], dtype=float32)]]\n",
      "Lowest loss of  A05 :  0.8276166\n",
      "Epoch 0, Loss: 3.036996364593506\n",
      "Epoch 20, Loss: 1.0899643898010254\n",
      "Epoch 40, Loss: 1.0661925077438354\n",
      "Epoch 60, Loss: 1.0450817346572876\n",
      "Epoch 80, Loss: 1.0382665395736694\n",
      "Epoch 100, Loss: 1.0370928049087524\n",
      "Epoch 120, Loss: 1.0357927083969116\n",
      "Epoch 140, Loss: 1.0348774194717407\n",
      "Epoch 160, Loss: 1.0340867042541504\n",
      "Epoch 180, Loss: 1.0334968566894531\n",
      "Epoch 200, Loss: 1.0331528186798096\n",
      "Epoch 220, Loss: 1.0330750942230225\n",
      "Epoch 240, Loss: 1.0332924127578735\n",
      "Epoch 260, Loss: 1.0338256359100342\n",
      "Epoch 280, Loss: 1.0346908569335938\n",
      "Epoch 300, Loss: 1.0359011888504028\n",
      "Epoch 320, Loss: 1.0374655723571777\n",
      "Epoch 340, Loss: 1.0393894910812378\n",
      "Epoch 360, Loss: 1.0416754484176636\n",
      "Epoch 380, Loss: 1.044323444366455\n",
      "Epoch 400, Loss: 1.0473302602767944\n",
      "Epoch 420, Loss: 1.0506916046142578\n",
      "Epoch 440, Loss: 1.05440092086792\n",
      "Epoch 460, Loss: 1.0584492683410645\n",
      "Epoch 480, Loss: 1.0628275871276855\n",
      "weights of  A06 :  [[array([[-0.2186577 , -0.4521912 ],\n",
      "       [-1.0580726 , -0.444634  ],\n",
      "       [ 0.35273945, -0.5182292 ],\n",
      "       [ 0.7905061 ,  0.9956999 ],\n",
      "       [-0.00393493, -0.2638359 ]], dtype=float32), array([ 0.13015161, -0.13015164], dtype=float32)]]\n",
      "Lowest loss of  A06 :  1.0330675\n",
      "Epoch 0, Loss: 2.8698198795318604\n",
      "Epoch 20, Loss: 0.9161679744720459\n",
      "Epoch 40, Loss: 0.8399719595909119\n",
      "Epoch 60, Loss: 0.8242343068122864\n",
      "Epoch 80, Loss: 0.819898247718811\n",
      "Epoch 100, Loss: 0.8192073106765747\n",
      "Epoch 120, Loss: 0.818610429763794\n",
      "Epoch 140, Loss: 0.8180288076400757\n",
      "Epoch 160, Loss: 0.8174435496330261\n",
      "Epoch 180, Loss: 0.8168607950210571\n",
      "Epoch 200, Loss: 0.816291093826294\n",
      "Epoch 220, Loss: 0.8157441020011902\n",
      "Epoch 240, Loss: 0.8152284622192383\n",
      "Epoch 260, Loss: 0.8147529363632202\n",
      "Epoch 280, Loss: 0.8143256306648254\n",
      "Epoch 300, Loss: 0.8139543533325195\n",
      "Epoch 320, Loss: 0.8136470913887024\n",
      "Epoch 340, Loss: 0.8134111166000366\n",
      "Epoch 360, Loss: 0.8132538795471191\n",
      "Epoch 380, Loss: 0.8131819367408752\n",
      "Epoch 400, Loss: 0.8132025599479675\n",
      "Epoch 420, Loss: 0.8133218884468079\n",
      "Epoch 440, Loss: 0.8135464191436768\n",
      "Epoch 460, Loss: 0.8138819932937622\n",
      "Epoch 480, Loss: 0.8143347501754761\n",
      "weights of  A07 :  [[array([[ 0.36034527,  0.21898432],\n",
      "       [-0.26933983, -0.01118385],\n",
      "       [-0.22163549,  0.12844473],\n",
      "       [ 0.80549484,  0.5641119 ],\n",
      "       [-0.05065851, -0.09860848]], dtype=float32), array([-0.29283094,  0.2928309 ], dtype=float32)]]\n",
      "Lowest loss of  A07 :  0.8131782\n",
      "Epoch 0, Loss: 1.9927998781204224\n",
      "Epoch 20, Loss: 1.2593097686767578\n",
      "Epoch 40, Loss: 1.1760004758834839\n",
      "Epoch 60, Loss: 1.1338236331939697\n",
      "Epoch 80, Loss: 1.0942716598510742\n",
      "Epoch 100, Loss: 1.055060863494873\n",
      "Epoch 120, Loss: 1.016627311706543\n",
      "Epoch 140, Loss: 0.9802656769752502\n",
      "Epoch 160, Loss: 0.9469917416572571\n",
      "Epoch 180, Loss: 0.9175728559494019\n",
      "Epoch 200, Loss: 0.8925676345825195\n",
      "Epoch 220, Loss: 0.8723539113998413\n",
      "Epoch 240, Loss: 0.8571528196334839\n",
      "Epoch 260, Loss: 0.8470538258552551\n",
      "Epoch 280, Loss: 0.8420383930206299\n",
      "Epoch 300, Loss: 0.8420021533966064\n",
      "Epoch 320, Loss: 0.8467760682106018\n",
      "Epoch 340, Loss: 0.8561445474624634\n",
      "Epoch 360, Loss: 0.8698610067367554\n",
      "Epoch 380, Loss: 0.887660801410675\n",
      "Epoch 400, Loss: 0.9092715978622437\n",
      "Epoch 420, Loss: 0.9344213008880615\n",
      "Epoch 440, Loss: 0.96284419298172\n",
      "Epoch 460, Loss: 0.99428391456604\n",
      "Epoch 480, Loss: 1.028497338294983\n",
      "weights of  A08 :  [[array([[-0.20275262, -0.90370154],\n",
      "       [-0.45975572,  0.52729136],\n",
      "       [ 0.4168105 ,  0.06528644],\n",
      "       [ 0.1406748 , -0.63969916],\n",
      "       [-0.7262208 , -0.32026994]], dtype=float32), array([ 0.21905023, -0.21905024], dtype=float32)]]\n",
      "Lowest loss of  A08 :  0.8414072\n",
      "Epoch 0, Loss: 2.408078908920288\n",
      "Epoch 20, Loss: 1.043730616569519\n",
      "Epoch 40, Loss: 1.0428364276885986\n",
      "Epoch 60, Loss: 1.0231432914733887\n",
      "Epoch 80, Loss: 1.0092686414718628\n",
      "Epoch 100, Loss: 0.9967658519744873\n",
      "Epoch 120, Loss: 0.9845361709594727\n",
      "Epoch 140, Loss: 0.9727891683578491\n",
      "Epoch 160, Loss: 0.9617968201637268\n",
      "Epoch 180, Loss: 0.9518073797225952\n",
      "Epoch 200, Loss: 0.9430215358734131\n",
      "Epoch 220, Loss: 0.9355890154838562\n",
      "Epoch 240, Loss: 0.9296243190765381\n",
      "Epoch 260, Loss: 0.9252078533172607\n",
      "Epoch 280, Loss: 0.9223922491073608\n",
      "Epoch 300, Loss: 0.9212064146995544\n",
      "Epoch 320, Loss: 0.9216585159301758\n",
      "Epoch 340, Loss: 0.9237393140792847\n",
      "Epoch 360, Loss: 0.9274250268936157\n",
      "Epoch 380, Loss: 0.9326784610748291\n",
      "Epoch 400, Loss: 0.9394524097442627\n",
      "Epoch 420, Loss: 0.9476905465126038\n",
      "Epoch 440, Loss: 0.9573292136192322\n",
      "Epoch 460, Loss: 0.968299388885498\n",
      "Epoch 480, Loss: 0.9805275201797485\n",
      "weights of  A09 :  [[array([[-0.49107054, -0.9045831 ],\n",
      "       [ 0.12390495,  1.0247321 ],\n",
      "       [-0.52759415, -0.17703873],\n",
      "       [-0.00151315, -0.11515334],\n",
      "       [ 0.55792594,  0.01821222]], dtype=float32), array([-0.3357502 ,  0.33575022], dtype=float32)]]\n",
      "Lowest loss of  A09 :  0.9211658\n",
      "Epoch 0, Loss: 1.1031780242919922\n",
      "Epoch 20, Loss: 1.0089952945709229\n",
      "Epoch 40, Loss: 0.9982778429985046\n",
      "Epoch 60, Loss: 1.0066664218902588\n",
      "Epoch 80, Loss: 1.0176481008529663\n",
      "Epoch 100, Loss: 1.0333794355392456\n",
      "Epoch 120, Loss: 1.0538897514343262\n",
      "Epoch 140, Loss: 1.0788205862045288\n",
      "Epoch 160, Loss: 1.1079747676849365\n",
      "Epoch 180, Loss: 1.1409790515899658\n",
      "Epoch 200, Loss: 1.1773464679718018\n",
      "Epoch 220, Loss: 1.2165446281433105\n",
      "Epoch 240, Loss: 1.258025884628296\n",
      "Epoch 260, Loss: 1.301248550415039\n",
      "Epoch 280, Loss: 1.3456968069076538\n",
      "Epoch 300, Loss: 1.3908922672271729\n",
      "Epoch 320, Loss: 1.4364020824432373\n",
      "Epoch 340, Loss: 1.4818445444107056\n",
      "Epoch 360, Loss: 1.5268893241882324\n",
      "Epoch 380, Loss: 1.5712573528289795\n",
      "Epoch 400, Loss: 1.6147189140319824\n",
      "Epoch 420, Loss: 1.657089114189148\n",
      "Epoch 440, Loss: 1.698225498199463\n",
      "Epoch 460, Loss: 1.7380220890045166\n",
      "Epoch 480, Loss: 1.7764053344726562\n",
      "weights of  P001 :  [[array([[ 0.89830345, -0.04880213],\n",
      "       [-0.5179167 ,  0.7105918 ],\n",
      "       [ 0.7518435 ,  0.42375496],\n",
      "       [ 0.43042204,  0.33345833],\n",
      "       [ 0.7492199 , -0.12572101]], dtype=float32), array([-0.09854699,  0.098547  ], dtype=float32)]]\n",
      "Lowest loss of  P001 :  0.9929253\n",
      "Epoch 0, Loss: 0.9383687973022461\n",
      "Epoch 20, Loss: 0.8729474544525146\n",
      "Epoch 40, Loss: 0.8677725791931152\n",
      "Epoch 60, Loss: 0.9004412889480591\n",
      "Epoch 80, Loss: 0.9504132270812988\n",
      "Epoch 100, Loss: 1.009240746498108\n",
      "Epoch 120, Loss: 1.0775959491729736\n",
      "Epoch 140, Loss: 1.1582932472229004\n",
      "Epoch 160, Loss: 1.252739429473877\n",
      "Epoch 180, Loss: 1.3606330156326294\n",
      "Epoch 200, Loss: 1.4807171821594238\n",
      "Epoch 220, Loss: 1.6114511489868164\n",
      "Epoch 240, Loss: 1.7513670921325684\n",
      "Epoch 260, Loss: 1.8991928100585938\n",
      "Epoch 280, Loss: 2.0538628101348877\n",
      "Epoch 300, Loss: 2.2144808769226074\n",
      "Epoch 320, Loss: 2.380277633666992\n",
      "Epoch 340, Loss: 2.550572156906128\n",
      "Epoch 360, Loss: 2.724749803543091\n",
      "Epoch 380, Loss: 2.90224289894104\n",
      "Epoch 400, Loss: 3.082517385482788\n",
      "Epoch 420, Loss: 3.265068769454956\n",
      "Epoch 440, Loss: 3.449416160583496\n",
      "Epoch 460, Loss: 3.635100841522217\n",
      "Epoch 480, Loss: 3.821681261062622\n",
      "weights of  P002 :  [[array([[-0.07726943,  0.43872285],\n",
      "       [-0.16156308, -0.20857367],\n",
      "       [ 0.04807567, -0.749363  ],\n",
      "       [-0.36337832, -0.04542478],\n",
      "       [-0.02314001, -0.93912333]], dtype=float32), array([ 0.19202985, -0.19202991], dtype=float32)]]\n",
      "Lowest loss of  P002 :  0.86391866\n",
      "Epoch 0, Loss: 1.1242525577545166\n",
      "Epoch 20, Loss: 0.9589628577232361\n",
      "Epoch 40, Loss: 0.9300886392593384\n",
      "Epoch 60, Loss: 0.9241598844528198\n",
      "Epoch 80, Loss: 0.9275730848312378\n",
      "Epoch 100, Loss: 0.9379711151123047\n",
      "Epoch 120, Loss: 0.9526519775390625\n",
      "Epoch 140, Loss: 0.9704968929290771\n",
      "Epoch 160, Loss: 0.9910959005355835\n",
      "Epoch 180, Loss: 1.0142250061035156\n",
      "Epoch 200, Loss: 1.039622187614441\n",
      "Epoch 220, Loss: 1.0669411420822144\n",
      "Epoch 240, Loss: 1.095785140991211\n",
      "Epoch 260, Loss: 1.1257518529891968\n",
      "Epoch 280, Loss: 1.1564645767211914\n",
      "Epoch 300, Loss: 1.1875882148742676\n",
      "Epoch 320, Loss: 1.218835711479187\n",
      "Epoch 340, Loss: 1.249967098236084\n",
      "Epoch 360, Loss: 1.2807860374450684\n",
      "Epoch 380, Loss: 1.3111340999603271\n",
      "Epoch 400, Loss: 1.340885877609253\n",
      "Epoch 420, Loss: 1.3699430227279663\n",
      "Epoch 440, Loss: 1.3982298374176025\n",
      "Epoch 460, Loss: 1.4256887435913086\n",
      "Epoch 480, Loss: 1.4522759914398193\n",
      "weights of  P003 :  [[array([[-0.945269  ,  0.10812742],\n",
      "       [-0.47984648, -0.5998268 ],\n",
      "       [ 0.13049054, -0.29552805],\n",
      "       [ 0.1733136 ,  0.25753   ],\n",
      "       [ 0.770744  ,  0.3807884 ]], dtype=float32), array([ 0.09205765, -0.09205767], dtype=float32)]]\n",
      "Lowest loss of  P003 :  0.9241066\n",
      "Epoch 0, Loss: 2.498823404312134\n",
      "Epoch 20, Loss: 1.2414546012878418\n",
      "Epoch 40, Loss: 1.1719138622283936\n",
      "Epoch 60, Loss: 1.155691146850586\n",
      "Epoch 80, Loss: 1.1424179077148438\n",
      "Epoch 100, Loss: 1.1300804615020752\n",
      "Epoch 120, Loss: 1.117045283317566\n",
      "Epoch 140, Loss: 1.1033399105072021\n",
      "Epoch 160, Loss: 1.0892099142074585\n",
      "Epoch 180, Loss: 1.0748175382614136\n",
      "Epoch 200, Loss: 1.0603234767913818\n",
      "Epoch 220, Loss: 1.0458778142929077\n",
      "Epoch 240, Loss: 1.031618595123291\n",
      "Epoch 260, Loss: 1.0176736116409302\n",
      "Epoch 280, Loss: 1.0041611194610596\n",
      "Epoch 300, Loss: 0.9911906719207764\n",
      "Epoch 320, Loss: 0.9788639545440674\n",
      "Epoch 340, Loss: 0.9672743082046509\n",
      "Epoch 360, Loss: 0.9565077424049377\n",
      "Epoch 380, Loss: 0.9466424584388733\n",
      "Epoch 400, Loss: 0.9377495050430298\n",
      "Epoch 420, Loss: 0.9298926591873169\n",
      "Epoch 440, Loss: 0.9231289625167847\n",
      "Epoch 460, Loss: 0.9175085425376892\n",
      "Epoch 480, Loss: 0.9130749106407166\n",
      "weights of  P004 :  [[array([[-0.13305964, -0.4970175 ],\n",
      "       [ 0.02194603, -0.21494001],\n",
      "       [ 0.7629053 ,  0.8124991 ],\n",
      "       [ 0.5289393 ,  0.13145694],\n",
      "       [-0.5043444 ,  0.40334594]], dtype=float32), array([ 0.19952817, -0.19952822], dtype=float32)]]\n",
      "Lowest loss of  P004 :  0.90999615\n",
      "Epoch 0, Loss: 1.2232816219329834\n",
      "Epoch 20, Loss: 1.1213951110839844\n",
      "Epoch 40, Loss: 1.109643816947937\n",
      "Epoch 60, Loss: 1.1024348735809326\n",
      "Epoch 80, Loss: 1.095100998878479\n",
      "Epoch 100, Loss: 1.0876708030700684\n",
      "Epoch 120, Loss: 1.080369234085083\n",
      "Epoch 140, Loss: 1.0733482837677002\n",
      "Epoch 160, Loss: 1.0667074918746948\n",
      "Epoch 180, Loss: 1.0605168342590332\n",
      "Epoch 200, Loss: 1.054808497428894\n",
      "Epoch 220, Loss: 1.0495898723602295\n",
      "Epoch 240, Loss: 1.0448544025421143\n",
      "Epoch 260, Loss: 1.0405855178833008\n",
      "Epoch 280, Loss: 1.0367612838745117\n",
      "Epoch 300, Loss: 1.0333561897277832\n",
      "Epoch 320, Loss: 1.0303434133529663\n",
      "Epoch 340, Loss: 1.0276951789855957\n",
      "Epoch 360, Loss: 1.0253839492797852\n",
      "Epoch 380, Loss: 1.0233818292617798\n",
      "Epoch 400, Loss: 1.0216612815856934\n",
      "Epoch 420, Loss: 1.020195722579956\n",
      "Epoch 440, Loss: 1.018958568572998\n",
      "Epoch 460, Loss: 1.0179247856140137\n",
      "Epoch 480, Loss: 1.0170702934265137\n",
      "weights of  P005 :  [[array([[-0.6109561 ,  0.34522718],\n",
      "       [-0.17640579, -0.3000556 ],\n",
      "       [-0.19354437, -1.1122072 ],\n",
      "       [-0.21604636, -0.45899728],\n",
      "       [ 0.9183391 ,  0.59651893]], dtype=float32), array([ 0.5298566 , -0.52985674], dtype=float32)]]\n",
      "Lowest loss of  P005 :  1.0164038\n",
      "Epoch 0, Loss: 1.2305145263671875\n",
      "Epoch 20, Loss: 0.9450074434280396\n",
      "Epoch 40, Loss: 0.9346799850463867\n",
      "Epoch 60, Loss: 0.9261680841445923\n",
      "Epoch 80, Loss: 0.917747437953949\n",
      "Epoch 100, Loss: 0.9097309708595276\n",
      "Epoch 120, Loss: 0.9024707078933716\n",
      "Epoch 140, Loss: 0.8961511254310608\n",
      "Epoch 160, Loss: 0.8908971548080444\n",
      "Epoch 180, Loss: 0.8867595791816711\n",
      "Epoch 200, Loss: 0.8837325572967529\n",
      "Epoch 220, Loss: 0.8817726373672485\n",
      "Epoch 240, Loss: 0.8808149695396423\n",
      "Epoch 260, Loss: 0.8807835578918457\n",
      "Epoch 280, Loss: 0.8816002607345581\n",
      "Epoch 300, Loss: 0.8831889033317566\n",
      "Epoch 320, Loss: 0.8854788541793823\n",
      "Epoch 340, Loss: 0.8884052634239197\n",
      "Epoch 360, Loss: 0.8919100761413574\n",
      "Epoch 380, Loss: 0.895940899848938\n",
      "Epoch 400, Loss: 0.9004504680633545\n",
      "Epoch 420, Loss: 0.9053953886032104\n",
      "Epoch 440, Loss: 0.9107365608215332\n",
      "Epoch 460, Loss: 0.9164367914199829\n",
      "Epoch 480, Loss: 0.9224616289138794\n",
      "weights of  P006 :  [[array([[ 0.39688522,  0.04857582],\n",
      "       [-0.24114369,  0.16628622],\n",
      "       [ 1.0661308 ,  0.4912891 ],\n",
      "       [-0.56621844, -0.07439345],\n",
      "       [-0.02505254, -0.05389089]], dtype=float32), array([-0.03429458,  0.03429454], dtype=float32)]]\n",
      "Lowest loss of  P006 :  0.880688\n",
      "Epoch 0, Loss: 1.4653029441833496\n",
      "Epoch 20, Loss: 0.9738680124282837\n",
      "Epoch 40, Loss: 0.910382866859436\n",
      "Epoch 60, Loss: 0.8430261015892029\n",
      "Epoch 80, Loss: 0.7863724827766418\n",
      "Epoch 100, Loss: 0.7423524260520935\n",
      "Epoch 120, Loss: 0.7134333252906799\n",
      "Epoch 140, Loss: 0.6998671293258667\n",
      "Epoch 160, Loss: 0.7011522054672241\n",
      "Epoch 180, Loss: 0.7162308096885681\n",
      "Epoch 200, Loss: 0.7437394857406616\n",
      "Epoch 220, Loss: 0.7822282314300537\n",
      "Epoch 240, Loss: 0.830267071723938\n",
      "Epoch 260, Loss: 0.8865187168121338\n",
      "Epoch 280, Loss: 0.9497742652893066\n",
      "Epoch 300, Loss: 1.018963098526001\n",
      "Epoch 320, Loss: 1.0931519269943237\n",
      "Epoch 340, Loss: 1.1715352535247803\n",
      "Epoch 360, Loss: 1.2534219026565552\n",
      "Epoch 380, Loss: 1.3382198810577393\n",
      "Epoch 400, Loss: 1.4254248142242432\n",
      "Epoch 420, Loss: 1.5146058797836304\n",
      "Epoch 440, Loss: 1.6053948402404785\n",
      "Epoch 460, Loss: 1.6974775791168213\n",
      "Epoch 480, Loss: 1.7905848026275635\n",
      "weights of  P007 :  [[array([[ 0.8542015 , -0.48396307],\n",
      "       [-0.07814513,  0.53272617],\n",
      "       [-0.11145052,  0.15816282],\n",
      "       [ 0.20987466,  0.07203206],\n",
      "       [-0.22822869,  0.08537187]], dtype=float32), array([ 0.15286912, -0.15286909], dtype=float32)]]\n",
      "Lowest loss of  P007 :  0.69864744\n",
      "Epoch 0, Loss: 3.1880130767822266\n",
      "Epoch 20, Loss: 1.1671687364578247\n",
      "Epoch 40, Loss: 0.9447263479232788\n",
      "Epoch 60, Loss: 0.8911995887756348\n",
      "Epoch 80, Loss: 0.8844897747039795\n",
      "Epoch 100, Loss: 0.8812944293022156\n",
      "Epoch 120, Loss: 0.8780124187469482\n",
      "Epoch 140, Loss: 0.8746421337127686\n",
      "Epoch 160, Loss: 0.871181845664978\n",
      "Epoch 180, Loss: 0.8676998615264893\n",
      "Epoch 200, Loss: 0.8642268180847168\n",
      "Epoch 220, Loss: 0.8608043789863586\n",
      "Epoch 240, Loss: 0.8574645519256592\n",
      "Epoch 260, Loss: 0.8542375564575195\n",
      "Epoch 280, Loss: 0.85114985704422\n",
      "Epoch 300, Loss: 0.8482252359390259\n",
      "Epoch 320, Loss: 0.8454840183258057\n",
      "Epoch 340, Loss: 0.8429449200630188\n",
      "Epoch 360, Loss: 0.8406233787536621\n",
      "Epoch 380, Loss: 0.8385332822799683\n",
      "Epoch 400, Loss: 0.8366861939430237\n",
      "Epoch 420, Loss: 0.8350918292999268\n",
      "Epoch 440, Loss: 0.8337584733963013\n",
      "Epoch 460, Loss: 0.8326925039291382\n",
      "Epoch 480, Loss: 0.8318994641304016\n",
      "weights of  P008 :  [[array([[-0.11905916, -0.6246541 ],\n",
      "       [ 0.20642017,  0.27173975],\n",
      "       [ 0.20969033,  0.4294997 ],\n",
      "       [ 0.12571958, -0.33288723],\n",
      "       [-0.68735623, -0.35369667]], dtype=float32), array([ 0.28362566, -0.28362566], dtype=float32)]]\n",
      "Lowest loss of  P008 :  0.8314025\n",
      "Epoch 0, Loss: 0.8542194366455078\n",
      "Epoch 20, Loss: 0.8411656022071838\n",
      "Epoch 40, Loss: 0.8627489805221558\n",
      "Epoch 60, Loss: 0.9216485023498535\n",
      "Epoch 80, Loss: 1.0098402500152588\n",
      "Epoch 100, Loss: 1.1182304620742798\n",
      "Epoch 120, Loss: 1.2384490966796875\n",
      "Epoch 140, Loss: 1.3635637760162354\n",
      "Epoch 160, Loss: 1.4886126518249512\n",
      "Epoch 180, Loss: 1.610387921333313\n",
      "Epoch 200, Loss: 1.727048397064209\n",
      "Epoch 220, Loss: 1.8377848863601685\n",
      "Epoch 240, Loss: 1.9424731731414795\n",
      "Epoch 260, Loss: 2.0413875579833984\n",
      "Epoch 280, Loss: 2.1349880695343018\n",
      "Epoch 300, Loss: 2.223778247833252\n",
      "Epoch 320, Loss: 2.308216094970703\n",
      "Epoch 340, Loss: 2.38867449760437\n",
      "Epoch 360, Loss: 2.4654271602630615\n",
      "Epoch 380, Loss: 2.5386545658111572\n",
      "Epoch 400, Loss: 2.608461856842041\n",
      "Epoch 420, Loss: 2.6748976707458496\n",
      "Epoch 440, Loss: 2.7379727363586426\n",
      "Epoch 460, Loss: 2.797679901123047\n",
      "Epoch 480, Loss: 2.854008913040161\n",
      "weights of  P009 :  [[array([[ 0.39306107,  0.48415807],\n",
      "       [ 0.1884635 ,  0.27115735],\n",
      "       [ 0.12581635, -0.40135583],\n",
      "       [-0.11338096,  0.7345752 ],\n",
      "       [ 0.4553697 ,  0.46591333]], dtype=float32), array([ 0.02026081, -0.0202608 ], dtype=float32)]]\n",
      "Lowest loss of  P009 :  0.84116066\n",
      "Epoch 0, Loss: 1.1739208698272705\n",
      "Epoch 20, Loss: 0.9046276807785034\n",
      "Epoch 40, Loss: 0.9108469486236572\n",
      "Epoch 60, Loss: 0.924071192741394\n",
      "Epoch 80, Loss: 0.9422568082809448\n",
      "Epoch 100, Loss: 0.9636913537979126\n",
      "Epoch 120, Loss: 0.9888152480125427\n",
      "Epoch 140, Loss: 1.0169785022735596\n",
      "Epoch 160, Loss: 1.0478870868682861\n",
      "Epoch 180, Loss: 1.0813056230545044\n",
      "Epoch 200, Loss: 1.116992712020874\n",
      "Epoch 220, Loss: 1.1548092365264893\n",
      "Epoch 240, Loss: 1.1946375370025635\n",
      "Epoch 260, Loss: 1.2363877296447754\n",
      "Epoch 280, Loss: 1.2799816131591797\n",
      "Epoch 300, Loss: 1.3253390789031982\n",
      "Epoch 320, Loss: 1.3723700046539307\n",
      "Epoch 340, Loss: 1.4209704399108887\n",
      "Epoch 360, Loss: 1.4710196256637573\n",
      "Epoch 380, Loss: 1.5223801136016846\n",
      "Epoch 400, Loss: 1.5748991966247559\n",
      "Epoch 420, Loss: 1.6284124851226807\n",
      "Epoch 440, Loss: 1.6827466487884521\n",
      "Epoch 460, Loss: 1.737722635269165\n",
      "Epoch 480, Loss: 1.7931597232818604\n",
      "weights of  P010 :  [[array([[ 0.699186  ,  0.4367495 ],\n",
      "       [-0.31467378,  0.49869674],\n",
      "       [-0.00672326, -0.20423672],\n",
      "       [ 0.16841502, -0.89089394],\n",
      "       [ 0.45759785,  0.68634176]], dtype=float32), array([-0.15594798,  0.15594798], dtype=float32)]]\n",
      "Lowest loss of  P010 :  0.8996599\n",
      "Epoch 0, Loss: 2.9278244972229004\n",
      "Epoch 20, Loss: 1.0808945894241333\n",
      "Epoch 40, Loss: 1.0525691509246826\n",
      "Epoch 60, Loss: 1.049607515335083\n",
      "Epoch 80, Loss: 1.0466489791870117\n",
      "Epoch 100, Loss: 1.0438967943191528\n",
      "Epoch 120, Loss: 1.0413017272949219\n",
      "Epoch 140, Loss: 1.0385850667953491\n",
      "Epoch 160, Loss: 1.0357273817062378\n",
      "Epoch 180, Loss: 1.0327554941177368\n",
      "Epoch 200, Loss: 1.0296916961669922\n",
      "Epoch 220, Loss: 1.026557207107544\n",
      "Epoch 240, Loss: 1.0233708620071411\n",
      "Epoch 260, Loss: 1.0201506614685059\n",
      "Epoch 280, Loss: 1.016912579536438\n",
      "Epoch 300, Loss: 1.0136719942092896\n",
      "Epoch 320, Loss: 1.0104432106018066\n",
      "Epoch 340, Loss: 1.0072393417358398\n",
      "Epoch 360, Loss: 1.004072666168213\n",
      "Epoch 380, Loss: 1.0009546279907227\n",
      "Epoch 400, Loss: 0.9978955984115601\n",
      "Epoch 420, Loss: 0.994905948638916\n",
      "Epoch 440, Loss: 0.9919943809509277\n",
      "Epoch 460, Loss: 0.989169716835022\n",
      "Epoch 480, Loss: 0.9864392280578613\n",
      "weights of  P011 :  [[array([[ 0.96611524,  0.5028509 ],\n",
      "       [ 0.06011639,  0.50194573],\n",
      "       [ 0.23040707,  0.47894165],\n",
      "       [ 0.4755947 ,  0.71665287],\n",
      "       [-0.10718147, -0.6047673 ]], dtype=float32), array([-0.10244564,  0.10244562], dtype=float32)]]\n",
      "Lowest loss of  P011 :  0.9839397\n",
      "Epoch 0, Loss: 1.067515254020691\n",
      "Epoch 20, Loss: 1.0082027912139893\n",
      "Epoch 40, Loss: 1.0012505054473877\n",
      "Epoch 60, Loss: 0.9927620887756348\n",
      "Epoch 80, Loss: 0.9843486547470093\n",
      "Epoch 100, Loss: 0.9764615893363953\n",
      "Epoch 120, Loss: 0.9696074724197388\n",
      "Epoch 140, Loss: 0.9641460180282593\n",
      "Epoch 160, Loss: 0.9603176116943359\n",
      "Epoch 180, Loss: 0.9582725763320923\n",
      "Epoch 200, Loss: 0.9580870866775513\n",
      "Epoch 220, Loss: 0.9597796201705933\n",
      "Epoch 240, Loss: 0.9633260369300842\n",
      "Epoch 260, Loss: 0.968672513961792\n",
      "Epoch 280, Loss: 0.9757442474365234\n",
      "Epoch 300, Loss: 0.9844537377357483\n",
      "Epoch 320, Loss: 0.994706392288208\n",
      "Epoch 340, Loss: 1.0064033269882202\n",
      "Epoch 360, Loss: 1.0194456577301025\n",
      "Epoch 380, Loss: 1.0337347984313965\n",
      "Epoch 400, Loss: 1.0491747856140137\n",
      "Epoch 420, Loss: 1.0656721591949463\n",
      "Epoch 440, Loss: 1.0831369161605835\n",
      "Epoch 460, Loss: 1.1014822721481323\n",
      "Epoch 480, Loss: 1.1206247806549072\n",
      "weights of  P012 :  [[array([[ 0.4329904 , -0.22487442],\n",
      "       [ 0.11027941,  0.04429702],\n",
      "       [ 0.1601005 ,  0.7259827 ],\n",
      "       [-0.58095765,  0.7483931 ],\n",
      "       [ 0.5556026 , -0.9419094 ]], dtype=float32), array([ 0.31807995, -0.31807995], dtype=float32)]]\n",
      "Lowest loss of  P012 :  0.9579359\n",
      "Epoch 0, Loss: 0.9346214532852173\n",
      "Epoch 20, Loss: 0.8855154514312744\n",
      "Epoch 40, Loss: 0.903091311454773\n",
      "Epoch 60, Loss: 0.9413620233535767\n",
      "Epoch 80, Loss: 1.0001294612884521\n",
      "Epoch 100, Loss: 1.0833617448806763\n",
      "Epoch 120, Loss: 1.1869614124298096\n",
      "Epoch 140, Loss: 1.3062658309936523\n",
      "Epoch 160, Loss: 1.4380285739898682\n",
      "Epoch 180, Loss: 1.5793671607971191\n",
      "Epoch 200, Loss: 1.7275582551956177\n",
      "Epoch 220, Loss: 1.8802847862243652\n",
      "Epoch 240, Loss: 2.0356338024139404\n",
      "Epoch 260, Loss: 2.1920130252838135\n",
      "Epoch 280, Loss: 2.3481032848358154\n",
      "Epoch 300, Loss: 2.5028176307678223\n",
      "Epoch 320, Loss: 2.6552579402923584\n",
      "Epoch 340, Loss: 2.8046865463256836\n",
      "Epoch 360, Loss: 2.950495719909668\n",
      "Epoch 380, Loss: 3.0921921730041504\n",
      "Epoch 400, Loss: 3.2293777465820312\n",
      "Epoch 420, Loss: 3.3617382049560547\n",
      "Epoch 440, Loss: 3.4890317916870117\n",
      "Epoch 460, Loss: 3.611081600189209\n",
      "Epoch 480, Loss: 3.7277657985687256\n",
      "weights of  P013 :  [[array([[ 0.6568377 ,  0.34693068],\n",
      "       [ 0.14000519,  0.21033959],\n",
      "       [-0.11477125,  0.18665655],\n",
      "       [-0.71101695, -0.8447575 ],\n",
      "       [ 0.533872  , -0.14015181]], dtype=float32), array([-0.06291809,  0.0629181 ], dtype=float32)]]\n",
      "Lowest loss of  P013 :  0.88543487\n",
      "Epoch 0, Loss: 1.5032145977020264\n",
      "Epoch 20, Loss: 0.9761704802513123\n",
      "Epoch 40, Loss: 0.9509177803993225\n",
      "Epoch 60, Loss: 0.936722457408905\n",
      "Epoch 80, Loss: 0.9249057769775391\n",
      "Epoch 100, Loss: 0.9156534075737\n",
      "Epoch 120, Loss: 0.9095155000686646\n",
      "Epoch 140, Loss: 0.9063470959663391\n",
      "Epoch 160, Loss: 0.9058657288551331\n",
      "Epoch 180, Loss: 0.9077088236808777\n",
      "Epoch 200, Loss: 0.911536693572998\n",
      "Epoch 220, Loss: 0.9170700311660767\n",
      "Epoch 240, Loss: 0.9241088032722473\n",
      "Epoch 260, Loss: 0.9325269460678101\n",
      "Epoch 280, Loss: 0.9422563314437866\n",
      "Epoch 300, Loss: 0.9532699584960938\n",
      "Epoch 320, Loss: 0.9655652046203613\n",
      "Epoch 340, Loss: 0.9791502952575684\n",
      "Epoch 360, Loss: 0.9940345287322998\n",
      "Epoch 380, Loss: 1.0102224349975586\n",
      "Epoch 400, Loss: 1.0277093648910522\n",
      "Epoch 420, Loss: 1.046480655670166\n",
      "Epoch 440, Loss: 1.0665117502212524\n",
      "Epoch 460, Loss: 1.087768793106079\n",
      "Epoch 480, Loss: 1.1102091073989868\n",
      "weights of  P014 :  [[array([[-0.3963218 , -0.85276365],\n",
      "       [ 0.31537044,  0.4452551 ],\n",
      "       [-0.11519437,  0.27861717],\n",
      "       [-0.36857697, -0.28966564],\n",
      "       [-0.6475743 , -0.6611718 ]], dtype=float32), array([-0.14247037,  0.14247042], dtype=float32)]]\n",
      "Lowest loss of  P014 :  0.9057484\n",
      "Epoch 0, Loss: 0.9684631824493408\n",
      "Epoch 20, Loss: 0.8827281594276428\n",
      "Epoch 40, Loss: 0.8612124919891357\n",
      "Epoch 60, Loss: 0.8989033699035645\n",
      "Epoch 80, Loss: 0.9780694842338562\n",
      "Epoch 100, Loss: 1.0854988098144531\n",
      "Epoch 120, Loss: 1.2147687673568726\n",
      "Epoch 140, Loss: 1.3597277402877808\n",
      "Epoch 160, Loss: 1.5153377056121826\n",
      "Epoch 180, Loss: 1.6786808967590332\n",
      "Epoch 200, Loss: 1.8480339050292969\n",
      "Epoch 220, Loss: 2.022374153137207\n",
      "Epoch 240, Loss: 2.2011752128601074\n",
      "Epoch 260, Loss: 2.3841440677642822\n",
      "Epoch 280, Loss: 2.571068286895752\n",
      "Epoch 300, Loss: 2.7617359161376953\n",
      "Epoch 320, Loss: 2.955899715423584\n",
      "Epoch 340, Loss: 3.1532623767852783\n",
      "Epoch 360, Loss: 3.3534820079803467\n",
      "Epoch 380, Loss: 3.5561845302581787\n",
      "Epoch 400, Loss: 3.760974645614624\n",
      "Epoch 420, Loss: 3.967454433441162\n",
      "Epoch 440, Loss: 4.175223350524902\n",
      "Epoch 460, Loss: 4.383897304534912\n",
      "Epoch 480, Loss: 4.593106269836426\n",
      "weights of  P015 :  [[array([[ 0.30707824,  0.2873844 ],\n",
      "       [-0.8353229 , -0.4626885 ],\n",
      "       [-0.10113323, -0.5619723 ],\n",
      "       [-0.03016833,  0.6249023 ],\n",
      "       [ 0.20261079, -0.3000296 ]], dtype=float32), array([-0.0151063 ,  0.01510628], dtype=float32)]]\n",
      "Lowest loss of  P015 :  0.86051327\n",
      "Epoch 0, Loss: 3.170217275619507\n",
      "Epoch 20, Loss: 1.1274187564849854\n",
      "Epoch 40, Loss: 1.0316307544708252\n",
      "Epoch 60, Loss: 0.9900993704795837\n",
      "Epoch 80, Loss: 0.9837135076522827\n",
      "Epoch 100, Loss: 0.9824692010879517\n",
      "Epoch 120, Loss: 0.9814928770065308\n",
      "Epoch 140, Loss: 0.980633020401001\n",
      "Epoch 160, Loss: 0.9797682762145996\n",
      "Epoch 180, Loss: 0.9789334535598755\n",
      "Epoch 200, Loss: 0.9781313538551331\n",
      "Epoch 220, Loss: 0.9773730039596558\n",
      "Epoch 240, Loss: 0.9766665697097778\n",
      "Epoch 260, Loss: 0.9760180115699768\n",
      "Epoch 280, Loss: 0.9754323959350586\n",
      "Epoch 300, Loss: 0.9749129414558411\n",
      "Epoch 320, Loss: 0.9744622707366943\n",
      "Epoch 340, Loss: 0.9740819931030273\n",
      "Epoch 360, Loss: 0.9737725257873535\n",
      "Epoch 380, Loss: 0.9735341668128967\n",
      "Epoch 400, Loss: 0.9733666181564331\n",
      "Epoch 420, Loss: 0.9732687473297119\n",
      "Epoch 440, Loss: 0.9732396602630615\n",
      "Epoch 460, Loss: 0.9732782244682312\n",
      "Epoch 480, Loss: 0.9733827114105225\n",
      "weights of  P016 :  [[array([[-0.22889462,  0.05249921],\n",
      "       [ 0.10135388,  0.5773432 ],\n",
      "       [ 1.0037917 ,  0.757886  ],\n",
      "       [ 0.33036175, -0.0784464 ],\n",
      "       [-0.5166893 , -0.6907961 ]], dtype=float32), array([-0.21419372,  0.2141937 ], dtype=float32)]]\n",
      "Lowest loss of  P016 :  0.9732395\n",
      "Epoch 0, Loss: 1.2284772396087646\n",
      "Epoch 20, Loss: 0.9913746118545532\n",
      "Epoch 40, Loss: 1.0016074180603027\n",
      "Epoch 60, Loss: 1.0200629234313965\n",
      "Epoch 80, Loss: 1.0237067937850952\n",
      "Epoch 100, Loss: 1.0275259017944336\n",
      "Epoch 120, Loss: 1.0311259031295776\n",
      "Epoch 140, Loss: 1.0357730388641357\n",
      "Epoch 160, Loss: 1.0395407676696777\n",
      "Epoch 180, Loss: 1.0419827699661255\n",
      "Epoch 200, Loss: 1.0431170463562012\n",
      "Epoch 220, Loss: 1.0432196855545044\n",
      "Epoch 240, Loss: 1.0426076650619507\n",
      "Epoch 260, Loss: 1.0416202545166016\n",
      "Epoch 280, Loss: 1.0405755043029785\n",
      "Epoch 300, Loss: 1.0397324562072754\n",
      "Epoch 320, Loss: 1.0392825603485107\n",
      "Epoch 340, Loss: 1.0393555164337158\n",
      "Epoch 360, Loss: 1.0400296449661255\n",
      "Epoch 380, Loss: 1.0413435697555542\n",
      "Epoch 400, Loss: 1.0433067083358765\n",
      "Epoch 420, Loss: 1.0459105968475342\n",
      "Epoch 440, Loss: 1.0491337776184082\n",
      "Epoch 460, Loss: 1.0529484748840332\n",
      "Epoch 480, Loss: 1.0573229789733887\n",
      "weights of  P017 :  [[array([[-0.53531677,  0.21359849],\n",
      "       [-1.0038286 , -0.51061237],\n",
      "       [-0.4850523 , -0.46957293],\n",
      "       [ 0.45424575,  0.241596  ],\n",
      "       [ 0.19876698, -0.652607  ]], dtype=float32), array([ 0.18629675, -0.18629672], dtype=float32)]]\n",
      "Lowest loss of  P017 :  0.9909711\n",
      "Epoch 0, Loss: 4.425915718078613\n",
      "Epoch 20, Loss: 0.8929407000541687\n",
      "Epoch 40, Loss: 0.8917807340621948\n",
      "Epoch 60, Loss: 0.8925002217292786\n",
      "Epoch 80, Loss: 0.8919265866279602\n",
      "Epoch 100, Loss: 0.8907563090324402\n",
      "Epoch 120, Loss: 0.890417754650116\n",
      "Epoch 140, Loss: 0.8901805877685547\n",
      "Epoch 160, Loss: 0.8899571895599365\n",
      "Epoch 180, Loss: 0.8897499442100525\n",
      "Epoch 200, Loss: 0.8895635008811951\n",
      "Epoch 220, Loss: 0.8894048929214478\n",
      "Epoch 240, Loss: 0.889279842376709\n",
      "Epoch 260, Loss: 0.8891943097114563\n",
      "Epoch 280, Loss: 0.8891540765762329\n",
      "Epoch 300, Loss: 0.8891650438308716\n",
      "Epoch 320, Loss: 0.8892327547073364\n",
      "Epoch 340, Loss: 0.8893628716468811\n",
      "Epoch 360, Loss: 0.8895608186721802\n",
      "Epoch 380, Loss: 0.8898321390151978\n",
      "Epoch 400, Loss: 0.8901820778846741\n",
      "Epoch 420, Loss: 0.8906160593032837\n",
      "Epoch 440, Loss: 0.8911392688751221\n",
      "Epoch 460, Loss: 0.8917568922042847\n",
      "Epoch 480, Loss: 0.8924740552902222\n",
      "weights of  P018 :  [[array([[ 0.05722253,  0.36538506],\n",
      "       [-0.50126743, -0.06957741],\n",
      "       [ 0.14224617, -0.2519087 ],\n",
      "       [ 0.31782502, -1.0239378 ],\n",
      "       [-0.588052  ,  0.32226244]], dtype=float32), array([-0.18346612,  0.18346614], dtype=float32)]]\n",
      "Lowest loss of  P018 :  0.8891518\n",
      "Epoch 0, Loss: 1.625356674194336\n",
      "Epoch 20, Loss: 0.9737155437469482\n",
      "Epoch 40, Loss: 0.9817049503326416\n",
      "Epoch 60, Loss: 0.9798710346221924\n",
      "Epoch 80, Loss: 0.9904087781906128\n",
      "Epoch 100, Loss: 1.0035877227783203\n",
      "Epoch 120, Loss: 1.0191842317581177\n",
      "Epoch 140, Loss: 1.0370556116104126\n",
      "Epoch 160, Loss: 1.0572346448898315\n",
      "Epoch 180, Loss: 1.0797151327133179\n",
      "Epoch 200, Loss: 1.1044847965240479\n",
      "Epoch 220, Loss: 1.13150954246521\n",
      "Epoch 240, Loss: 1.160747766494751\n",
      "Epoch 260, Loss: 1.192143201828003\n",
      "Epoch 280, Loss: 1.2256312370300293\n",
      "Epoch 300, Loss: 1.2611398696899414\n",
      "Epoch 320, Loss: 1.2985904216766357\n",
      "Epoch 340, Loss: 1.3379000425338745\n",
      "Epoch 360, Loss: 1.3789823055267334\n",
      "Epoch 380, Loss: 1.4217486381530762\n",
      "Epoch 400, Loss: 1.4661086797714233\n",
      "Epoch 420, Loss: 1.511972188949585\n",
      "Epoch 440, Loss: 1.559248685836792\n",
      "Epoch 460, Loss: 1.6078484058380127\n",
      "Epoch 480, Loss: 1.65768301486969\n",
      "weights of  P019 :  [[array([[-0.23601806, -0.94488525],\n",
      "       [-0.64535165,  0.4951506 ],\n",
      "       [-0.12022394,  0.26223677],\n",
      "       [-0.5191472 , -0.05990155],\n",
      "       [ 0.7581882 , -0.890204  ]], dtype=float32), array([-0.21086507,  0.21086508], dtype=float32)]]\n",
      "Lowest loss of  P019 :  0.9647372\n",
      "Epoch 0, Loss: 2.8561315536499023\n",
      "Epoch 20, Loss: 1.1433720588684082\n",
      "Epoch 40, Loss: 1.0159193277359009\n",
      "Epoch 60, Loss: 0.9894390106201172\n",
      "Epoch 80, Loss: 0.9872317910194397\n",
      "Epoch 100, Loss: 0.9855567216873169\n",
      "Epoch 120, Loss: 0.9837640523910522\n",
      "Epoch 140, Loss: 0.9818575382232666\n",
      "Epoch 160, Loss: 0.9798479080200195\n",
      "Epoch 180, Loss: 0.9777367115020752\n",
      "Epoch 200, Loss: 0.9755344986915588\n",
      "Epoch 220, Loss: 0.9732531905174255\n",
      "Epoch 240, Loss: 0.9709035158157349\n",
      "Epoch 260, Loss: 0.9684956073760986\n",
      "Epoch 280, Loss: 0.9660390615463257\n",
      "Epoch 300, Loss: 0.9635429382324219\n",
      "Epoch 320, Loss: 0.9610158205032349\n",
      "Epoch 340, Loss: 0.9584658741950989\n",
      "Epoch 360, Loss: 0.9559011459350586\n",
      "Epoch 380, Loss: 0.9533291459083557\n",
      "Epoch 400, Loss: 0.9507573843002319\n",
      "Epoch 420, Loss: 0.9481929540634155\n",
      "Epoch 440, Loss: 0.9456427097320557\n",
      "Epoch 460, Loss: 0.9431132674217224\n",
      "Epoch 480, Loss: 0.9406111836433411\n",
      "weights of  P020 :  [[array([[ 0.5893391 , -0.33840016],\n",
      "       [-0.43298668, -0.8881976 ],\n",
      "       [-0.01078139,  0.13250944],\n",
      "       [-0.00293509,  0.78447264],\n",
      "       [-0.5697534 ,  0.05429921]], dtype=float32), array([ 0.01345154, -0.01345157], dtype=float32)]]\n",
      "Lowest loss of  P020 :  0.9382653\n",
      "Epoch 0, Loss: 0.9982109665870667\n",
      "Epoch 20, Loss: 0.8966349959373474\n",
      "Epoch 40, Loss: 0.8869316577911377\n",
      "Epoch 60, Loss: 0.8841270208358765\n",
      "Epoch 80, Loss: 0.8843360543251038\n",
      "Epoch 100, Loss: 0.8876075148582458\n",
      "Epoch 120, Loss: 0.8939316272735596\n",
      "Epoch 140, Loss: 0.9030776619911194\n",
      "Epoch 160, Loss: 0.9147189855575562\n",
      "Epoch 180, Loss: 0.9285380840301514\n",
      "Epoch 200, Loss: 0.9442317485809326\n",
      "Epoch 220, Loss: 0.961552619934082\n",
      "Epoch 240, Loss: 0.9803106784820557\n",
      "Epoch 260, Loss: 1.0003623962402344\n",
      "Epoch 280, Loss: 1.0216007232666016\n",
      "Epoch 300, Loss: 1.043940544128418\n",
      "Epoch 320, Loss: 1.0673097372055054\n",
      "Epoch 340, Loss: 1.0916390419006348\n",
      "Epoch 360, Loss: 1.1168588399887085\n",
      "Epoch 380, Loss: 1.1428965330123901\n",
      "Epoch 400, Loss: 1.1696743965148926\n",
      "Epoch 420, Loss: 1.197110891342163\n",
      "Epoch 440, Loss: 1.2251214981079102\n",
      "Epoch 460, Loss: 1.2536203861236572\n",
      "Epoch 480, Loss: 1.2825210094451904\n",
      "weights of  P021 :  [[array([[ 0.32824004, -0.02112797],\n",
      "       [ 0.30248037,  0.48663187],\n",
      "       [-0.253789  ,  0.21478833],\n",
      "       [ 0.03156318, -0.83362985],\n",
      "       [ 0.55071056,  0.74995124]], dtype=float32), array([ 0.09532636, -0.09532634], dtype=float32)]]\n",
      "Lowest loss of  P021 :  0.8837184\n",
      "Epoch 0, Loss: 1.1618081331253052\n",
      "Epoch 20, Loss: 0.9140039682388306\n",
      "Epoch 40, Loss: 0.9075272083282471\n",
      "Epoch 60, Loss: 0.903224527835846\n",
      "Epoch 80, Loss: 0.903398871421814\n",
      "Epoch 100, Loss: 0.9069256782531738\n",
      "Epoch 120, Loss: 0.9146586656570435\n",
      "Epoch 140, Loss: 0.9257313013076782\n",
      "Epoch 160, Loss: 0.9400500059127808\n",
      "Epoch 180, Loss: 0.9573619365692139\n",
      "Epoch 200, Loss: 0.9775623083114624\n",
      "Epoch 220, Loss: 1.0006109476089478\n",
      "Epoch 240, Loss: 1.026458740234375\n",
      "Epoch 260, Loss: 1.0550355911254883\n",
      "Epoch 280, Loss: 1.0862213373184204\n",
      "Epoch 300, Loss: 1.1198428869247437\n",
      "Epoch 320, Loss: 1.1556819677352905\n",
      "Epoch 340, Loss: 1.1934850215911865\n",
      "Epoch 360, Loss: 1.2329761981964111\n",
      "Epoch 380, Loss: 1.2738678455352783\n",
      "Epoch 400, Loss: 1.3158714771270752\n",
      "Epoch 420, Loss: 1.3587040901184082\n",
      "Epoch 440, Loss: 1.4020944833755493\n",
      "Epoch 460, Loss: 1.4457862377166748\n",
      "Epoch 480, Loss: 1.4895410537719727\n",
      "weights of  P022 :  [[array([[ 0.42173216, -0.2692537 ],\n",
      "       [-0.715889  , -0.19594239],\n",
      "       [ 0.11225542,  0.00207059],\n",
      "       [ 0.03229613,  0.30108178],\n",
      "       [ 0.9858751 ,  0.60329235]], dtype=float32), array([-0.05951508,  0.05951506], dtype=float32)]]\n",
      "Lowest loss of  P022 :  0.90284765\n",
      "Epoch 0, Loss: 1.6152011156082153\n",
      "Epoch 20, Loss: 0.9931433200836182\n",
      "Epoch 40, Loss: 0.910995364189148\n",
      "Epoch 60, Loss: 0.895535409450531\n",
      "Epoch 80, Loss: 0.8813204765319824\n",
      "Epoch 100, Loss: 0.8697128295898438\n",
      "Epoch 120, Loss: 0.8568623065948486\n",
      "Epoch 140, Loss: 0.8438712954521179\n",
      "Epoch 160, Loss: 0.8315579891204834\n",
      "Epoch 180, Loss: 0.8203002214431763\n",
      "Epoch 200, Loss: 0.8105625510215759\n",
      "Epoch 220, Loss: 0.8025579452514648\n",
      "Epoch 240, Loss: 0.7964107394218445\n",
      "Epoch 260, Loss: 0.7921651601791382\n",
      "Epoch 280, Loss: 0.7898200750350952\n",
      "Epoch 300, Loss: 0.7893456816673279\n",
      "Epoch 320, Loss: 0.7906928062438965\n",
      "Epoch 340, Loss: 0.7937990427017212\n",
      "Epoch 360, Loss: 0.7985928058624268\n",
      "Epoch 380, Loss: 0.8049960136413574\n",
      "Epoch 400, Loss: 0.8129271268844604\n",
      "Epoch 420, Loss: 0.8223025798797607\n",
      "Epoch 440, Loss: 0.8330384492874146\n",
      "Epoch 460, Loss: 0.8450511693954468\n",
      "Epoch 480, Loss: 0.8582587838172913\n",
      "weights of  P023 :  [[array([[ 0.6574178 ,  0.32977027],\n",
      "       [-0.29878947,  0.04617205],\n",
      "       [ 0.22809151, -0.31838435],\n",
      "       [-0.4505684 ,  0.16078664],\n",
      "       [-0.31696483,  0.28327936]], dtype=float32), array([ 0.00183413, -0.00183409], dtype=float32)]]\n",
      "Lowest loss of  P023 :  0.7892914\n",
      "Epoch 0, Loss: 1.1015965938568115\n",
      "Epoch 20, Loss: 1.0251199007034302\n",
      "Epoch 40, Loss: 1.008155345916748\n",
      "Epoch 60, Loss: 1.0159740447998047\n",
      "Epoch 80, Loss: 1.036818027496338\n",
      "Epoch 100, Loss: 1.065925121307373\n",
      "Epoch 120, Loss: 1.1015723943710327\n",
      "Epoch 140, Loss: 1.142702579498291\n",
      "Epoch 160, Loss: 1.187438726425171\n",
      "Epoch 180, Loss: 1.2342188358306885\n",
      "Epoch 200, Loss: 1.2819406986236572\n",
      "Epoch 220, Loss: 1.3296782970428467\n",
      "Epoch 240, Loss: 1.3766956329345703\n",
      "Epoch 260, Loss: 1.4224498271942139\n",
      "Epoch 280, Loss: 1.4665484428405762\n",
      "Epoch 300, Loss: 1.5087285041809082\n",
      "Epoch 320, Loss: 1.5488362312316895\n",
      "Epoch 340, Loss: 1.5868043899536133\n",
      "Epoch 360, Loss: 1.6226321458816528\n",
      "Epoch 380, Loss: 1.6563684940338135\n",
      "Epoch 400, Loss: 1.6880948543548584\n",
      "Epoch 420, Loss: 1.717915415763855\n",
      "Epoch 440, Loss: 1.7459430694580078\n",
      "Epoch 460, Loss: 1.772294521331787\n",
      "Epoch 480, Loss: 1.7970833778381348\n",
      "weights of  P024 :  [[array([[-0.4356889 , -0.5115302 ],\n",
      "       [-0.37587723, -0.0283371 ],\n",
      "       [ 0.6623086 ,  0.34560966],\n",
      "       [-0.8039838 , -0.58886725],\n",
      "       [-0.6962386 , -0.76953876]], dtype=float32), array([-0.23431998,  0.23431998], dtype=float32)]]\n",
      "Lowest loss of  P024 :  1.0081524\n",
      "Epoch 0, Loss: 2.2951266765594482\n",
      "Epoch 20, Loss: 0.9705324172973633\n",
      "Epoch 40, Loss: 0.9190154075622559\n",
      "Epoch 60, Loss: 0.9111975431442261\n",
      "Epoch 80, Loss: 0.9088998436927795\n",
      "Epoch 100, Loss: 0.9077717065811157\n",
      "Epoch 120, Loss: 0.9070990085601807\n",
      "Epoch 140, Loss: 0.9065552353858948\n",
      "Epoch 160, Loss: 0.9061692357063293\n",
      "Epoch 180, Loss: 0.9059885740280151\n",
      "Epoch 200, Loss: 0.9060506820678711\n",
      "Epoch 220, Loss: 0.9063886404037476\n",
      "Epoch 240, Loss: 0.9070332050323486\n",
      "Epoch 260, Loss: 0.908012866973877\n",
      "Epoch 280, Loss: 0.9093535542488098\n",
      "Epoch 300, Loss: 0.9110785722732544\n",
      "Epoch 320, Loss: 0.9132091999053955\n",
      "Epoch 340, Loss: 0.9157640337944031\n",
      "Epoch 360, Loss: 0.9187597036361694\n",
      "Epoch 380, Loss: 0.9222109317779541\n",
      "Epoch 400, Loss: 0.9261301755905151\n",
      "Epoch 420, Loss: 0.9305276274681091\n",
      "Epoch 440, Loss: 0.9354126453399658\n",
      "Epoch 460, Loss: 0.9407922029495239\n",
      "Epoch 480, Loss: 0.9466720223426819\n",
      "weights of  P025 :  [[array([[ 0.4465413 ,  0.13977085],\n",
      "       [-0.36318696, -0.01389388],\n",
      "       [ 0.6809516 ,  0.97907877],\n",
      "       [ 0.28482783,  0.42043722],\n",
      "       [ 0.39900056, -0.24326828]], dtype=float32), array([ 0.06451768, -0.06451768], dtype=float32)]]\n",
      "Lowest loss of  P025 :  0.9059804\n",
      "Epoch 0, Loss: 1.144233226776123\n",
      "Epoch 20, Loss: 1.0924570560455322\n",
      "Epoch 40, Loss: 1.087233543395996\n",
      "Epoch 60, Loss: 1.0853725671768188\n",
      "Epoch 80, Loss: 1.0849933624267578\n",
      "Epoch 100, Loss: 1.086564302444458\n",
      "Epoch 120, Loss: 1.0906375646591187\n",
      "Epoch 140, Loss: 1.0975775718688965\n",
      "Epoch 160, Loss: 1.1076598167419434\n",
      "Epoch 180, Loss: 1.1210662126541138\n",
      "Epoch 200, Loss: 1.137895941734314\n",
      "Epoch 220, Loss: 1.1581768989562988\n",
      "Epoch 240, Loss: 1.181877613067627\n",
      "Epoch 260, Loss: 1.2089165449142456\n",
      "Epoch 280, Loss: 1.239173412322998\n",
      "Epoch 300, Loss: 1.2724957466125488\n",
      "Epoch 320, Loss: 1.308708906173706\n",
      "Epoch 340, Loss: 1.3476206064224243\n",
      "Epoch 360, Loss: 1.3890280723571777\n",
      "Epoch 380, Loss: 1.4327229261398315\n",
      "Epoch 400, Loss: 1.4784929752349854\n",
      "Epoch 420, Loss: 1.5261276960372925\n",
      "Epoch 440, Loss: 1.5754200220108032\n",
      "Epoch 460, Loss: 1.6261682510375977\n",
      "Epoch 480, Loss: 1.6781766414642334\n",
      "weights of  P026 :  [[array([[ 0.62265646, -0.30286855],\n",
      "       [-0.8322308 , -0.062644  ],\n",
      "       [ 0.88212574, -0.5731081 ],\n",
      "       [ 0.48262492,  0.9745504 ],\n",
      "       [-0.6552097 ,  0.6384663 ]], dtype=float32), array([ 0.11769956, -0.11769957], dtype=float32)]]\n",
      "Lowest loss of  P026 :  1.0849143\n",
      "Epoch 0, Loss: 0.944242537021637\n",
      "Epoch 20, Loss: 0.9014356732368469\n",
      "Epoch 40, Loss: 0.9001060724258423\n",
      "Epoch 60, Loss: 0.9138482213020325\n",
      "Epoch 80, Loss: 0.9430492520332336\n",
      "Epoch 100, Loss: 0.9866576790809631\n",
      "Epoch 120, Loss: 1.0443215370178223\n",
      "Epoch 140, Loss: 1.1154203414916992\n",
      "Epoch 160, Loss: 1.198803186416626\n",
      "Epoch 180, Loss: 1.2929372787475586\n",
      "Epoch 200, Loss: 1.3961260318756104\n",
      "Epoch 220, Loss: 1.5066707134246826\n",
      "Epoch 240, Loss: 1.622969388961792\n",
      "Epoch 260, Loss: 1.7435674667358398\n",
      "Epoch 280, Loss: 1.8671765327453613\n",
      "Epoch 300, Loss: 1.9926762580871582\n",
      "Epoch 320, Loss: 2.119100570678711\n",
      "Epoch 340, Loss: 2.2456235885620117\n",
      "Epoch 360, Loss: 2.3715386390686035\n",
      "Epoch 380, Loss: 2.496244192123413\n",
      "Epoch 400, Loss: 2.6192281246185303\n",
      "Epoch 420, Loss: 2.74005126953125\n",
      "Epoch 440, Loss: 2.8583414554595947\n",
      "Epoch 460, Loss: 2.9737846851348877\n",
      "Epoch 480, Loss: 3.0861148834228516\n",
      "weights of  P027 :  [[array([[-0.33870423, -0.6770317 ],\n",
      "       [-0.47281227, -0.5434091 ],\n",
      "       [-0.6988254 ,  0.5016489 ],\n",
      "       [ 0.42544153, -0.53433055],\n",
      "       [-0.25962365,  0.2227048 ]], dtype=float32), array([-0.0560729 ,  0.05607289], dtype=float32)]]\n",
      "Lowest loss of  P027 :  0.8981701\n",
      "Epoch 0, Loss: 1.1251622438430786\n",
      "Epoch 20, Loss: 0.9121680855751038\n",
      "Epoch 40, Loss: 0.9039789438247681\n",
      "Epoch 60, Loss: 0.9019008278846741\n",
      "Epoch 80, Loss: 0.9018895626068115\n",
      "Epoch 100, Loss: 0.9057613015174866\n",
      "Epoch 120, Loss: 0.9130748510360718\n",
      "Epoch 140, Loss: 0.9239323139190674\n",
      "Epoch 160, Loss: 0.9378350973129272\n",
      "Epoch 180, Loss: 0.9545753002166748\n",
      "Epoch 200, Loss: 0.9736919403076172\n",
      "Epoch 220, Loss: 0.994836688041687\n",
      "Epoch 240, Loss: 1.0176358222961426\n",
      "Epoch 260, Loss: 1.0417718887329102\n",
      "Epoch 280, Loss: 1.0669584274291992\n",
      "Epoch 300, Loss: 1.0929522514343262\n",
      "Epoch 320, Loss: 1.119546890258789\n",
      "Epoch 340, Loss: 1.1465692520141602\n",
      "Epoch 360, Loss: 1.173875331878662\n",
      "Epoch 380, Loss: 1.2013452053070068\n",
      "Epoch 400, Loss: 1.2288771867752075\n",
      "Epoch 420, Loss: 1.2563841342926025\n",
      "Epoch 440, Loss: 1.2837897539138794\n",
      "Epoch 460, Loss: 1.3110253810882568\n",
      "Epoch 480, Loss: 1.3380284309387207\n",
      "weights of  P028 :  [[array([[ 0.31813332,  0.5181666 ],\n",
      "       [-0.32479405, -0.59842086],\n",
      "       [-0.50033706,  0.74935216],\n",
      "       [ 0.11953858, -0.08427926],\n",
      "       [ 0.64345306, -0.3381096 ]], dtype=float32), array([ 0.08465735, -0.08465733], dtype=float32)]]\n",
      "Lowest loss of  P028 :  0.9014838\n",
      "Epoch 0, Loss: 6.387195110321045\n",
      "Epoch 20, Loss: 3.7270047664642334\n",
      "Epoch 40, Loss: 1.2871800661087036\n",
      "Epoch 60, Loss: 0.7958968281745911\n",
      "Epoch 80, Loss: 0.7304370999336243\n",
      "Epoch 100, Loss: 0.7236523628234863\n",
      "Epoch 120, Loss: 0.7157588005065918\n",
      "Epoch 140, Loss: 0.7086727023124695\n",
      "Epoch 160, Loss: 0.7019599676132202\n",
      "Epoch 180, Loss: 0.6959479451179504\n",
      "Epoch 200, Loss: 0.690667450428009\n",
      "Epoch 220, Loss: 0.6862678527832031\n",
      "Epoch 240, Loss: 0.6828329563140869\n",
      "Epoch 260, Loss: 0.6804459691047668\n",
      "Epoch 280, Loss: 0.6791681051254272\n",
      "Epoch 300, Loss: 0.6790496110916138\n",
      "Epoch 320, Loss: 0.6801284551620483\n",
      "Epoch 340, Loss: 0.6824328303337097\n",
      "Epoch 360, Loss: 0.6859827041625977\n",
      "Epoch 380, Loss: 0.6907905340194702\n",
      "Epoch 400, Loss: 0.6968628764152527\n",
      "Epoch 420, Loss: 0.704200804233551\n",
      "Epoch 440, Loss: 0.7128008008003235\n",
      "Epoch 460, Loss: 0.722655713558197\n",
      "Epoch 480, Loss: 0.7337543964385986\n",
      "weights of  P029 :  [[array([[ 0.63597167, -0.48777175],\n",
      "       [-0.33235732,  0.09591942],\n",
      "       [-0.24821359,  0.03487998],\n",
      "       [ 0.17201696, -0.27254152],\n",
      "       [ 0.09408498,  0.63390166]], dtype=float32), array([ 0.46135938, -0.46135938], dtype=float32)]]\n",
      "Lowest loss of  P029 :  0.67895526\n",
      "Epoch 0, Loss: 2.2336084842681885\n",
      "Epoch 20, Loss: 1.1360584497451782\n",
      "Epoch 40, Loss: 1.0439554452896118\n",
      "Epoch 60, Loss: 1.041900396347046\n",
      "Epoch 80, Loss: 1.0411324501037598\n",
      "Epoch 100, Loss: 1.0408880710601807\n",
      "Epoch 120, Loss: 1.0406628847122192\n",
      "Epoch 140, Loss: 1.0404715538024902\n",
      "Epoch 160, Loss: 1.0402789115905762\n",
      "Epoch 180, Loss: 1.0400820970535278\n",
      "Epoch 200, Loss: 1.0398824214935303\n",
      "Epoch 220, Loss: 1.039682388305664\n",
      "Epoch 240, Loss: 1.039483666419983\n",
      "Epoch 260, Loss: 1.0392881631851196\n",
      "Epoch 280, Loss: 1.0390976667404175\n",
      "Epoch 300, Loss: 1.0389139652252197\n",
      "Epoch 320, Loss: 1.0387386083602905\n",
      "Epoch 340, Loss: 1.0385732650756836\n",
      "Epoch 360, Loss: 1.0384193658828735\n",
      "Epoch 380, Loss: 1.038278579711914\n",
      "Epoch 400, Loss: 1.0381520986557007\n",
      "Epoch 420, Loss: 1.0380417108535767\n",
      "Epoch 440, Loss: 1.0379483699798584\n",
      "Epoch 460, Loss: 1.0378735065460205\n",
      "Epoch 480, Loss: 1.0378183126449585\n",
      "weights of  P030 :  [[array([[ 0.8268261 ,  0.00651346],\n",
      "       [ 0.08912411, -0.7719061 ],\n",
      "       [-0.62745446,  0.15368658],\n",
      "       [-0.60667354,  0.47921735],\n",
      "       [-0.95680916, -0.64188427]], dtype=float32), array([-0.22680937,  0.22680934], dtype=float32)]]\n",
      "Lowest loss of  P030 :  1.0377854\n",
      "Epoch 0, Loss: 1.7372757196426392\n",
      "Epoch 20, Loss: 1.1465367078781128\n",
      "Epoch 40, Loss: 0.9958856105804443\n",
      "Epoch 60, Loss: 0.9742224216461182\n",
      "Epoch 80, Loss: 0.9528746604919434\n",
      "Epoch 100, Loss: 0.9454635381698608\n",
      "Epoch 120, Loss: 0.9451665878295898\n",
      "Epoch 140, Loss: 0.9503891468048096\n",
      "Epoch 160, Loss: 0.9606550931930542\n",
      "Epoch 180, Loss: 0.9754860401153564\n",
      "Epoch 200, Loss: 0.9945780634880066\n",
      "Epoch 220, Loss: 1.017657995223999\n",
      "Epoch 240, Loss: 1.0444064140319824\n",
      "Epoch 260, Loss: 1.074472427368164\n",
      "Epoch 280, Loss: 1.107487678527832\n",
      "Epoch 300, Loss: 1.1430854797363281\n",
      "Epoch 320, Loss: 1.180916666984558\n",
      "Epoch 340, Loss: 1.2206553220748901\n",
      "Epoch 360, Loss: 1.2620066404342651\n",
      "Epoch 380, Loss: 1.3047072887420654\n",
      "Epoch 400, Loss: 1.348527193069458\n",
      "Epoch 420, Loss: 1.3932656049728394\n",
      "Epoch 440, Loss: 1.4387524127960205\n",
      "Epoch 460, Loss: 1.484839916229248\n",
      "Epoch 480, Loss: 1.5314064025878906\n",
      "weights of  P031 :  [[array([[-0.44186515, -0.599515  ],\n",
      "       [-0.46642303,  0.38516602],\n",
      "       [-0.3997274 , -1.1844199 ],\n",
      "       [ 0.23185803, -0.46143416],\n",
      "       [-0.0240148 ,  0.45218313]], dtype=float32), array([-0.35153785,  0.35153785], dtype=float32)]]\n",
      "Lowest loss of  P031 :  0.94457126\n",
      "Epoch 0, Loss: 1.0718014240264893\n",
      "Epoch 20, Loss: 1.0432037115097046\n",
      "Epoch 40, Loss: 1.0586144924163818\n",
      "Epoch 60, Loss: 1.1011539697647095\n",
      "Epoch 80, Loss: 1.1564600467681885\n",
      "Epoch 100, Loss: 1.2227015495300293\n",
      "Epoch 120, Loss: 1.3032172918319702\n",
      "Epoch 140, Loss: 1.400402545928955\n",
      "Epoch 160, Loss: 1.5142936706542969\n",
      "Epoch 180, Loss: 1.6437113285064697\n",
      "Epoch 200, Loss: 1.787153720855713\n",
      "Epoch 220, Loss: 1.943182110786438\n",
      "Epoch 240, Loss: 2.110494375228882\n",
      "Epoch 260, Loss: 2.2879116535186768\n",
      "Epoch 280, Loss: 2.474358558654785\n",
      "Epoch 300, Loss: 2.6688642501831055\n",
      "Epoch 320, Loss: 2.8705551624298096\n",
      "Epoch 340, Loss: 3.0786502361297607\n",
      "Epoch 360, Loss: 3.2924516201019287\n",
      "Epoch 380, Loss: 3.5113353729248047\n",
      "Epoch 400, Loss: 3.734741449356079\n",
      "Epoch 420, Loss: 3.962169647216797\n",
      "Epoch 440, Loss: 4.193170547485352\n",
      "Epoch 460, Loss: 4.427340984344482\n",
      "Epoch 480, Loss: 4.664318084716797\n",
      "weights of  P032 :  [[array([[ 0.26477146, -0.37079993],\n",
      "       [-0.93116665, -0.825315  ],\n",
      "       [ 0.76249975, -0.01315804],\n",
      "       [-0.70453304,  0.20424512],\n",
      "       [-0.63232535,  0.90812546]], dtype=float32), array([-0.13943243,  0.13943243], dtype=float32)]]\n",
      "Lowest loss of  P032 :  1.0419295\n",
      "Epoch 0, Loss: 1.9562153816223145\n",
      "Epoch 20, Loss: 1.0299440622329712\n",
      "Epoch 40, Loss: 0.9749356508255005\n",
      "Epoch 60, Loss: 0.9427251815795898\n",
      "Epoch 80, Loss: 0.9208364486694336\n",
      "Epoch 100, Loss: 0.9085783958435059\n",
      "Epoch 120, Loss: 0.9041798114776611\n",
      "Epoch 140, Loss: 0.9054051041603088\n",
      "Epoch 160, Loss: 0.9107986092567444\n",
      "Epoch 180, Loss: 0.919163167476654\n",
      "Epoch 200, Loss: 0.9297361373901367\n",
      "Epoch 220, Loss: 0.9420053362846375\n",
      "Epoch 240, Loss: 0.9556135535240173\n",
      "Epoch 260, Loss: 0.9703036546707153\n",
      "Epoch 280, Loss: 0.9858726859092712\n",
      "Epoch 300, Loss: 1.0021480321884155\n",
      "Epoch 320, Loss: 1.0189745426177979\n",
      "Epoch 340, Loss: 1.0362097024917603\n",
      "Epoch 360, Loss: 1.0537199974060059\n",
      "Epoch 380, Loss: 1.071381688117981\n",
      "Epoch 400, Loss: 1.0890817642211914\n",
      "Epoch 420, Loss: 1.1067167520523071\n",
      "Epoch 440, Loss: 1.1241947412490845\n",
      "Epoch 460, Loss: 1.141434669494629\n",
      "Epoch 480, Loss: 1.1583659648895264\n",
      "weights of  P033 :  [[array([[ 0.12356377, -0.6942456 ],\n",
      "       [ 0.5114013 ,  0.90054256],\n",
      "       [-0.48508465, -0.40009317],\n",
      "       [ 0.45761818,  0.86262053],\n",
      "       [ 0.4050554 , -0.18364725]], dtype=float32), array([ 0.16216762, -0.16216764], dtype=float32)]]\n",
      "Lowest loss of  P033 :  0.9040195\n",
      "Epoch 0, Loss: 2.815819263458252\n",
      "Epoch 20, Loss: 0.9587402939796448\n",
      "Epoch 40, Loss: 0.9369134306907654\n",
      "Epoch 60, Loss: 0.914024829864502\n",
      "Epoch 80, Loss: 0.8911538124084473\n",
      "Epoch 100, Loss: 0.8691999912261963\n",
      "Epoch 120, Loss: 0.8495467305183411\n",
      "Epoch 140, Loss: 0.8329365253448486\n",
      "Epoch 160, Loss: 0.8198837041854858\n",
      "Epoch 180, Loss: 0.8106333017349243\n",
      "Epoch 200, Loss: 0.8052607178688049\n",
      "Epoch 220, Loss: 0.8037146329879761\n",
      "Epoch 240, Loss: 0.805849552154541\n",
      "Epoch 260, Loss: 0.8114602565765381\n",
      "Epoch 280, Loss: 0.8203081488609314\n",
      "Epoch 300, Loss: 0.8321398496627808\n",
      "Epoch 320, Loss: 0.8467001914978027\n",
      "Epoch 340, Loss: 0.8637415170669556\n",
      "Epoch 360, Loss: 0.883029043674469\n",
      "Epoch 380, Loss: 0.9043439626693726\n",
      "Epoch 400, Loss: 0.9274851083755493\n",
      "Epoch 420, Loss: 0.9522684812545776\n",
      "Epoch 440, Loss: 0.9785279035568237\n",
      "Epoch 460, Loss: 1.0061129331588745\n",
      "Epoch 480, Loss: 1.0348880290985107\n",
      "weights of  P034 :  [[array([[ 0.8032809 , -0.2513406 ],\n",
      "       [-0.04172864,  1.0790453 ],\n",
      "       [-0.29839584, -0.5583793 ],\n",
      "       [ 0.72074354,  0.57609236],\n",
      "       [ 0.11051194,  0.53501856]], dtype=float32), array([-0.37597686,  0.37597686], dtype=float32)]]\n",
      "Lowest loss of  P034 :  0.8037007\n",
      "Epoch 0, Loss: 1.4103275537490845\n",
      "Epoch 20, Loss: 0.852154552936554\n",
      "Epoch 40, Loss: 0.8193227052688599\n",
      "Epoch 60, Loss: 0.8156129121780396\n",
      "Epoch 80, Loss: 0.8151034116744995\n",
      "Epoch 100, Loss: 0.8173871040344238\n",
      "Epoch 120, Loss: 0.8217265009880066\n",
      "Epoch 140, Loss: 0.8279774188995361\n",
      "Epoch 160, Loss: 0.8359822630882263\n",
      "Epoch 180, Loss: 0.8455695509910583\n",
      "Epoch 200, Loss: 0.8565946221351624\n",
      "Epoch 220, Loss: 0.8689323663711548\n",
      "Epoch 240, Loss: 0.8824723958969116\n",
      "Epoch 260, Loss: 0.8971158266067505\n",
      "Epoch 280, Loss: 0.9127718210220337\n",
      "Epoch 300, Loss: 0.9293553829193115\n",
      "Epoch 320, Loss: 0.946785569190979\n",
      "Epoch 340, Loss: 0.9649845361709595\n",
      "Epoch 360, Loss: 0.9838769435882568\n",
      "Epoch 380, Loss: 1.0033897161483765\n",
      "Epoch 400, Loss: 1.023451805114746\n",
      "Epoch 420, Loss: 1.043994426727295\n",
      "Epoch 440, Loss: 1.0649516582489014\n",
      "Epoch 460, Loss: 1.0862599611282349\n",
      "Epoch 480, Loss: 1.1078588962554932\n",
      "weights of  P035 :  [[array([[ 0.536934  , -0.5273382 ],\n",
      "       [ 0.21594833,  0.04755257],\n",
      "       [-0.17252275,  0.8389429 ],\n",
      "       [-0.05684952, -0.36455616],\n",
      "       [-0.36535725,  0.41456404]], dtype=float32), array([ 0.08607224, -0.08607223], dtype=float32)]]\n",
      "Lowest loss of  P035 :  0.81464803\n",
      "Epoch 0, Loss: 0.8801611661911011\n",
      "Epoch 20, Loss: 0.8475786447525024\n",
      "Epoch 40, Loss: 0.8167433738708496\n",
      "Epoch 60, Loss: 0.8174655437469482\n",
      "Epoch 80, Loss: 0.8427882194519043\n",
      "Epoch 100, Loss: 0.884483277797699\n",
      "Epoch 120, Loss: 0.933853805065155\n",
      "Epoch 140, Loss: 0.9841389656066895\n",
      "Epoch 160, Loss: 1.0308103561401367\n",
      "Epoch 180, Loss: 1.0712943077087402\n",
      "Epoch 200, Loss: 1.1046072244644165\n",
      "Epoch 220, Loss: 1.1308493614196777\n",
      "Epoch 240, Loss: 1.1507718563079834\n",
      "Epoch 260, Loss: 1.1654260158538818\n",
      "Epoch 280, Loss: 1.175915241241455\n",
      "Epoch 300, Loss: 1.1832524538040161\n",
      "Epoch 320, Loss: 1.1882870197296143\n",
      "Epoch 340, Loss: 1.1916909217834473\n",
      "Epoch 360, Loss: 1.1939680576324463\n",
      "Epoch 380, Loss: 1.1954827308654785\n",
      "Epoch 400, Loss: 1.1964893341064453\n",
      "Epoch 420, Loss: 1.1971611976623535\n",
      "Epoch 440, Loss: 1.197613000869751\n",
      "Epoch 460, Loss: 1.197919487953186\n",
      "Epoch 480, Loss: 1.1981298923492432\n",
      "weights of  P036 :  [[array([[-0.41187477,  0.34075424],\n",
      "       [-0.11741062, -0.38806087],\n",
      "       [ 0.43309507,  0.33309275],\n",
      "       [-0.20473303,  0.10619994],\n",
      "       [ 0.60403055, -0.3009206 ]], dtype=float32), array([ 0.24055982, -0.24055983], dtype=float32)]]\n",
      "Lowest loss of  P036 :  0.8133711\n",
      "Epoch 0, Loss: 2.2759146690368652\n",
      "Epoch 20, Loss: 1.0204627513885498\n",
      "Epoch 40, Loss: 0.925233006477356\n",
      "Epoch 60, Loss: 0.9250075817108154\n",
      "Epoch 80, Loss: 0.9211989045143127\n",
      "Epoch 100, Loss: 0.9193819165229797\n",
      "Epoch 120, Loss: 0.917454183101654\n",
      "Epoch 140, Loss: 0.9154384136199951\n",
      "Epoch 160, Loss: 0.9133569598197937\n",
      "Epoch 180, Loss: 0.9112396240234375\n",
      "Epoch 200, Loss: 0.9091120958328247\n",
      "Epoch 220, Loss: 0.9069974422454834\n",
      "Epoch 240, Loss: 0.9049164056777954\n",
      "Epoch 260, Loss: 0.9028875827789307\n",
      "Epoch 280, Loss: 0.9009280204772949\n",
      "Epoch 300, Loss: 0.8990535736083984\n",
      "Epoch 320, Loss: 0.8972783088684082\n",
      "Epoch 340, Loss: 0.8956153392791748\n",
      "Epoch 360, Loss: 0.8940761685371399\n",
      "Epoch 380, Loss: 0.8926711082458496\n",
      "Epoch 400, Loss: 0.8914093971252441\n",
      "Epoch 420, Loss: 0.8902989625930786\n",
      "Epoch 440, Loss: 0.8893468976020813\n",
      "Epoch 460, Loss: 0.8885588645935059\n",
      "Epoch 480, Loss: 0.8879397511482239\n",
      "weights of  P037 :  [[array([[ 0.5169334 , -0.9206688 ],\n",
      "       [-0.09304346,  0.16272022],\n",
      "       [ 0.08111849,  0.6799331 ],\n",
      "       [-0.0331327 , -0.5277552 ],\n",
      "       [-0.38521385, -0.16043952]], dtype=float32), array([ 0.29991096, -0.29991096], dtype=float32)]]\n",
      "Lowest loss of  P037 :  0.88751173\n",
      "Epoch 0, Loss: 1.0069997310638428\n",
      "Epoch 20, Loss: 0.8839972019195557\n",
      "Epoch 40, Loss: 0.8683515191078186\n",
      "Epoch 60, Loss: 0.8604551553726196\n",
      "Epoch 80, Loss: 0.8554517030715942\n",
      "Epoch 100, Loss: 0.8546931743621826\n",
      "Epoch 120, Loss: 0.8581570386886597\n",
      "Epoch 140, Loss: 0.865625262260437\n",
      "Epoch 160, Loss: 0.8766927123069763\n",
      "Epoch 180, Loss: 0.8908068537712097\n",
      "Epoch 200, Loss: 0.9073784947395325\n",
      "Epoch 220, Loss: 0.9258542656898499\n",
      "Epoch 240, Loss: 0.9457534551620483\n",
      "Epoch 260, Loss: 0.9666810035705566\n",
      "Epoch 280, Loss: 0.9883267283439636\n",
      "Epoch 300, Loss: 1.0104563236236572\n",
      "Epoch 320, Loss: 1.0328985452651978\n",
      "Epoch 340, Loss: 1.0555311441421509\n",
      "Epoch 360, Loss: 1.078268051147461\n",
      "Epoch 380, Loss: 1.101048231124878\n",
      "Epoch 400, Loss: 1.123827338218689\n",
      "Epoch 420, Loss: 1.1465702056884766\n",
      "Epoch 440, Loss: 1.1692473888397217\n",
      "Epoch 460, Loss: 1.191831111907959\n",
      "Epoch 480, Loss: 1.214294672012329\n",
      "weights of  P038 :  [[array([[-0.38067392, -0.55409926],\n",
      "       [ 0.13944797, -0.02687845],\n",
      "       [-0.34097424, -0.2657629 ],\n",
      "       [ 0.42875308,  0.25095087],\n",
      "       [-0.15583599,  0.9162621 ]], dtype=float32), array([-0.21552837,  0.21552838], dtype=float32)]]\n",
      "Lowest loss of  P038 :  0.85456127\n",
      "Epoch 0, Loss: 1.0210204124450684\n",
      "Epoch 20, Loss: 0.938637912273407\n",
      "Epoch 40, Loss: 0.9366070032119751\n",
      "Epoch 60, Loss: 0.9216063022613525\n",
      "Epoch 80, Loss: 0.9331802129745483\n",
      "Epoch 100, Loss: 0.9580814838409424\n",
      "Epoch 120, Loss: 0.9900183081626892\n",
      "Epoch 140, Loss: 1.0253289937973022\n",
      "Epoch 160, Loss: 1.061018466949463\n",
      "Epoch 180, Loss: 1.0954859256744385\n",
      "Epoch 200, Loss: 1.1280863285064697\n",
      "Epoch 220, Loss: 1.1587154865264893\n",
      "Epoch 240, Loss: 1.1875507831573486\n",
      "Epoch 260, Loss: 1.2148630619049072\n",
      "Epoch 280, Loss: 1.2409172058105469\n",
      "Epoch 300, Loss: 1.2659270763397217\n",
      "Epoch 320, Loss: 1.2900410890579224\n",
      "Epoch 340, Loss: 1.313348412513733\n",
      "Epoch 360, Loss: 1.335889458656311\n",
      "Epoch 380, Loss: 1.3576688766479492\n",
      "Epoch 400, Loss: 1.3786683082580566\n",
      "Epoch 420, Loss: 1.3988549709320068\n",
      "Epoch 440, Loss: 1.4181914329528809\n",
      "Epoch 460, Loss: 1.436639666557312\n",
      "Epoch 480, Loss: 1.454166293144226\n",
      "weights of  P039 :  [[array([[ 0.6321575 ,  0.4745812 ],\n",
      "       [ 0.63660616,  0.30826074],\n",
      "       [-0.10534925,  0.5017873 ],\n",
      "       [ 0.07499938, -1.00792   ],\n",
      "       [ 0.3062273 ,  0.20597614]], dtype=float32), array([ 0.10616518, -0.1061652 ], dtype=float32)]]\n",
      "Lowest loss of  P039 :  0.92157847\n",
      "Epoch 0, Loss: 1.2292619943618774\n",
      "Epoch 20, Loss: 1.0278213024139404\n",
      "Epoch 40, Loss: 0.9350043535232544\n",
      "Epoch 60, Loss: 0.8861594796180725\n",
      "Epoch 80, Loss: 0.8697949051856995\n",
      "Epoch 100, Loss: 0.8659411668777466\n",
      "Epoch 120, Loss: 0.8653241395950317\n",
      "Epoch 140, Loss: 0.8665342330932617\n",
      "Epoch 160, Loss: 0.8697742223739624\n",
      "Epoch 180, Loss: 0.875289797782898\n",
      "Epoch 200, Loss: 0.8832629919052124\n",
      "Epoch 220, Loss: 0.8938255906105042\n",
      "Epoch 240, Loss: 0.9070640802383423\n",
      "Epoch 260, Loss: 0.923022985458374\n",
      "Epoch 280, Loss: 0.9417095184326172\n",
      "Epoch 300, Loss: 0.9630976915359497\n",
      "Epoch 320, Loss: 0.9871324300765991\n",
      "Epoch 340, Loss: 1.0137338638305664\n",
      "Epoch 360, Loss: 1.0428011417388916\n",
      "Epoch 380, Loss: 1.0742169618606567\n",
      "Epoch 400, Loss: 1.107850193977356\n",
      "Epoch 420, Loss: 1.1435598134994507\n",
      "Epoch 440, Loss: 1.1811977624893188\n",
      "Epoch 460, Loss: 1.220611333847046\n",
      "Epoch 480, Loss: 1.261646032333374\n",
      "weights of  P040 :  [[array([[-0.53794897, -0.5500931 ],\n",
      "       [ 0.48370776,  0.60473204],\n",
      "       [ 0.34686285, -0.2812313 ],\n",
      "       [-0.15396252, -0.5495196 ],\n",
      "       [-0.35556722,  0.02098453]], dtype=float32), array([ 0.31840965, -0.31840965], dtype=float32)]]\n",
      "Lowest loss of  P040 :  0.8653041\n",
      "Epoch 0, Loss: 6.808785915374756\n",
      "Epoch 20, Loss: 2.7380642890930176\n",
      "Epoch 40, Loss: 0.9703669548034668\n",
      "Epoch 60, Loss: 0.8382105827331543\n",
      "Epoch 80, Loss: 0.8314827680587769\n",
      "Epoch 100, Loss: 0.8310598731040955\n",
      "Epoch 120, Loss: 0.8308970928192139\n",
      "Epoch 140, Loss: 0.8312177062034607\n",
      "Epoch 160, Loss: 0.8315892815589905\n",
      "Epoch 180, Loss: 0.8320680260658264\n",
      "Epoch 200, Loss: 0.8326373100280762\n",
      "Epoch 220, Loss: 0.8333003520965576\n",
      "Epoch 240, Loss: 0.8340683579444885\n",
      "Epoch 260, Loss: 0.8349481821060181\n",
      "Epoch 280, Loss: 0.8359472751617432\n",
      "Epoch 300, Loss: 0.8370723128318787\n",
      "Epoch 320, Loss: 0.8383303880691528\n",
      "Epoch 340, Loss: 0.8397279977798462\n",
      "Epoch 360, Loss: 0.8412714600563049\n",
      "Epoch 380, Loss: 0.8429666757583618\n",
      "Epoch 400, Loss: 0.8448193073272705\n",
      "Epoch 420, Loss: 0.8468348383903503\n",
      "Epoch 440, Loss: 0.8490182161331177\n",
      "Epoch 460, Loss: 0.8513740301132202\n",
      "Epoch 480, Loss: 0.8539066910743713\n",
      "weights of  P041 :  [[array([[ 0.5012945 , -0.04554597],\n",
      "       [-0.9117764 ,  0.08204287],\n",
      "       [-0.04328124, -0.14666386],\n",
      "       [-0.33038825, -0.34890723],\n",
      "       [ 0.05635992, -0.56075263]], dtype=float32), array([ 0.33832678, -0.33832678], dtype=float32)]]\n",
      "Lowest loss of  P041 :  0.8301413\n",
      "Epoch 0, Loss: 1.6341160535812378\n",
      "Epoch 20, Loss: 1.154248833656311\n",
      "Epoch 40, Loss: 1.0482536554336548\n",
      "Epoch 60, Loss: 0.9659327268600464\n",
      "Epoch 80, Loss: 0.8954326510429382\n",
      "Epoch 100, Loss: 0.8380460739135742\n",
      "Epoch 120, Loss: 0.7957223653793335\n",
      "Epoch 140, Loss: 0.7683749198913574\n",
      "Epoch 160, Loss: 0.7548045516014099\n",
      "Epoch 180, Loss: 0.7533159255981445\n",
      "Epoch 200, Loss: 0.7621065378189087\n",
      "Epoch 220, Loss: 0.7794778347015381\n",
      "Epoch 240, Loss: 0.8039263486862183\n",
      "Epoch 260, Loss: 0.8341694474220276\n",
      "Epoch 280, Loss: 0.8691329956054688\n",
      "Epoch 300, Loss: 0.907927393913269\n",
      "Epoch 320, Loss: 0.9498186111450195\n",
      "Epoch 340, Loss: 0.9942013621330261\n",
      "Epoch 360, Loss: 1.0405762195587158\n",
      "Epoch 380, Loss: 1.088529109954834\n",
      "Epoch 400, Loss: 1.1377164125442505\n",
      "Epoch 420, Loss: 1.1878504753112793\n",
      "Epoch 440, Loss: 1.238690972328186\n",
      "Epoch 460, Loss: 1.2900354862213135\n",
      "Epoch 480, Loss: 1.3417129516601562\n",
      "weights of  P042 :  [[array([[ 0.7521862 , -0.4374143 ],\n",
      "       [-0.6469514 ,  0.20059997],\n",
      "       [ 0.6156963 ,  0.33971027],\n",
      "       [ 0.47807112,  0.60458225],\n",
      "       [-0.43267354, -0.2687652 ]], dtype=float32), array([-0.1329425 ,  0.13294248], dtype=float32)]]\n",
      "Lowest loss of  P042 :  0.75257593\n",
      "Epoch 0, Loss: 1.1441583633422852\n",
      "Epoch 20, Loss: 1.0185445547103882\n",
      "Epoch 40, Loss: 0.9595499634742737\n",
      "Epoch 60, Loss: 0.9374420046806335\n",
      "Epoch 80, Loss: 0.9437402486801147\n",
      "Epoch 100, Loss: 0.971335768699646\n",
      "Epoch 120, Loss: 1.0138894319534302\n",
      "Epoch 140, Loss: 1.0659129619598389\n",
      "Epoch 160, Loss: 1.1240849494934082\n",
      "Epoch 180, Loss: 1.1866719722747803\n",
      "Epoch 200, Loss: 1.2529354095458984\n",
      "Epoch 220, Loss: 1.3227108716964722\n",
      "Epoch 240, Loss: 1.3960908651351929\n",
      "Epoch 260, Loss: 1.4732290506362915\n",
      "Epoch 280, Loss: 1.5542398691177368\n",
      "Epoch 300, Loss: 1.6391597986221313\n",
      "Epoch 320, Loss: 1.7279436588287354\n",
      "Epoch 340, Loss: 1.82047700881958\n",
      "Epoch 360, Loss: 1.9165949821472168\n",
      "Epoch 380, Loss: 2.016099214553833\n",
      "Epoch 400, Loss: 2.118772268295288\n",
      "Epoch 420, Loss: 2.2243916988372803\n",
      "Epoch 440, Loss: 2.3327345848083496\n",
      "Epoch 460, Loss: 2.4435863494873047\n",
      "Epoch 480, Loss: 2.5567402839660645\n",
      "weights of  P043 :  [[array([[ 0.09048389, -0.16404343],\n",
      "       [ 0.08591505,  0.64820576],\n",
      "       [-0.508881  , -0.33946732],\n",
      "       [ 0.76423013, -0.10252789],\n",
      "       [-0.40262234, -1.142986  ]], dtype=float32), array([-0.08099576,  0.08099577], dtype=float32)]]\n",
      "Lowest loss of  P043 :  0.936269\n",
      "Epoch 0, Loss: 1.0017677545547485\n",
      "Epoch 20, Loss: 0.8974848985671997\n",
      "Epoch 40, Loss: 0.8322252631187439\n",
      "Epoch 60, Loss: 0.7979859709739685\n",
      "Epoch 80, Loss: 0.7942507863044739\n",
      "Epoch 100, Loss: 0.8169788122177124\n",
      "Epoch 120, Loss: 0.8597350120544434\n",
      "Epoch 140, Loss: 0.9174113273620605\n",
      "Epoch 160, Loss: 0.9860862493515015\n",
      "Epoch 180, Loss: 1.0627779960632324\n",
      "Epoch 200, Loss: 1.1452581882476807\n",
      "Epoch 220, Loss: 1.2318445444107056\n",
      "Epoch 240, Loss: 1.3212283849716187\n",
      "Epoch 260, Loss: 1.4123544692993164\n",
      "Epoch 280, Loss: 1.5043458938598633\n",
      "Epoch 300, Loss: 1.5964553356170654\n",
      "Epoch 320, Loss: 1.688037633895874\n",
      "Epoch 340, Loss: 1.7785319089889526\n",
      "Epoch 360, Loss: 1.8674523830413818\n",
      "Epoch 380, Loss: 1.9543805122375488\n",
      "Epoch 400, Loss: 2.038959264755249\n",
      "Epoch 420, Loss: 2.1208901405334473\n",
      "Epoch 440, Loss: 2.1999282836914062\n",
      "Epoch 460, Loss: 2.275878429412842\n",
      "Epoch 480, Loss: 2.3485918045043945\n",
      "weights of  P044 :  [[array([[ 0.18265295,  0.1638199 ],\n",
      "       [-0.606455  ,  0.17082605],\n",
      "       [-0.03803873, -0.14939825],\n",
      "       [ 0.5555262 ,  0.4909836 ],\n",
      "       [ 0.62038654, -0.40248024]], dtype=float32), array([-0.3331023 ,  0.33310232], dtype=float32)]]\n",
      "Lowest loss of  P044 :  0.7921703\n",
      "Epoch 0, Loss: 1.0153920650482178\n",
      "Epoch 20, Loss: 1.031085729598999\n",
      "Epoch 40, Loss: 1.1052906513214111\n",
      "Epoch 60, Loss: 1.204138159751892\n",
      "Epoch 80, Loss: 1.3110002279281616\n",
      "Epoch 100, Loss: 1.4168925285339355\n",
      "Epoch 120, Loss: 1.5183167457580566\n",
      "Epoch 140, Loss: 1.614591121673584\n",
      "Epoch 160, Loss: 1.7060258388519287\n",
      "Epoch 180, Loss: 1.79301118850708\n",
      "Epoch 200, Loss: 1.8757072687149048\n",
      "Epoch 220, Loss: 1.9540421962738037\n",
      "Epoch 240, Loss: 2.027817964553833\n",
      "Epoch 260, Loss: 2.096827507019043\n",
      "Epoch 280, Loss: 2.1609227657318115\n",
      "Epoch 300, Loss: 2.2200369834899902\n",
      "Epoch 320, Loss: 2.274184226989746\n",
      "Epoch 340, Loss: 2.323448657989502\n",
      "Epoch 360, Loss: 2.367973804473877\n",
      "Epoch 380, Loss: 2.4079501628875732\n",
      "Epoch 400, Loss: 2.443605422973633\n",
      "Epoch 420, Loss: 2.4751999378204346\n",
      "Epoch 440, Loss: 2.503014087677002\n",
      "Epoch 460, Loss: 2.5273423194885254\n",
      "Epoch 480, Loss: 2.548487663269043\n",
      "weights of  P045 :  [[array([[-0.24038836, -0.788978  ],\n",
      "       [ 0.0307434 ,  0.94054097],\n",
      "       [ 0.09131283, -0.5753325 ],\n",
      "       [ 0.665198  ,  0.7898012 ],\n",
      "       [ 0.53365767, -0.4794584 ]], dtype=float32), array([ 0.05301222, -0.05301222], dtype=float32)]]\n",
      "Lowest loss of  P045 :  0.9956337\n",
      "Epoch 0, Loss: 0.9872470498085022\n",
      "Epoch 20, Loss: 0.9799938201904297\n",
      "Epoch 40, Loss: 1.0134637355804443\n",
      "Epoch 60, Loss: 1.0897263288497925\n",
      "Epoch 80, Loss: 1.197736144065857\n",
      "Epoch 100, Loss: 1.326465368270874\n",
      "Epoch 120, Loss: 1.4674955606460571\n",
      "Epoch 140, Loss: 1.6148276329040527\n",
      "Epoch 160, Loss: 1.7641164064407349\n",
      "Epoch 180, Loss: 1.9121726751327515\n",
      "Epoch 200, Loss: 2.0566327571868896\n",
      "Epoch 220, Loss: 2.1957757472991943\n",
      "Epoch 240, Loss: 2.3283910751342773\n",
      "Epoch 260, Loss: 2.4536778926849365\n",
      "Epoch 280, Loss: 2.571155548095703\n",
      "Epoch 300, Loss: 2.680591583251953\n",
      "Epoch 320, Loss: 2.7819433212280273\n",
      "Epoch 340, Loss: 2.8753135204315186\n",
      "Epoch 360, Loss: 2.960909605026245\n",
      "Epoch 380, Loss: 3.0390210151672363\n",
      "Epoch 400, Loss: 3.1099905967712402\n",
      "Epoch 420, Loss: 3.1742022037506104\n",
      "Epoch 440, Loss: 3.2320616245269775\n",
      "Epoch 460, Loss: 3.283989667892456\n",
      "Epoch 480, Loss: 3.3304126262664795\n",
      "weights of  P046 :  [[array([[-0.678156  , -0.24720228],\n",
      "       [-0.32777572,  0.5992097 ],\n",
      "       [ 0.9379041 , -0.02618322],\n",
      "       [ 0.5920546 ,  0.13998272],\n",
      "       [ 0.5446931 , -0.74974996]], dtype=float32), array([ 0.0023053 , -0.00230527], dtype=float32)]]\n",
      "Lowest loss of  P046 :  0.9783145\n",
      "Epoch 0, Loss: 2.682314872741699\n",
      "Epoch 20, Loss: 1.1227917671203613\n",
      "Epoch 40, Loss: 1.0275535583496094\n",
      "Epoch 60, Loss: 1.007145643234253\n",
      "Epoch 80, Loss: 1.004404902458191\n",
      "Epoch 100, Loss: 1.0039219856262207\n",
      "Epoch 120, Loss: 1.0032978057861328\n",
      "Epoch 140, Loss: 1.0027186870574951\n",
      "Epoch 160, Loss: 1.0021365880966187\n",
      "Epoch 180, Loss: 1.0015615224838257\n",
      "Epoch 200, Loss: 1.001001238822937\n",
      "Epoch 220, Loss: 1.0004626512527466\n",
      "Epoch 240, Loss: 0.9999520778656006\n",
      "Epoch 260, Loss: 0.9994741678237915\n",
      "Epoch 280, Loss: 0.9990333318710327\n",
      "Epoch 300, Loss: 0.9986326694488525\n",
      "Epoch 320, Loss: 0.9982747435569763\n",
      "Epoch 340, Loss: 0.9979611039161682\n",
      "Epoch 360, Loss: 0.9976928234100342\n",
      "Epoch 380, Loss: 0.9974701404571533\n",
      "Epoch 400, Loss: 0.9972928762435913\n",
      "Epoch 420, Loss: 0.9971598982810974\n",
      "Epoch 440, Loss: 0.9970699548721313\n",
      "Epoch 460, Loss: 0.9970210790634155\n",
      "Epoch 480, Loss: 0.9970110654830933\n",
      "weights of  P047 :  [[array([[ 0.11075268, -0.09851168],\n",
      "       [-0.8798828 , -0.78043616],\n",
      "       [ 0.40740758,  0.29043192],\n",
      "       [-0.15403116, -1.0175302 ],\n",
      "       [-0.19630101,  0.6204015 ]], dtype=float32), array([ 0.18712017, -0.18712017], dtype=float32)]]\n",
      "Lowest loss of  P047 :  0.9970101\n",
      "Epoch 0, Loss: 1.1434285640716553\n",
      "Epoch 20, Loss: 0.9110516309738159\n",
      "Epoch 40, Loss: 0.9095537066459656\n",
      "Epoch 60, Loss: 0.9151450395584106\n",
      "Epoch 80, Loss: 0.9285060167312622\n",
      "Epoch 100, Loss: 0.9463956952095032\n",
      "Epoch 120, Loss: 0.969910204410553\n",
      "Epoch 140, Loss: 0.998092532157898\n",
      "Epoch 160, Loss: 1.0304365158081055\n",
      "Epoch 180, Loss: 1.0662875175476074\n",
      "Epoch 200, Loss: 1.1049561500549316\n",
      "Epoch 220, Loss: 1.145812749862671\n",
      "Epoch 240, Loss: 1.188281536102295\n",
      "Epoch 260, Loss: 1.2318518161773682\n",
      "Epoch 280, Loss: 1.276079773902893\n",
      "Epoch 300, Loss: 1.3205866813659668\n",
      "Epoch 320, Loss: 1.3650524616241455\n",
      "Epoch 340, Loss: 1.409209966659546\n",
      "Epoch 360, Loss: 1.45283842086792\n",
      "Epoch 380, Loss: 1.49575674533844\n",
      "Epoch 400, Loss: 1.5378191471099854\n",
      "Epoch 420, Loss: 1.5789095163345337\n",
      "Epoch 440, Loss: 1.6189361810684204\n",
      "Epoch 460, Loss: 1.6578290462493896\n",
      "Epoch 480, Loss: 1.6955360174179077\n",
      "weights of  P048 :  [[array([[ 0.3810235 ,  0.07389186],\n",
      "       [-0.6466723 , -0.6719657 ],\n",
      "       [ 0.7160307 , -0.56319875],\n",
      "       [-0.81703997,  0.08195871],\n",
      "       [ 0.13093401,  0.45605123]], dtype=float32), array([-0.14196949,  0.1419695 ], dtype=float32)]]\n",
      "Lowest loss of  P048 :  0.902972\n",
      "Epoch 0, Loss: 0.8357959389686584\n",
      "Epoch 20, Loss: 0.8012473583221436\n",
      "Epoch 40, Loss: 0.804320752620697\n",
      "Epoch 60, Loss: 0.8205093741416931\n",
      "Epoch 80, Loss: 0.8459912538528442\n",
      "Epoch 100, Loss: 0.8890848755836487\n",
      "Epoch 120, Loss: 0.9521540403366089\n",
      "Epoch 140, Loss: 1.0320054292678833\n",
      "Epoch 160, Loss: 1.124284267425537\n",
      "Epoch 180, Loss: 1.2252403497695923\n",
      "Epoch 200, Loss: 1.332085132598877\n",
      "Epoch 220, Loss: 1.4427826404571533\n",
      "Epoch 240, Loss: 1.5558358430862427\n",
      "Epoch 260, Loss: 1.6701701879501343\n",
      "Epoch 280, Loss: 1.7850346565246582\n",
      "Epoch 300, Loss: 1.8999087810516357\n",
      "Epoch 320, Loss: 2.014425754547119\n",
      "Epoch 340, Loss: 2.128316879272461\n",
      "Epoch 360, Loss: 2.241370916366577\n",
      "Epoch 380, Loss: 2.3534111976623535\n",
      "Epoch 400, Loss: 2.464277982711792\n",
      "Epoch 420, Loss: 2.5738232135772705\n",
      "Epoch 440, Loss: 2.681901693344116\n",
      "Epoch 460, Loss: 2.7883734703063965\n",
      "Epoch 480, Loss: 2.893104314804077\n",
      "weights of  P049 :  [[array([[ 0.00537529, -0.4611068 ],\n",
      "       [ 0.6042771 ,  0.20468584],\n",
      "       [ 0.75238556, -0.03248253],\n",
      "       [ 0.31890288, -0.04493547],\n",
      "       [ 0.02057846, -0.4444769 ]], dtype=float32), array([-0.00524246,  0.00524247], dtype=float32)]]\n",
      "Lowest loss of  P049 :  0.7990297\n",
      "Epoch 0, Loss: 1.5703593492507935\n",
      "Epoch 20, Loss: 0.9126582145690918\n",
      "Epoch 40, Loss: 0.8618432283401489\n",
      "Epoch 60, Loss: 0.8541313409805298\n",
      "Epoch 80, Loss: 0.8535095453262329\n",
      "Epoch 100, Loss: 0.8528685569763184\n",
      "Epoch 120, Loss: 0.8522185683250427\n",
      "Epoch 140, Loss: 0.8515931367874146\n",
      "Epoch 160, Loss: 0.8510197997093201\n",
      "Epoch 180, Loss: 0.8505104780197144\n",
      "Epoch 200, Loss: 0.8500815629959106\n",
      "Epoch 220, Loss: 0.849750280380249\n",
      "Epoch 240, Loss: 0.8495327830314636\n",
      "Epoch 260, Loss: 0.8494442701339722\n",
      "Epoch 280, Loss: 0.8494987487792969\n",
      "Epoch 300, Loss: 0.8497093319892883\n",
      "Epoch 320, Loss: 0.8500882983207703\n",
      "Epoch 340, Loss: 0.8506467938423157\n",
      "Epoch 360, Loss: 0.8513951301574707\n",
      "Epoch 380, Loss: 0.8523430228233337\n",
      "Epoch 400, Loss: 0.8534990549087524\n",
      "Epoch 420, Loss: 0.8548710942268372\n",
      "Epoch 440, Loss: 0.8564662933349609\n",
      "Epoch 460, Loss: 0.8582910299301147\n",
      "Epoch 480, Loss: 0.860351026058197\n",
      "weights of  P050 :  [[array([[ 0.27242666,  0.19516112],\n",
      "       [-0.10465047, -0.22287866],\n",
      "       [-0.78137255,  0.2714379 ],\n",
      "       [ 0.5912227 ,  0.01835704],\n",
      "       [ 0.6495084 , -0.11190149]], dtype=float32), array([ 0.26034835, -0.26034835], dtype=float32)]]\n",
      "Lowest loss of  P050 :  0.84944296\n",
      "Epoch 0, Loss: 7.116440296173096\n",
      "Epoch 20, Loss: 2.451113224029541\n",
      "Epoch 40, Loss: 1.0568989515304565\n",
      "Epoch 60, Loss: 0.827083170413971\n",
      "Epoch 80, Loss: 0.796889066696167\n",
      "Epoch 100, Loss: 0.7965689301490784\n",
      "Epoch 120, Loss: 0.7966717481613159\n",
      "Epoch 140, Loss: 0.7968351244926453\n",
      "Epoch 160, Loss: 0.7970992922782898\n",
      "Epoch 180, Loss: 0.7973952293395996\n",
      "Epoch 200, Loss: 0.7977200746536255\n",
      "Epoch 220, Loss: 0.7980746030807495\n",
      "Epoch 240, Loss: 0.798460066318512\n",
      "Epoch 260, Loss: 0.7988778948783875\n",
      "Epoch 280, Loss: 0.7993296384811401\n",
      "Epoch 300, Loss: 0.7998163104057312\n",
      "Epoch 320, Loss: 0.8003395199775696\n",
      "Epoch 340, Loss: 0.8009006381034851\n",
      "Epoch 360, Loss: 0.8015010356903076\n",
      "Epoch 380, Loss: 0.8021423816680908\n",
      "Epoch 400, Loss: 0.8028261661529541\n",
      "Epoch 420, Loss: 0.8035539388656616\n",
      "Epoch 440, Loss: 0.8043271899223328\n",
      "Epoch 460, Loss: 0.8051474094390869\n",
      "Epoch 480, Loss: 0.8060164451599121\n",
      "weights of  P051 :  [[array([[-0.14408806,  0.5037133 ],\n",
      "       [-0.43702596, -0.43686923],\n",
      "       [-0.01231249,  0.34978142],\n",
      "       [ 0.50530714, -0.13400555],\n",
      "       [-0.15452617, -0.3559783 ]], dtype=float32), array([-0.29696915,  0.29696915], dtype=float32)]]\n",
      "Lowest loss of  P051 :  0.79605377\n",
      "Epoch 0, Loss: 1.3955471515655518\n",
      "Epoch 20, Loss: 0.9729336500167847\n",
      "Epoch 40, Loss: 0.8777008652687073\n",
      "Epoch 60, Loss: 0.8806697726249695\n",
      "Epoch 80, Loss: 0.9004491567611694\n",
      "Epoch 100, Loss: 0.9208468794822693\n",
      "Epoch 120, Loss: 0.9437347650527954\n",
      "Epoch 140, Loss: 0.9682685136795044\n",
      "Epoch 160, Loss: 0.9927738904953003\n",
      "Epoch 180, Loss: 1.0168209075927734\n",
      "Epoch 200, Loss: 1.040063738822937\n",
      "Epoch 220, Loss: 1.0621676445007324\n",
      "Epoch 240, Loss: 1.0829877853393555\n",
      "Epoch 260, Loss: 1.1024878025054932\n",
      "Epoch 280, Loss: 1.1207112073898315\n",
      "Epoch 300, Loss: 1.137760877609253\n",
      "Epoch 320, Loss: 1.1537811756134033\n",
      "Epoch 340, Loss: 1.1689414978027344\n",
      "Epoch 360, Loss: 1.1834217309951782\n",
      "Epoch 380, Loss: 1.1974023580551147\n",
      "Epoch 400, Loss: 1.2110549211502075\n",
      "Epoch 420, Loss: 1.224536418914795\n",
      "Epoch 440, Loss: 1.2379841804504395\n",
      "Epoch 460, Loss: 1.251515507698059\n",
      "Epoch 480, Loss: 1.2652242183685303\n",
      "weights of  P052 :  [[array([[ 0.64581746, -0.60588783],\n",
      "       [-0.07643942, -0.5051816 ],\n",
      "       [-0.04769002,  0.5835416 ],\n",
      "       [ 0.6882667 ,  0.7566137 ],\n",
      "       [-0.18108004, -0.04347951]], dtype=float32), array([ 0.32361114, -0.32361117], dtype=float32)]]\n",
      "Lowest loss of  P052 :  0.8744363\n",
      "Epoch 0, Loss: 1.0326764583587646\n",
      "Epoch 20, Loss: 0.8891948461532593\n",
      "Epoch 40, Loss: 0.8887597918510437\n",
      "Epoch 60, Loss: 0.913974404335022\n",
      "Epoch 80, Loss: 0.9483704566955566\n",
      "Epoch 100, Loss: 0.9937949180603027\n",
      "Epoch 120, Loss: 1.0467066764831543\n",
      "Epoch 140, Loss: 1.1060822010040283\n",
      "Epoch 160, Loss: 1.1702885627746582\n",
      "Epoch 180, Loss: 1.2382094860076904\n",
      "Epoch 200, Loss: 1.3090603351593018\n",
      "Epoch 220, Loss: 1.382286787033081\n",
      "Epoch 240, Loss: 1.45755934715271\n",
      "Epoch 260, Loss: 1.5347230434417725\n",
      "Epoch 280, Loss: 1.613757610321045\n",
      "Epoch 300, Loss: 1.69474196434021\n",
      "Epoch 320, Loss: 1.7778265476226807\n",
      "Epoch 340, Loss: 1.863206386566162\n",
      "Epoch 360, Loss: 1.9511009454727173\n",
      "Epoch 380, Loss: 2.041736364364624\n",
      "Epoch 400, Loss: 2.135333299636841\n",
      "Epoch 420, Loss: 2.2320942878723145\n",
      "Epoch 440, Loss: 2.332198143005371\n",
      "Epoch 460, Loss: 2.435793876647949\n",
      "Epoch 480, Loss: 2.542999267578125\n",
      "weights of  P053 :  [[array([[-0.13638794,  0.22667024],\n",
      "       [ 0.6818404 , -0.05700257],\n",
      "       [-0.61454403, -0.14488539],\n",
      "       [ 0.31600058,  0.6465084 ],\n",
      "       [ 0.30411282,  0.817701  ]], dtype=float32), array([ 0.12522636, -0.12522636], dtype=float32)]]\n",
      "Lowest loss of  P053 :  0.87435603\n",
      "Epoch 0, Loss: 0.807091236114502\n",
      "Epoch 20, Loss: 0.8197914958000183\n",
      "Epoch 40, Loss: 0.8740766048431396\n",
      "Epoch 60, Loss: 0.9611507654190063\n",
      "Epoch 80, Loss: 1.0599017143249512\n",
      "Epoch 100, Loss: 1.1564263105392456\n",
      "Epoch 120, Loss: 1.2498120069503784\n",
      "Epoch 140, Loss: 1.3407483100891113\n",
      "Epoch 160, Loss: 1.4300020933151245\n",
      "Epoch 180, Loss: 1.517812728881836\n",
      "Epoch 200, Loss: 1.6042475700378418\n",
      "Epoch 220, Loss: 1.689395785331726\n",
      "Epoch 240, Loss: 1.77336585521698\n",
      "Epoch 260, Loss: 1.856276035308838\n",
      "Epoch 280, Loss: 1.9382164478302002\n",
      "Epoch 300, Loss: 2.0192248821258545\n",
      "Epoch 320, Loss: 2.0992770195007324\n",
      "Epoch 340, Loss: 2.1782891750335693\n",
      "Epoch 360, Loss: 2.256131887435913\n",
      "Epoch 380, Loss: 2.332645893096924\n",
      "Epoch 400, Loss: 2.4076552391052246\n",
      "Epoch 420, Loss: 2.480980396270752\n",
      "Epoch 440, Loss: 2.552448034286499\n",
      "Epoch 460, Loss: 2.6218957901000977\n",
      "Epoch 480, Loss: 2.689181327819824\n",
      "weights of  P054 :  [[array([[-0.4543095 ,  0.3606565 ],\n",
      "       [-0.8962672 ,  0.878172  ],\n",
      "       [ 0.13685204,  0.30569062],\n",
      "       [-0.3539008 ,  0.5854891 ],\n",
      "       [-0.4328133 , -0.3627185 ]], dtype=float32), array([ 0.0777628, -0.0777628], dtype=float32)]]\n",
      "Lowest loss of  P054 :  0.79451036\n",
      "Epoch 0, Loss: 2.7370777130126953\n",
      "Epoch 20, Loss: 1.203522801399231\n",
      "Epoch 40, Loss: 1.058892011642456\n",
      "Epoch 60, Loss: 1.03742516040802\n",
      "Epoch 80, Loss: 1.0317574739456177\n",
      "Epoch 100, Loss: 1.0247482061386108\n",
      "Epoch 120, Loss: 1.0175930261611938\n",
      "Epoch 140, Loss: 1.0100536346435547\n",
      "Epoch 160, Loss: 1.0022344589233398\n",
      "Epoch 180, Loss: 0.9942278861999512\n",
      "Epoch 200, Loss: 0.9861181974411011\n",
      "Epoch 220, Loss: 0.9779830574989319\n",
      "Epoch 240, Loss: 0.9698953628540039\n",
      "Epoch 260, Loss: 0.9619224071502686\n",
      "Epoch 280, Loss: 0.9541276693344116\n",
      "Epoch 300, Loss: 0.9465705156326294\n",
      "Epoch 320, Loss: 0.9393061399459839\n",
      "Epoch 340, Loss: 0.9323865175247192\n",
      "Epoch 360, Loss: 0.9258597493171692\n",
      "Epoch 380, Loss: 0.9197705388069153\n",
      "Epoch 400, Loss: 0.9141603112220764\n",
      "Epoch 420, Loss: 0.9090671539306641\n",
      "Epoch 440, Loss: 0.9045261144638062\n",
      "Epoch 460, Loss: 0.900568962097168\n",
      "Epoch 480, Loss: 0.8972241282463074\n",
      "weights of  P055 :  [[array([[-0.02553141,  0.17993356],\n",
      "       [-0.38418323,  0.01756962],\n",
      "       [ 0.6325819 ,  0.50975174],\n",
      "       [ 0.32197323,  0.10586219],\n",
      "       [-0.5598998 , -0.9431436 ]], dtype=float32), array([ 0.00083847, -0.00083853], dtype=float32)]]\n",
      "Lowest loss of  P055 :  0.8946373\n",
      "Epoch 0, Loss: 1.124550223350525\n",
      "Epoch 20, Loss: 1.014756679534912\n",
      "Epoch 40, Loss: 1.0674912929534912\n",
      "Epoch 60, Loss: 1.15604567527771\n",
      "Epoch 80, Loss: 1.2629024982452393\n",
      "Epoch 100, Loss: 1.3922622203826904\n",
      "Epoch 120, Loss: 1.5344232320785522\n",
      "Epoch 140, Loss: 1.6858848333358765\n",
      "Epoch 160, Loss: 1.842071771621704\n",
      "Epoch 180, Loss: 2.0003273487091064\n",
      "Epoch 200, Loss: 2.1586313247680664\n",
      "Epoch 220, Loss: 2.3157641887664795\n",
      "Epoch 240, Loss: 2.470954179763794\n",
      "Epoch 260, Loss: 2.6237664222717285\n",
      "Epoch 280, Loss: 2.773963451385498\n",
      "Epoch 300, Loss: 2.921419143676758\n",
      "Epoch 320, Loss: 3.066053867340088\n",
      "Epoch 340, Loss: 3.2077994346618652\n",
      "Epoch 360, Loss: 3.34657621383667\n",
      "Epoch 380, Loss: 3.4822847843170166\n",
      "Epoch 400, Loss: 3.6148030757904053\n",
      "Epoch 420, Loss: 3.7439937591552734\n",
      "Epoch 440, Loss: 3.8697080612182617\n",
      "Epoch 460, Loss: 3.9917914867401123\n",
      "Epoch 480, Loss: 4.110095977783203\n",
      "weights of  P056 :  [[array([[ 1.0933715 , -0.7469188 ],\n",
      "       [-0.73206854,  0.37020487],\n",
      "       [-0.20926076,  0.38365728],\n",
      "       [-0.75355893,  0.3696596 ],\n",
      "       [ 0.7155293 , -0.06092473]], dtype=float32), array([-0.17733867,  0.17733869], dtype=float32)]]\n",
      "Lowest loss of  P056 :  1.0144191\n",
      "Epoch 0, Loss: 1.2334041595458984\n",
      "Epoch 20, Loss: 0.9934307336807251\n",
      "Epoch 40, Loss: 0.9582917094230652\n",
      "Epoch 60, Loss: 0.9201239943504333\n",
      "Epoch 80, Loss: 0.8962053060531616\n",
      "Epoch 100, Loss: 0.8798675537109375\n",
      "Epoch 120, Loss: 0.8701767325401306\n",
      "Epoch 140, Loss: 0.8652024865150452\n",
      "Epoch 160, Loss: 0.8637139201164246\n",
      "Epoch 180, Loss: 0.8649317622184753\n",
      "Epoch 200, Loss: 0.8684407472610474\n",
      "Epoch 220, Loss: 0.8740583658218384\n",
      "Epoch 240, Loss: 0.8817163109779358\n",
      "Epoch 260, Loss: 0.8913888931274414\n",
      "Epoch 280, Loss: 0.903049111366272\n",
      "Epoch 300, Loss: 0.916649341583252\n",
      "Epoch 320, Loss: 0.9321166276931763\n",
      "Epoch 340, Loss: 0.9493546485900879\n",
      "Epoch 360, Loss: 0.9682490825653076\n",
      "Epoch 380, Loss: 0.9886735677719116\n",
      "Epoch 400, Loss: 1.0104939937591553\n",
      "Epoch 420, Loss: 1.0335733890533447\n",
      "Epoch 440, Loss: 1.0577733516693115\n",
      "Epoch 460, Loss: 1.0829577445983887\n",
      "Epoch 480, Loss: 1.1089918613433838\n",
      "weights of  P057 :  [[array([[-0.06184629, -0.6482088 ],\n",
      "       [ 0.12863992, -0.10239434],\n",
      "       [ 0.51155615,  0.9063783 ],\n",
      "       [ 0.06757386, -0.16210085],\n",
      "       [-0.08995171,  0.40704113]], dtype=float32), array([ 0.08370573, -0.08370568], dtype=float32)]]\n",
      "Lowest loss of  P057 :  0.8637139\n",
      "Epoch 0, Loss: 2.206749439239502\n",
      "Epoch 20, Loss: 1.147068738937378\n",
      "Epoch 40, Loss: 1.1404974460601807\n",
      "Epoch 60, Loss: 1.101230263710022\n",
      "Epoch 80, Loss: 1.0833075046539307\n",
      "Epoch 100, Loss: 1.064320683479309\n",
      "Epoch 120, Loss: 1.0456711053848267\n",
      "Epoch 140, Loss: 1.0275559425354004\n",
      "Epoch 160, Loss: 1.0103460550308228\n",
      "Epoch 180, Loss: 0.9942873120307922\n",
      "Epoch 200, Loss: 0.9795343279838562\n",
      "Epoch 220, Loss: 0.9661545753479004\n",
      "Epoch 240, Loss: 0.9541680812835693\n",
      "Epoch 260, Loss: 0.9435489177703857\n",
      "Epoch 280, Loss: 0.9342459440231323\n",
      "Epoch 300, Loss: 0.9261915683746338\n",
      "Epoch 320, Loss: 0.9193106889724731\n",
      "Epoch 340, Loss: 0.9135274887084961\n",
      "Epoch 360, Loss: 0.9087697267532349\n",
      "Epoch 380, Loss: 0.9049713611602783\n",
      "Epoch 400, Loss: 0.9020742774009705\n",
      "Epoch 420, Loss: 0.9000287055969238\n",
      "Epoch 440, Loss: 0.8987933993339539\n",
      "Epoch 460, Loss: 0.8983341455459595\n",
      "Epoch 480, Loss: 0.8986239433288574\n",
      "weights of  P058 :  [[array([[ 0.3669379 , -0.5739314 ],\n",
      "       [-0.31482852,  0.8706981 ],\n",
      "       [ 0.3743925 , -0.30084828],\n",
      "       [ 0.3573123 ,  0.35696855],\n",
      "       [ 0.20758384,  0.671725  ]], dtype=float32), array([ 0.0946533 , -0.09465332], dtype=float32)]]\n",
      "Lowest loss of  P058 :  0.89832985\n",
      "Epoch 0, Loss: 1.2550090551376343\n",
      "Epoch 20, Loss: 1.0657856464385986\n",
      "Epoch 40, Loss: 0.9569822549819946\n",
      "Epoch 60, Loss: 0.889647364616394\n",
      "Epoch 80, Loss: 0.8480769991874695\n",
      "Epoch 100, Loss: 0.8216039538383484\n",
      "Epoch 120, Loss: 0.805084764957428\n",
      "Epoch 140, Loss: 0.7949981689453125\n",
      "Epoch 160, Loss: 0.7896894812583923\n",
      "Epoch 180, Loss: 0.7883397340774536\n",
      "Epoch 200, Loss: 0.7903376817703247\n",
      "Epoch 220, Loss: 0.7951982021331787\n",
      "Epoch 240, Loss: 0.8025922775268555\n",
      "Epoch 260, Loss: 0.812339723110199\n",
      "Epoch 280, Loss: 0.8243559002876282\n",
      "Epoch 300, Loss: 0.838604748249054\n",
      "Epoch 320, Loss: 0.8550705313682556\n",
      "Epoch 340, Loss: 0.8737406730651855\n",
      "Epoch 360, Loss: 0.8945945501327515\n",
      "Epoch 380, Loss: 0.9176005125045776\n",
      "Epoch 400, Loss: 0.9427131414413452\n",
      "Epoch 420, Loss: 0.9698747396469116\n",
      "Epoch 440, Loss: 0.9990180134773254\n",
      "Epoch 460, Loss: 1.0300670862197876\n",
      "Epoch 480, Loss: 1.0629403591156006\n",
      "weights of  P059 :  [[array([[-0.06371101,  0.69080347],\n",
      "       [ 0.5327656 ,  0.5139815 ],\n",
      "       [ 0.13008487, -0.06349376],\n",
      "       [ 0.14152512, -0.0682893 ],\n",
      "       [-0.04134411, -0.11857467]], dtype=float32), array([ 0.19573852, -0.19573854], dtype=float32)]]\n",
      "Lowest loss of  P059 :  0.7883151\n",
      "Epoch 0, Loss: 1.0173197984695435\n",
      "Epoch 20, Loss: 0.9017475843429565\n",
      "Epoch 40, Loss: 0.8956930637359619\n",
      "Epoch 60, Loss: 0.9481605291366577\n",
      "Epoch 80, Loss: 1.019812822341919\n",
      "Epoch 100, Loss: 1.10439932346344\n",
      "Epoch 120, Loss: 1.2030606269836426\n",
      "Epoch 140, Loss: 1.314814805984497\n",
      "Epoch 160, Loss: 1.4377777576446533\n",
      "Epoch 180, Loss: 1.5703977346420288\n",
      "Epoch 200, Loss: 1.7115203142166138\n",
      "Epoch 220, Loss: 1.8601616621017456\n",
      "Epoch 240, Loss: 2.0154359340667725\n",
      "Epoch 260, Loss: 2.176567554473877\n",
      "Epoch 280, Loss: 2.342891216278076\n",
      "Epoch 300, Loss: 2.5138397216796875\n",
      "Epoch 320, Loss: 2.6889286041259766\n",
      "Epoch 340, Loss: 2.8677470684051514\n",
      "Epoch 360, Loss: 3.0499420166015625\n",
      "Epoch 380, Loss: 3.235212802886963\n",
      "Epoch 400, Loss: 3.4233012199401855\n",
      "Epoch 420, Loss: 3.613983154296875\n",
      "Epoch 440, Loss: 3.807065010070801\n",
      "Epoch 460, Loss: 4.002374172210693\n",
      "Epoch 480, Loss: 4.199760437011719\n",
      "weights of  P060 :  [[array([[ 1.0705724 , -0.5633245 ],\n",
      "       [-0.39812303, -0.3194931 ],\n",
      "       [-0.34605062,  0.819806  ],\n",
      "       [ 0.62059367, -0.29186833],\n",
      "       [-0.39477575, -0.6184144 ]], dtype=float32), array([-0.0284764 ,  0.02847635], dtype=float32)]]\n",
      "Lowest loss of  P060 :  0.88751125\n",
      "Epoch 0, Loss: 0.8904542326927185\n",
      "Epoch 20, Loss: 0.8371707797050476\n",
      "Epoch 40, Loss: 0.8525937795639038\n",
      "Epoch 60, Loss: 0.9223524332046509\n",
      "Epoch 80, Loss: 1.0342650413513184\n",
      "Epoch 100, Loss: 1.178430199623108\n",
      "Epoch 120, Loss: 1.3467433452606201\n",
      "Epoch 140, Loss: 1.532222032546997\n",
      "Epoch 160, Loss: 1.7288569211959839\n",
      "Epoch 180, Loss: 1.9317288398742676\n",
      "Epoch 200, Loss: 2.1370489597320557\n",
      "Epoch 220, Loss: 2.342068672180176\n",
      "Epoch 240, Loss: 2.5449001789093018\n",
      "Epoch 260, Loss: 2.744318962097168\n",
      "Epoch 280, Loss: 2.9395768642425537\n",
      "Epoch 300, Loss: 3.1302592754364014\n",
      "Epoch 320, Loss: 3.3161721229553223\n",
      "Epoch 340, Loss: 3.4972593784332275\n",
      "Epoch 360, Loss: 3.6735479831695557\n",
      "Epoch 380, Loss: 3.8451051712036133\n",
      "Epoch 400, Loss: 4.012015342712402\n",
      "Epoch 420, Loss: 4.174361228942871\n",
      "Epoch 440, Loss: 4.332215785980225\n",
      "Epoch 460, Loss: 4.485637664794922\n",
      "Epoch 480, Loss: 4.634669780731201\n",
      "weights of  P061 :  [[array([[ 0.3171292 , -0.0824191 ],\n",
      "       [-0.01233193,  0.6997461 ],\n",
      "       [ 0.74898136,  0.68370825],\n",
      "       [ 0.13685246,  0.13156715],\n",
      "       [ 0.1798878 , -0.20389424]], dtype=float32), array([-0.18347593,  0.18347588], dtype=float32)]]\n",
      "Lowest loss of  P061 :  0.8352331\n",
      "Epoch 0, Loss: 1.1875684261322021\n",
      "Epoch 20, Loss: 1.046929121017456\n",
      "Epoch 40, Loss: 0.9776101112365723\n",
      "Epoch 60, Loss: 0.9316007494926453\n",
      "Epoch 80, Loss: 0.9104012250900269\n",
      "Epoch 100, Loss: 0.9128484725952148\n",
      "Epoch 120, Loss: 0.9356580972671509\n",
      "Epoch 140, Loss: 0.9751521348953247\n",
      "Epoch 160, Loss: 1.0279803276062012\n",
      "Epoch 180, Loss: 1.0913468599319458\n",
      "Epoch 200, Loss: 1.1630194187164307\n",
      "Epoch 220, Loss: 1.2412526607513428\n",
      "Epoch 240, Loss: 1.3246961832046509\n",
      "Epoch 260, Loss: 1.412306547164917\n",
      "Epoch 280, Loss: 1.5032778978347778\n",
      "Epoch 300, Loss: 1.596985101699829\n",
      "Epoch 320, Loss: 1.6929419040679932\n",
      "Epoch 340, Loss: 1.790767788887024\n",
      "Epoch 360, Loss: 1.8901655673980713\n",
      "Epoch 380, Loss: 1.9909018278121948\n",
      "Epoch 400, Loss: 2.0927932262420654\n",
      "Epoch 420, Loss: 2.1956961154937744\n",
      "Epoch 440, Loss: 2.2994983196258545\n",
      "Epoch 460, Loss: 2.404113531112671\n",
      "Epoch 480, Loss: 2.509476900100708\n",
      "weights of  P062 :  [[array([[ 1.0567106 ,  0.39881012],\n",
      "       [-0.6538117 ,  0.3788407 ],\n",
      "       [ 0.51574403,  0.60043204],\n",
      "       [-0.11603455, -0.58064014],\n",
      "       [-0.47526118, -1.0502658 ]], dtype=float32), array([ 0.17420153, -0.17420147], dtype=float32)]]\n",
      "Lowest loss of  P062 :  0.90875405\n",
      "Epoch 0, Loss: 1.0893269777297974\n",
      "Epoch 20, Loss: 1.0470693111419678\n",
      "Epoch 40, Loss: 1.0312583446502686\n",
      "Epoch 60, Loss: 1.0214245319366455\n",
      "Epoch 80, Loss: 1.0152170658111572\n",
      "Epoch 100, Loss: 1.0098357200622559\n",
      "Epoch 120, Loss: 1.003557562828064\n",
      "Epoch 140, Loss: 0.9960001707077026\n",
      "Epoch 160, Loss: 0.9876541495323181\n",
      "Epoch 180, Loss: 0.9791757464408875\n",
      "Epoch 200, Loss: 0.9710812568664551\n",
      "Epoch 220, Loss: 0.9636541604995728\n",
      "Epoch 240, Loss: 0.957005500793457\n",
      "Epoch 260, Loss: 0.9511536359786987\n",
      "Epoch 280, Loss: 0.9460760951042175\n",
      "Epoch 300, Loss: 0.9417327642440796\n",
      "Epoch 320, Loss: 0.9380753040313721\n",
      "Epoch 340, Loss: 0.9350499510765076\n",
      "Epoch 360, Loss: 0.9326003789901733\n",
      "Epoch 380, Loss: 0.9306697845458984\n",
      "Epoch 400, Loss: 0.9292024970054626\n",
      "Epoch 420, Loss: 0.9281450510025024\n",
      "Epoch 440, Loss: 0.9274468421936035\n",
      "Epoch 460, Loss: 0.9270609617233276\n",
      "Epoch 480, Loss: 0.9269444942474365\n",
      "weights of  P063 :  [[array([[ 6.2786251e-01,  7.2023988e-01],\n",
      "       [ 6.6768721e-02,  6.8566298e-01],\n",
      "       [ 2.0369779e-01,  5.8707124e-01],\n",
      "       [-2.6487800e-01,  6.8864459e-04],\n",
      "       [ 4.9918237e-01, -7.8166097e-01]], dtype=float32), array([-0.7947568 ,  0.79475695], dtype=float32)]]\n",
      "Lowest loss of  P063 :  0.9269445\n",
      "Epoch 0, Loss: 1.1048271656036377\n",
      "Epoch 20, Loss: 0.9339998960494995\n",
      "Epoch 40, Loss: 0.9154905080795288\n",
      "Epoch 60, Loss: 0.9013894200325012\n",
      "Epoch 80, Loss: 0.8870398998260498\n",
      "Epoch 100, Loss: 0.8720253705978394\n",
      "Epoch 120, Loss: 0.8568910956382751\n",
      "Epoch 140, Loss: 0.8420945405960083\n",
      "Epoch 160, Loss: 0.8280224204063416\n",
      "Epoch 180, Loss: 0.8149824142456055\n",
      "Epoch 200, Loss: 0.8032296299934387\n",
      "Epoch 220, Loss: 0.792964518070221\n",
      "Epoch 240, Loss: 0.7843407988548279\n",
      "Epoch 260, Loss: 0.7774705290794373\n",
      "Epoch 280, Loss: 0.7724288702011108\n",
      "Epoch 300, Loss: 0.769258975982666\n",
      "Epoch 320, Loss: 0.7679765224456787\n",
      "Epoch 340, Loss: 0.7685743570327759\n",
      "Epoch 360, Loss: 0.7710262537002563\n",
      "Epoch 380, Loss: 0.7752911448478699\n",
      "Epoch 400, Loss: 0.7813160419464111\n",
      "Epoch 420, Loss: 0.7890395522117615\n",
      "Epoch 440, Loss: 0.7983944416046143\n",
      "Epoch 460, Loss: 0.8093093633651733\n",
      "Epoch 480, Loss: 0.8217117190361023\n",
      "weights of  P064 :  [[array([[ 0.31120092, -0.20961383],\n",
      "       [-0.18516181,  0.6213931 ],\n",
      "       [ 0.08711229, -0.5253308 ],\n",
      "       [ 0.19204187, -0.06533469],\n",
      "       [-0.17681104,  0.05755117]], dtype=float32), array([ 0.17302184, -0.17302184], dtype=float32)]]\n",
      "Lowest loss of  P064 :  0.76794624\n",
      "Epoch 0, Loss: 0.9917142987251282\n",
      "Epoch 20, Loss: 0.914938747882843\n",
      "Epoch 40, Loss: 0.8674407005310059\n",
      "Epoch 60, Loss: 0.8467718958854675\n",
      "Epoch 80, Loss: 0.849897563457489\n",
      "Epoch 100, Loss: 0.8719890117645264\n",
      "Epoch 120, Loss: 0.9095290899276733\n",
      "Epoch 140, Loss: 0.960417628288269\n",
      "Epoch 160, Loss: 1.0231518745422363\n",
      "Epoch 180, Loss: 1.0962936878204346\n",
      "Epoch 200, Loss: 1.1783199310302734\n",
      "Epoch 220, Loss: 1.2676509618759155\n",
      "Epoch 240, Loss: 1.362729549407959\n",
      "Epoch 260, Loss: 1.4620864391326904\n",
      "Epoch 280, Loss: 1.5643796920776367\n",
      "Epoch 300, Loss: 1.6684134006500244\n",
      "Epoch 320, Loss: 1.7731373310089111\n",
      "Epoch 340, Loss: 1.8776419162750244\n",
      "Epoch 360, Loss: 1.9811487197875977\n",
      "Epoch 380, Loss: 2.0829977989196777\n",
      "Epoch 400, Loss: 2.1826372146606445\n",
      "Epoch 420, Loss: 2.279611587524414\n",
      "Epoch 440, Loss: 2.3735499382019043\n",
      "Epoch 460, Loss: 2.4641571044921875\n",
      "Epoch 480, Loss: 2.5512044429779053\n",
      "weights of  P065 :  [[array([[-0.16186531, -0.5164616 ],\n",
      "       [-0.5061462 ,  0.27090055],\n",
      "       [ 0.69021195,  0.37952924],\n",
      "       [ 0.31502163,  0.42485029],\n",
      "       [ 0.3585774 ,  0.09824334]], dtype=float32), array([-0.09031474,  0.09031477], dtype=float32)]]\n",
      "Lowest loss of  P065 :  0.84542614\n",
      "Epoch 0, Loss: 1.3184857368469238\n",
      "Epoch 20, Loss: 1.0356309413909912\n",
      "Epoch 40, Loss: 0.9994336366653442\n",
      "Epoch 60, Loss: 0.9763133525848389\n",
      "Epoch 80, Loss: 0.9612592458724976\n",
      "Epoch 100, Loss: 0.9532232284545898\n",
      "Epoch 120, Loss: 0.9519765377044678\n",
      "Epoch 140, Loss: 0.9567646384239197\n",
      "Epoch 160, Loss: 0.9665655493736267\n",
      "Epoch 180, Loss: 0.980278491973877\n",
      "Epoch 200, Loss: 0.9968469142913818\n",
      "Epoch 220, Loss: 1.0153303146362305\n",
      "Epoch 240, Loss: 1.034939169883728\n",
      "Epoch 260, Loss: 1.055042028427124\n",
      "Epoch 280, Loss: 1.075156807899475\n",
      "Epoch 300, Loss: 1.094930648803711\n",
      "Epoch 320, Loss: 1.1141200065612793\n",
      "Epoch 340, Loss: 1.132568120956421\n",
      "Epoch 360, Loss: 1.1501840353012085\n",
      "Epoch 380, Loss: 1.1669267416000366\n",
      "Epoch 400, Loss: 1.1827895641326904\n",
      "Epoch 420, Loss: 1.1977901458740234\n",
      "Epoch 440, Loss: 1.2119590044021606\n",
      "Epoch 460, Loss: 1.2253363132476807\n",
      "Epoch 480, Loss: 1.237964153289795\n",
      "weights of  P066 :  [[array([[-0.88178176, -0.4757862 ],\n",
      "       [-0.6870556 ,  0.11287351],\n",
      "       [-0.01891337, -0.97217566],\n",
      "       [ 0.5863879 ,  0.03589971],\n",
      "       [-0.03501644,  0.33216742]], dtype=float32), array([ 0.03727645, -0.03727644], dtype=float32)]]\n",
      "Lowest loss of  P066 :  0.9516751\n",
      "Epoch 0, Loss: 4.8332200050354\n",
      "Epoch 20, Loss: 1.619703769683838\n",
      "Epoch 40, Loss: 1.1524956226348877\n",
      "Epoch 60, Loss: 1.053934931755066\n",
      "Epoch 80, Loss: 1.0394777059555054\n",
      "Epoch 100, Loss: 1.0368082523345947\n",
      "Epoch 120, Loss: 1.0355887413024902\n",
      "Epoch 140, Loss: 1.0344533920288086\n",
      "Epoch 160, Loss: 1.0332647562026978\n",
      "Epoch 180, Loss: 1.032031536102295\n",
      "Epoch 200, Loss: 1.0307670831680298\n",
      "Epoch 220, Loss: 1.0294792652130127\n",
      "Epoch 240, Loss: 1.0281755924224854\n",
      "Epoch 260, Loss: 1.0268633365631104\n",
      "Epoch 280, Loss: 1.0255495309829712\n",
      "Epoch 300, Loss: 1.0242398977279663\n",
      "Epoch 320, Loss: 1.0229402780532837\n",
      "Epoch 340, Loss: 1.0216560363769531\n",
      "Epoch 360, Loss: 1.0203919410705566\n",
      "Epoch 380, Loss: 1.019152283668518\n",
      "Epoch 400, Loss: 1.0179417133331299\n",
      "Epoch 420, Loss: 1.0167638063430786\n",
      "Epoch 440, Loss: 1.015622615814209\n",
      "Epoch 460, Loss: 1.0145213603973389\n",
      "Epoch 480, Loss: 1.0134631395339966\n",
      "weights of  P067 :  [[array([[-0.7432731 , -0.7369829 ],\n",
      "       [ 0.06933916, -0.39036572],\n",
      "       [ 0.28953233, -0.3411051 ],\n",
      "       [ 0.14592606,  0.6298532 ],\n",
      "       [ 0.57758135,  1.0138036 ]], dtype=float32), array([ 0.32844418, -0.32844415], dtype=float32)]]\n",
      "Lowest loss of  P067 :  1.0125004\n",
      "Epoch 0, Loss: 1.3657491207122803\n",
      "Epoch 20, Loss: 0.9312350749969482\n",
      "Epoch 40, Loss: 0.9367159605026245\n",
      "Epoch 60, Loss: 0.9375114440917969\n",
      "Epoch 80, Loss: 0.9418188333511353\n",
      "Epoch 100, Loss: 0.9484587907791138\n",
      "Epoch 120, Loss: 0.9563336372375488\n",
      "Epoch 140, Loss: 0.9655618667602539\n",
      "Epoch 160, Loss: 0.9760801792144775\n",
      "Epoch 180, Loss: 0.9878801107406616\n",
      "Epoch 200, Loss: 1.0009119510650635\n",
      "Epoch 220, Loss: 1.0151231288909912\n",
      "Epoch 240, Loss: 1.0304462909698486\n",
      "Epoch 260, Loss: 1.0468065738677979\n",
      "Epoch 280, Loss: 1.0641216039657593\n",
      "Epoch 300, Loss: 1.0823040008544922\n",
      "Epoch 320, Loss: 1.1012629270553589\n",
      "Epoch 340, Loss: 1.1209056377410889\n",
      "Epoch 360, Loss: 1.1411395072937012\n",
      "Epoch 380, Loss: 1.1618727445602417\n",
      "Epoch 400, Loss: 1.183016061782837\n",
      "Epoch 420, Loss: 1.204484462738037\n",
      "Epoch 440, Loss: 1.2261955738067627\n",
      "Epoch 460, Loss: 1.2480734586715698\n",
      "Epoch 480, Loss: 1.2700467109680176\n",
      "weights of  P068 :  [[array([[ 0.62728924,  0.6938234 ],\n",
      "       [ 0.10760409,  0.23931424],\n",
      "       [ 0.5891661 , -0.3050213 ],\n",
      "       [-1.0309769 , -0.02444729],\n",
      "       [ 0.12058778, -0.2233788 ]], dtype=float32), array([ 0.19699733, -0.19699733], dtype=float32)]]\n",
      "Lowest loss of  P068 :  0.92970634\n",
      "Epoch 0, Loss: 1.0458487272262573\n",
      "Epoch 20, Loss: 0.9065189361572266\n",
      "Epoch 40, Loss: 0.8965018391609192\n",
      "Epoch 60, Loss: 0.8875971436500549\n",
      "Epoch 80, Loss: 0.880807638168335\n",
      "Epoch 100, Loss: 0.8766821026802063\n",
      "Epoch 120, Loss: 0.8756030201911926\n",
      "Epoch 140, Loss: 0.8776530623435974\n",
      "Epoch 160, Loss: 0.8827487230300903\n",
      "Epoch 180, Loss: 0.8907060623168945\n",
      "Epoch 200, Loss: 0.9012956023216248\n",
      "Epoch 220, Loss: 0.9142794609069824\n",
      "Epoch 240, Loss: 0.9294324517250061\n",
      "Epoch 260, Loss: 0.9465526342391968\n",
      "Epoch 280, Loss: 0.9654650688171387\n",
      "Epoch 300, Loss: 0.9860210418701172\n",
      "Epoch 320, Loss: 1.0080935955047607\n",
      "Epoch 340, Loss: 1.0315747261047363\n",
      "Epoch 360, Loss: 1.056370735168457\n",
      "Epoch 380, Loss: 1.082397222518921\n",
      "Epoch 400, Loss: 1.1095764636993408\n",
      "Epoch 420, Loss: 1.1378345489501953\n",
      "Epoch 440, Loss: 1.1670989990234375\n",
      "Epoch 460, Loss: 1.1972978115081787\n",
      "Epoch 480, Loss: 1.2283580303192139\n",
      "weights of  P069 :  [[array([[ 0.1666834 , -0.31867835],\n",
      "       [-0.33160934,  0.5911632 ],\n",
      "       [-0.66522205, -0.7637407 ],\n",
      "       [ 0.17000183, -0.32037345],\n",
      "       [-0.3937485 , -0.2735713 ]], dtype=float32), array([-0.20079117,  0.20079115], dtype=float32)]]\n",
      "Lowest loss of  P069 :  0.8755644\n",
      "Epoch 0, Loss: 1.0779528617858887\n",
      "Epoch 20, Loss: 0.9374975562095642\n",
      "Epoch 40, Loss: 0.9695462584495544\n",
      "Epoch 60, Loss: 1.0002031326293945\n",
      "Epoch 80, Loss: 1.046425223350525\n",
      "Epoch 100, Loss: 1.106501817703247\n",
      "Epoch 120, Loss: 1.1796152591705322\n",
      "Epoch 140, Loss: 1.261763095855713\n",
      "Epoch 160, Loss: 1.3502082824707031\n",
      "Epoch 180, Loss: 1.4425448179244995\n",
      "Epoch 200, Loss: 1.5367681980133057\n",
      "Epoch 220, Loss: 1.6312775611877441\n",
      "Epoch 240, Loss: 1.7248615026474\n",
      "Epoch 260, Loss: 1.8166168928146362\n",
      "Epoch 280, Loss: 1.9058825969696045\n",
      "Epoch 300, Loss: 1.992191195487976\n",
      "Epoch 320, Loss: 2.075226306915283\n",
      "Epoch 340, Loss: 2.154789447784424\n",
      "Epoch 360, Loss: 2.2307746410369873\n",
      "Epoch 380, Loss: 2.303145170211792\n",
      "Epoch 400, Loss: 2.371919631958008\n",
      "Epoch 420, Loss: 2.4371559619903564\n",
      "Epoch 440, Loss: 2.4989423751831055\n",
      "Epoch 460, Loss: 2.5573887825012207\n",
      "Epoch 480, Loss: 2.612619400024414\n",
      "weights of  P070 :  [[array([[ 0.623234  , -0.6627403 ],\n",
      "       [-0.7531096 , -0.5432225 ],\n",
      "       [ 0.04212425,  0.19399105],\n",
      "       [-0.20985293,  0.01789915],\n",
      "       [ 0.7380896 ,  0.58187896]], dtype=float32), array([ 0.1291153, -0.1291153], dtype=float32)]]\n",
      "Lowest loss of  P070 :  0.9341387\n",
      "Epoch 0, Loss: 1.2242964506149292\n",
      "Epoch 20, Loss: 0.9694051742553711\n",
      "Epoch 40, Loss: 0.9568495750427246\n",
      "Epoch 60, Loss: 0.9681096076965332\n",
      "Epoch 80, Loss: 0.9849231839179993\n",
      "Epoch 100, Loss: 1.0104809999465942\n",
      "Epoch 120, Loss: 1.0435247421264648\n",
      "Epoch 140, Loss: 1.0849058628082275\n",
      "Epoch 160, Loss: 1.1341767311096191\n",
      "Epoch 180, Loss: 1.191246747970581\n",
      "Epoch 200, Loss: 1.255831241607666\n",
      "Epoch 220, Loss: 1.3275734186172485\n",
      "Epoch 240, Loss: 1.4060795307159424\n",
      "Epoch 260, Loss: 1.4909138679504395\n",
      "Epoch 280, Loss: 1.5816221237182617\n",
      "Epoch 300, Loss: 1.6777454614639282\n",
      "Epoch 320, Loss: 1.778832197189331\n",
      "Epoch 340, Loss: 1.8844457864761353\n",
      "Epoch 360, Loss: 1.9941731691360474\n",
      "Epoch 380, Loss: 2.1076269149780273\n",
      "Epoch 400, Loss: 2.2244482040405273\n",
      "Epoch 420, Loss: 2.3443069458007812\n",
      "Epoch 440, Loss: 2.4668996334075928\n",
      "Epoch 460, Loss: 2.591951608657837\n",
      "Epoch 480, Loss: 2.719210624694824\n",
      "weights of  P071 :  [[array([[ 0.8725417 ,  0.4605223 ],\n",
      "       [-0.1042191 ,  0.68955505],\n",
      "       [ 0.4748259 ,  0.18409878],\n",
      "       [ 0.93906873,  0.05946507],\n",
      "       [-0.48116982, -0.79982615]], dtype=float32), array([-0.13250218,  0.13250218], dtype=float32)]]\n",
      "Lowest loss of  P071 :  0.9522848\n",
      "Epoch 0, Loss: 5.074901580810547\n",
      "Epoch 20, Loss: 0.9547809362411499\n",
      "Epoch 40, Loss: 0.7358332872390747\n",
      "Epoch 60, Loss: 0.6560445427894592\n",
      "Epoch 80, Loss: 0.6436830163002014\n",
      "Epoch 100, Loss: 0.6430417895317078\n",
      "Epoch 120, Loss: 0.6439273953437805\n",
      "Epoch 140, Loss: 0.645526647567749\n",
      "Epoch 160, Loss: 0.6478565335273743\n",
      "Epoch 180, Loss: 0.6509390473365784\n",
      "Epoch 200, Loss: 0.6548268795013428\n",
      "Epoch 220, Loss: 0.6595389246940613\n",
      "Epoch 240, Loss: 0.6650829315185547\n",
      "Epoch 260, Loss: 0.6714584827423096\n",
      "Epoch 280, Loss: 0.678655743598938\n",
      "Epoch 300, Loss: 0.686657726764679\n",
      "Epoch 320, Loss: 0.6954423189163208\n",
      "Epoch 340, Loss: 0.7049833536148071\n",
      "Epoch 360, Loss: 0.7152516841888428\n",
      "Epoch 380, Loss: 0.726216197013855\n",
      "Epoch 400, Loss: 0.7378451228141785\n",
      "Epoch 420, Loss: 0.7501053214073181\n",
      "Epoch 440, Loss: 0.7629644870758057\n",
      "Epoch 460, Loss: 0.776390552520752\n",
      "Epoch 480, Loss: 0.7903522253036499\n",
      "weights of  P072 :  [[array([[-0.01348626, -0.30080312],\n",
      "       [-0.33249116,  1.1624671 ],\n",
      "       [ 0.21874137, -0.30250365],\n",
      "       [ 0.47785896,  0.25816953],\n",
      "       [ 0.37991518, -0.24110387]], dtype=float32), array([ 0.23102023, -0.23102023], dtype=float32)]]\n",
      "Lowest loss of  P072 :  0.6424985\n",
      "Epoch 0, Loss: 3.63370680809021\n",
      "Epoch 20, Loss: 0.9332802891731262\n",
      "Epoch 40, Loss: 0.9351887106895447\n",
      "Epoch 60, Loss: 0.9342864155769348\n",
      "Epoch 80, Loss: 0.9355248212814331\n",
      "Epoch 100, Loss: 0.9362087249755859\n",
      "Epoch 120, Loss: 0.9372893571853638\n",
      "Epoch 140, Loss: 0.938562273979187\n",
      "Epoch 160, Loss: 0.939985454082489\n",
      "Epoch 180, Loss: 0.9415650963783264\n",
      "Epoch 200, Loss: 0.94329833984375\n",
      "Epoch 220, Loss: 0.9451901912689209\n",
      "Epoch 240, Loss: 0.9472463726997375\n",
      "Epoch 260, Loss: 0.9494725465774536\n",
      "Epoch 280, Loss: 0.951873779296875\n",
      "Epoch 300, Loss: 0.9544558525085449\n",
      "Epoch 320, Loss: 0.9572242498397827\n",
      "Epoch 340, Loss: 0.9601844549179077\n",
      "Epoch 360, Loss: 0.9633418321609497\n",
      "Epoch 380, Loss: 0.9667013883590698\n",
      "Epoch 400, Loss: 0.9702684879302979\n",
      "Epoch 420, Loss: 0.9740477800369263\n",
      "Epoch 440, Loss: 0.978043794631958\n",
      "Epoch 460, Loss: 0.982261061668396\n",
      "Epoch 480, Loss: 0.9867038726806641\n",
      "weights of  P073 :  [[array([[ 0.22105394, -0.5485749 ],\n",
      "       [-0.9595467 , -0.38840216],\n",
      "       [ 0.35280415, -0.24521646],\n",
      "       [-0.03958008, -0.5778947 ],\n",
      "       [-0.7321639 ,  0.43592152]], dtype=float32), array([ 0.19971684, -0.19971685], dtype=float32)]]\n",
      "Lowest loss of  P073 :  0.9332803\n",
      "Epoch 0, Loss: 1.0404196977615356\n",
      "Epoch 20, Loss: 0.9841077327728271\n",
      "Epoch 40, Loss: 0.9834407567977905\n",
      "Epoch 60, Loss: 0.9822992086410522\n",
      "Epoch 80, Loss: 0.9818397760391235\n",
      "Epoch 100, Loss: 0.9819844961166382\n",
      "Epoch 120, Loss: 0.9827743768692017\n",
      "Epoch 140, Loss: 0.9842379093170166\n",
      "Epoch 160, Loss: 0.9863343238830566\n",
      "Epoch 180, Loss: 0.9889923930168152\n",
      "Epoch 200, Loss: 0.9921169877052307\n",
      "Epoch 220, Loss: 0.9956004619598389\n",
      "Epoch 240, Loss: 0.9993326663970947\n",
      "Epoch 260, Loss: 1.0032076835632324\n",
      "Epoch 280, Loss: 1.0071293115615845\n",
      "Epoch 300, Loss: 1.0110139846801758\n",
      "Epoch 320, Loss: 1.0147926807403564\n",
      "Epoch 340, Loss: 1.018410563468933\n",
      "Epoch 360, Loss: 1.021826982498169\n",
      "Epoch 380, Loss: 1.025014042854309\n",
      "Epoch 400, Loss: 1.0279549360275269\n",
      "Epoch 420, Loss: 1.0306423902511597\n",
      "Epoch 440, Loss: 1.033076524734497\n",
      "Epoch 460, Loss: 1.0352641344070435\n",
      "Epoch 480, Loss: 1.037215232849121\n",
      "weights of  P074 :  [[array([[-0.719718  , -0.87987375],\n",
      "       [-0.5042203 ,  0.21961942],\n",
      "       [ 0.6617737 ,  0.67012894],\n",
      "       [-0.01153839,  0.2003805 ],\n",
      "       [ 0.5526472 , -0.34799778]], dtype=float32), array([-0.04246555,  0.04246553], dtype=float32)]]\n",
      "Lowest loss of  P074 :  0.9818215\n",
      "Epoch 0, Loss: 1.2210890054702759\n",
      "Epoch 20, Loss: 1.0730204582214355\n",
      "Epoch 40, Loss: 0.9529880881309509\n",
      "Epoch 60, Loss: 0.8665116429328918\n",
      "Epoch 80, Loss: 0.8094297051429749\n",
      "Epoch 100, Loss: 0.7813093066215515\n",
      "Epoch 120, Loss: 0.7781002521514893\n",
      "Epoch 140, Loss: 0.7948976755142212\n",
      "Epoch 160, Loss: 0.827072024345398\n",
      "Epoch 180, Loss: 0.8705925941467285\n",
      "Epoch 200, Loss: 0.9222127199172974\n",
      "Epoch 220, Loss: 0.9794101715087891\n",
      "Epoch 240, Loss: 1.0402783155441284\n",
      "Epoch 260, Loss: 1.1033976078033447\n",
      "Epoch 280, Loss: 1.1677194833755493\n",
      "Epoch 300, Loss: 1.2324695587158203\n",
      "Epoch 320, Loss: 1.2970728874206543\n",
      "Epoch 340, Loss: 1.3610994815826416\n",
      "Epoch 360, Loss: 1.4242221117019653\n",
      "Epoch 380, Loss: 1.4861903190612793\n",
      "Epoch 400, Loss: 1.5468084812164307\n",
      "Epoch 420, Loss: 1.6059225797653198\n",
      "Epoch 440, Loss: 1.6634109020233154\n",
      "Epoch 460, Loss: 1.719177007675171\n",
      "Epoch 480, Loss: 1.7731451988220215\n",
      "weights of  P075 :  [[array([[ 0.31273904, -0.06996996],\n",
      "       [ 0.22043568,  0.30357265],\n",
      "       [ 0.38784537,  0.34981686],\n",
      "       [ 0.11345368,  0.748554  ],\n",
      "       [-0.31208044, -0.35244575]], dtype=float32), array([-0.02611376,  0.02611373], dtype=float32)]]\n",
      "Lowest loss of  P075 :  0.7766909\n",
      "Epoch 0, Loss: 1.0082459449768066\n",
      "Epoch 20, Loss: 0.9805546998977661\n",
      "Epoch 40, Loss: 0.9639678001403809\n",
      "Epoch 60, Loss: 0.9600108861923218\n",
      "Epoch 80, Loss: 0.9700481295585632\n",
      "Epoch 100, Loss: 0.99134361743927\n",
      "Epoch 120, Loss: 1.0201210975646973\n",
      "Epoch 140, Loss: 1.0531593561172485\n",
      "Epoch 160, Loss: 1.0883369445800781\n",
      "Epoch 180, Loss: 1.1245503425598145\n",
      "Epoch 200, Loss: 1.1614142656326294\n",
      "Epoch 220, Loss: 1.1989555358886719\n",
      "Epoch 240, Loss: 1.2373863458633423\n",
      "Epoch 260, Loss: 1.2769643068313599\n",
      "Epoch 280, Loss: 1.317921757698059\n",
      "Epoch 300, Loss: 1.3604397773742676\n",
      "Epoch 320, Loss: 1.4046428203582764\n",
      "Epoch 340, Loss: 1.4506070613861084\n",
      "Epoch 360, Loss: 1.4983681440353394\n",
      "Epoch 380, Loss: 1.54793119430542\n",
      "Epoch 400, Loss: 1.5992779731750488\n",
      "Epoch 420, Loss: 1.6523716449737549\n",
      "Epoch 440, Loss: 1.7071609497070312\n",
      "Epoch 460, Loss: 1.7635822296142578\n",
      "Epoch 480, Loss: 1.8215619325637817\n",
      "weights of  P076 :  [[array([[ 0.6061227 , -1.0073571 ],\n",
      "       [-0.1717078 ,  0.70587784],\n",
      "       [ 0.07833578,  0.71401525],\n",
      "       [ 0.6416176 ,  0.381939  ],\n",
      "       [ 0.1421613 ,  0.02562428]], dtype=float32), array([-0.09634326,  0.09634322], dtype=float32)]]\n",
      "Lowest loss of  P076 :  0.95966536\n",
      "Epoch 0, Loss: 0.969206690788269\n",
      "Epoch 20, Loss: 0.8527385592460632\n",
      "Epoch 40, Loss: 0.8090776801109314\n",
      "Epoch 60, Loss: 0.7996507287025452\n",
      "Epoch 80, Loss: 0.818449854850769\n",
      "Epoch 100, Loss: 0.8625643849372864\n",
      "Epoch 120, Loss: 0.9265472888946533\n",
      "Epoch 140, Loss: 1.0045546293258667\n",
      "Epoch 160, Loss: 1.0918190479278564\n",
      "Epoch 180, Loss: 1.1845905780792236\n",
      "Epoch 200, Loss: 1.2798991203308105\n",
      "Epoch 220, Loss: 1.3754509687423706\n",
      "Epoch 240, Loss: 1.4695489406585693\n",
      "Epoch 260, Loss: 1.560974359512329\n",
      "Epoch 280, Loss: 1.6488702297210693\n",
      "Epoch 300, Loss: 1.732649803161621\n",
      "Epoch 320, Loss: 1.8119282722473145\n",
      "Epoch 340, Loss: 1.8864777088165283\n",
      "Epoch 360, Loss: 1.95618736743927\n",
      "Epoch 380, Loss: 2.0210366249084473\n",
      "Epoch 400, Loss: 2.0810742378234863\n",
      "Epoch 420, Loss: 2.1364049911499023\n",
      "Epoch 440, Loss: 2.1871728897094727\n",
      "Epoch 460, Loss: 2.2335567474365234\n",
      "Epoch 480, Loss: 2.2757585048675537\n",
      "weights of  P077 :  [[array([[-0.05168314,  0.10665978],\n",
      "       [-0.02988556, -0.6067491 ],\n",
      "       [ 0.15423709,  0.1605889 ],\n",
      "       [-0.09961054,  0.08634163],\n",
      "       [-0.8132095 , -0.17385603]], dtype=float32), array([ 0.06978766, -0.06978764], dtype=float32)]]\n",
      "Lowest loss of  P077 :  0.7991136\n",
      "Epoch 0, Loss: 1.3037619590759277\n",
      "Epoch 20, Loss: 1.030981183052063\n",
      "Epoch 40, Loss: 1.0072574615478516\n",
      "Epoch 60, Loss: 0.9910275936126709\n",
      "Epoch 80, Loss: 0.9754189252853394\n",
      "Epoch 100, Loss: 0.9620416164398193\n",
      "Epoch 120, Loss: 0.9509221315383911\n",
      "Epoch 140, Loss: 0.9419532418251038\n",
      "Epoch 160, Loss: 0.9348823428153992\n",
      "Epoch 180, Loss: 0.9294184446334839\n",
      "Epoch 200, Loss: 0.9252949357032776\n",
      "Epoch 220, Loss: 0.9222913384437561\n",
      "Epoch 240, Loss: 0.9202331900596619\n",
      "Epoch 260, Loss: 0.9189828634262085\n",
      "Epoch 280, Loss: 0.9184281229972839\n",
      "Epoch 300, Loss: 0.918473482131958\n",
      "Epoch 320, Loss: 0.9190347194671631\n",
      "Epoch 340, Loss: 0.920034646987915\n",
      "Epoch 360, Loss: 0.9214026927947998\n",
      "Epoch 380, Loss: 0.9230741858482361\n",
      "Epoch 400, Loss: 0.924990177154541\n",
      "Epoch 420, Loss: 0.9270980954170227\n",
      "Epoch 440, Loss: 0.9293513298034668\n",
      "Epoch 460, Loss: 0.9317098259925842\n",
      "Epoch 480, Loss: 0.9341388940811157\n",
      "weights of  P078 :  [[array([[ 0.5524952 ,  0.5502122 ],\n",
      "       [-0.8055426 , -0.55992997],\n",
      "       [-0.12213191,  0.2098498 ],\n",
      "       [-0.33033097, -0.15154563],\n",
      "       [ 0.69808066,  0.32581258]], dtype=float32), array([-0.40211722,  0.40211722], dtype=float32)]]\n",
      "Lowest loss of  P078 :  0.9183791\n",
      "Epoch 0, Loss: 0.9987351894378662\n",
      "Epoch 20, Loss: 0.9021960496902466\n",
      "Epoch 40, Loss: 0.886881947517395\n",
      "Epoch 60, Loss: 0.8769026398658752\n",
      "Epoch 80, Loss: 0.8817408084869385\n",
      "Epoch 100, Loss: 0.8985006809234619\n",
      "Epoch 120, Loss: 0.9230949878692627\n",
      "Epoch 140, Loss: 0.9534003138542175\n",
      "Epoch 160, Loss: 0.9871398210525513\n",
      "Epoch 180, Loss: 1.0231356620788574\n",
      "Epoch 200, Loss: 1.0608742237091064\n",
      "Epoch 220, Loss: 1.1003164052963257\n",
      "Epoch 240, Loss: 1.1416865587234497\n",
      "Epoch 260, Loss: 1.1853020191192627\n",
      "Epoch 280, Loss: 1.23146653175354\n",
      "Epoch 300, Loss: 1.2804104089736938\n",
      "Epoch 320, Loss: 1.332268238067627\n",
      "Epoch 340, Loss: 1.3870753049850464\n",
      "Epoch 360, Loss: 1.444779396057129\n",
      "Epoch 380, Loss: 1.5052555799484253\n",
      "Epoch 400, Loss: 1.5683244466781616\n",
      "Epoch 420, Loss: 1.6337649822235107\n",
      "Epoch 440, Loss: 1.7013304233551025\n",
      "Epoch 460, Loss: 1.7707583904266357\n",
      "Epoch 480, Loss: 1.8417783975601196\n",
      "weights of  P079 :  [[array([[-0.00945681,  0.12706377],\n",
      "       [-0.83440346, -0.45411575],\n",
      "       [ 0.3965854 ,  0.26287562],\n",
      "       [ 0.09817384,  0.90740687],\n",
      "       [-0.23275584,  0.42195415]], dtype=float32), array([ 0.04257606, -0.04257603], dtype=float32)]]\n",
      "Lowest loss of  P079 :  0.8765378\n",
      "Epoch 0, Loss: 2.1026296615600586\n",
      "Epoch 20, Loss: 0.8733423352241516\n",
      "Epoch 40, Loss: 0.8704599142074585\n",
      "Epoch 60, Loss: 0.8582702875137329\n",
      "Epoch 80, Loss: 0.8556060791015625\n",
      "Epoch 100, Loss: 0.8562968373298645\n",
      "Epoch 120, Loss: 0.8571092486381531\n",
      "Epoch 140, Loss: 0.8583122491836548\n",
      "Epoch 160, Loss: 0.8597331643104553\n",
      "Epoch 180, Loss: 0.8614537715911865\n",
      "Epoch 200, Loss: 0.8634828925132751\n",
      "Epoch 220, Loss: 0.865848183631897\n",
      "Epoch 240, Loss: 0.8685751557350159\n",
      "Epoch 260, Loss: 0.8716865181922913\n",
      "Epoch 280, Loss: 0.8752042651176453\n",
      "Epoch 300, Loss: 0.8791497945785522\n",
      "Epoch 320, Loss: 0.8835424780845642\n",
      "Epoch 340, Loss: 0.888400673866272\n",
      "Epoch 360, Loss: 0.8937414884567261\n",
      "Epoch 380, Loss: 0.8995802998542786\n",
      "Epoch 400, Loss: 0.9059308171272278\n",
      "Epoch 420, Loss: 0.9128059148788452\n",
      "Epoch 440, Loss: 0.9202163219451904\n",
      "Epoch 460, Loss: 0.9281718730926514\n",
      "Epoch 480, Loss: 0.9366803169250488\n",
      "weights of  P080 :  [[array([[ 0.27222666,  0.38949403],\n",
      "       [-0.3864864 ,  0.04553222],\n",
      "       [-0.05683   , -1.0558556 ],\n",
      "       [-0.0447135 ,  0.24316695],\n",
      "       [ 0.03077068,  0.4371891 ]], dtype=float32), array([-0.2167515 ,  0.21675152], dtype=float32)]]\n",
      "Lowest loss of  P080 :  0.8541875\n",
      "Epoch 0, Loss: 2.8642897605895996\n",
      "Epoch 20, Loss: 1.0368468761444092\n",
      "Epoch 40, Loss: 1.0354387760162354\n",
      "Epoch 60, Loss: 1.0334113836288452\n",
      "Epoch 80, Loss: 1.0335345268249512\n",
      "Epoch 100, Loss: 1.0338718891143799\n",
      "Epoch 120, Loss: 1.034083366394043\n",
      "Epoch 140, Loss: 1.0344579219818115\n",
      "Epoch 160, Loss: 1.034854531288147\n",
      "Epoch 180, Loss: 1.0353111028671265\n",
      "Epoch 200, Loss: 1.035831093788147\n",
      "Epoch 220, Loss: 1.036413311958313\n",
      "Epoch 240, Loss: 1.0370612144470215\n",
      "Epoch 260, Loss: 1.0377784967422485\n",
      "Epoch 280, Loss: 1.0385675430297852\n",
      "Epoch 300, Loss: 1.039431095123291\n",
      "Epoch 320, Loss: 1.0403711795806885\n",
      "Epoch 340, Loss: 1.0413901805877686\n",
      "Epoch 360, Loss: 1.0424896478652954\n",
      "Epoch 380, Loss: 1.0436714887619019\n",
      "Epoch 400, Loss: 1.0449368953704834\n",
      "Epoch 420, Loss: 1.0462870597839355\n",
      "Epoch 440, Loss: 1.0477226972579956\n",
      "Epoch 460, Loss: 1.0492446422576904\n",
      "Epoch 480, Loss: 1.0508530139923096\n",
      "weights of  P081 :  [[array([[ 1.0319482e+00, -2.5696773e-04],\n",
      "       [-6.7484581e-01,  8.4105566e-02],\n",
      "       [ 7.1313649e-02,  3.3184332e-01],\n",
      "       [-5.4166466e-01, -8.6037916e-01],\n",
      "       [ 7.1962595e-01,  5.5868137e-01]], dtype=float32), array([-0.18586825,  0.18586825], dtype=float32)]]\n",
      "Lowest loss of  P081 :  1.0328708\n",
      "Epoch 0, Loss: 1.009188175201416\n",
      "Epoch 20, Loss: 0.9738500118255615\n",
      "Epoch 40, Loss: 0.9617690443992615\n",
      "Epoch 60, Loss: 0.9678908586502075\n",
      "Epoch 80, Loss: 0.9909863471984863\n",
      "Epoch 100, Loss: 1.0282396078109741\n",
      "Epoch 120, Loss: 1.076887845993042\n",
      "Epoch 140, Loss: 1.1348540782928467\n",
      "Epoch 160, Loss: 1.2005375623703003\n",
      "Epoch 180, Loss: 1.2727670669555664\n",
      "Epoch 200, Loss: 1.3505990505218506\n",
      "Epoch 220, Loss: 1.433215856552124\n",
      "Epoch 240, Loss: 1.5198369026184082\n",
      "Epoch 260, Loss: 1.6096880435943604\n",
      "Epoch 280, Loss: 1.7019991874694824\n",
      "Epoch 300, Loss: 1.7960121631622314\n",
      "Epoch 320, Loss: 1.8909986019134521\n",
      "Epoch 340, Loss: 1.9862754344940186\n",
      "Epoch 360, Loss: 2.0812137126922607\n",
      "Epoch 380, Loss: 2.1752476692199707\n",
      "Epoch 400, Loss: 2.267876148223877\n",
      "Epoch 420, Loss: 2.3586626052856445\n",
      "Epoch 440, Loss: 2.447235107421875\n",
      "Epoch 460, Loss: 2.5332798957824707\n",
      "Epoch 480, Loss: 2.6165401935577393\n",
      "weights of  P082 :  [[array([[-0.21602614,  0.49243128],\n",
      "       [-0.46876952, -0.31219885],\n",
      "       [ 0.09926134, -0.34412274],\n",
      "       [-0.6299612 , -0.61324817],\n",
      "       [-0.63284713, -0.9931451 ]], dtype=float32), array([ 0.02078412, -0.02078412], dtype=float32)]]\n",
      "Lowest loss of  P082 :  0.9615171\n",
      "Epoch 0, Loss: 0.8590065836906433\n",
      "Epoch 20, Loss: 0.8383574485778809\n",
      "Epoch 40, Loss: 0.8406184315681458\n",
      "Epoch 60, Loss: 0.8632805943489075\n",
      "Epoch 80, Loss: 0.9002150297164917\n",
      "Epoch 100, Loss: 0.9513692259788513\n",
      "Epoch 120, Loss: 1.0156691074371338\n",
      "Epoch 140, Loss: 1.0910483598709106\n",
      "Epoch 160, Loss: 1.1763768196105957\n",
      "Epoch 180, Loss: 1.2703864574432373\n",
      "Epoch 200, Loss: 1.371934413909912\n",
      "Epoch 220, Loss: 1.4798821210861206\n",
      "Epoch 240, Loss: 1.5931110382080078\n",
      "Epoch 260, Loss: 1.7105188369750977\n",
      "Epoch 280, Loss: 1.8310497999191284\n",
      "Epoch 300, Loss: 1.9537112712860107\n",
      "Epoch 320, Loss: 2.0775885581970215\n",
      "Epoch 340, Loss: 2.2018508911132812\n",
      "Epoch 360, Loss: 2.3257534503936768\n",
      "Epoch 380, Loss: 2.4486324787139893\n",
      "Epoch 400, Loss: 2.569906711578369\n",
      "Epoch 420, Loss: 2.689070224761963\n",
      "Epoch 440, Loss: 2.805689573287964\n",
      "Epoch 460, Loss: 2.9193918704986572\n",
      "Epoch 480, Loss: 3.0298714637756348\n",
      "weights of  P083 :  [[array([[ 0.5542775 , -0.167863  ],\n",
      "       [ 0.40731564, -0.36217266],\n",
      "       [-0.25519982,  0.3520343 ],\n",
      "       [-0.6164606 , -0.45553988],\n",
      "       [-0.12208886,  0.00826115]], dtype=float32), array([ 0.08069941, -0.08069941], dtype=float32)]]\n",
      "Lowest loss of  P083 :  0.8350305\n",
      "Epoch 0, Loss: 0.9867277145385742\n",
      "Epoch 20, Loss: 0.9069010019302368\n",
      "Epoch 40, Loss: 0.9024011492729187\n",
      "Epoch 60, Loss: 0.9292572736740112\n",
      "Epoch 80, Loss: 0.9562878608703613\n",
      "Epoch 100, Loss: 0.9731357097625732\n",
      "Epoch 120, Loss: 0.9822673797607422\n",
      "Epoch 140, Loss: 0.9880160093307495\n",
      "Epoch 160, Loss: 0.9930776357650757\n",
      "Epoch 180, Loss: 0.9985421895980835\n",
      "Epoch 200, Loss: 1.004582405090332\n",
      "Epoch 220, Loss: 1.0110238790512085\n",
      "Epoch 240, Loss: 1.0176361799240112\n",
      "Epoch 260, Loss: 1.0242271423339844\n",
      "Epoch 280, Loss: 1.0306506156921387\n",
      "Epoch 300, Loss: 1.0367947816848755\n",
      "Epoch 320, Loss: 1.0425748825073242\n",
      "Epoch 340, Loss: 1.0479298830032349\n",
      "Epoch 360, Loss: 1.0528223514556885\n",
      "Epoch 380, Loss: 1.057234287261963\n",
      "Epoch 400, Loss: 1.0611649751663208\n",
      "Epoch 420, Loss: 1.0646281242370605\n",
      "Epoch 440, Loss: 1.0676466226577759\n",
      "Epoch 460, Loss: 1.0702513456344604\n",
      "Epoch 480, Loss: 1.0724774599075317\n",
      "weights of  P084 :  [[array([[ 0.48261356,  0.18405892],\n",
      "       [-0.84740496,  0.4108976 ],\n",
      "       [-0.5314046 , -0.9216118 ],\n",
      "       [-0.21255177, -0.46566367],\n",
      "       [ 0.5420501 , -0.24357483]], dtype=float32), array([-0.20620848,  0.2062085 ], dtype=float32)]]\n",
      "Lowest loss of  P084 :  0.8977562\n",
      "Epoch 0, Loss: 1.9067500829696655\n",
      "Epoch 20, Loss: 0.962472677230835\n",
      "Epoch 40, Loss: 0.9308030605316162\n",
      "Epoch 60, Loss: 0.9035911560058594\n",
      "Epoch 80, Loss: 0.8889148235321045\n",
      "Epoch 100, Loss: 0.8861669301986694\n",
      "Epoch 120, Loss: 0.8922469615936279\n",
      "Epoch 140, Loss: 0.9072704315185547\n",
      "Epoch 160, Loss: 0.9304175972938538\n",
      "Epoch 180, Loss: 0.9608892798423767\n",
      "Epoch 200, Loss: 0.997848629951477\n",
      "Epoch 220, Loss: 1.0405263900756836\n",
      "Epoch 240, Loss: 1.0882322788238525\n",
      "Epoch 260, Loss: 1.140365719795227\n",
      "Epoch 280, Loss: 1.1964091062545776\n",
      "Epoch 300, Loss: 1.2559192180633545\n",
      "Epoch 320, Loss: 1.3185175657272339\n",
      "Epoch 340, Loss: 1.383880615234375\n",
      "Epoch 360, Loss: 1.4517312049865723\n",
      "Epoch 380, Loss: 1.5218318700790405\n",
      "Epoch 400, Loss: 1.593977928161621\n",
      "Epoch 420, Loss: 1.6679919958114624\n",
      "Epoch 440, Loss: 1.7437206506729126\n",
      "Epoch 460, Loss: 1.821029543876648\n",
      "Epoch 480, Loss: 1.899801254272461\n",
      "weights of  P085 :  [[array([[ 0.96670294,  0.36694548],\n",
      "       [-0.26222062,  0.82850516],\n",
      "       [ 0.08528771,  0.57270306],\n",
      "       [-0.72119355, -0.85330796],\n",
      "       [ 0.5681535 ,  0.21166746]], dtype=float32), array([-0.26177096,  0.261771  ], dtype=float32)]]\n",
      "Lowest loss of  P085 :  0.8859222\n",
      "Epoch 0, Loss: 1.84755539894104\n",
      "Epoch 20, Loss: 0.9768282175064087\n",
      "Epoch 40, Loss: 0.95546555519104\n",
      "Epoch 60, Loss: 0.9433495998382568\n",
      "Epoch 80, Loss: 0.9448825120925903\n",
      "Epoch 100, Loss: 0.9514176845550537\n",
      "Epoch 120, Loss: 0.9605379700660706\n",
      "Epoch 140, Loss: 0.9714184403419495\n",
      "Epoch 160, Loss: 0.9835346937179565\n",
      "Epoch 180, Loss: 0.9966491460800171\n",
      "Epoch 200, Loss: 1.0107167959213257\n",
      "Epoch 220, Loss: 1.0257792472839355\n",
      "Epoch 240, Loss: 1.0419011116027832\n",
      "Epoch 260, Loss: 1.0591377019882202\n",
      "Epoch 280, Loss: 1.0775278806686401\n",
      "Epoch 300, Loss: 1.0970932245254517\n",
      "Epoch 320, Loss: 1.117842435836792\n",
      "Epoch 340, Loss: 1.139775037765503\n",
      "Epoch 360, Loss: 1.1628841161727905\n",
      "Epoch 380, Loss: 1.1871583461761475\n",
      "Epoch 400, Loss: 1.212583065032959\n",
      "Epoch 420, Loss: 1.239140510559082\n",
      "Epoch 440, Loss: 1.2668113708496094\n",
      "Epoch 460, Loss: 1.295573115348816\n",
      "Epoch 480, Loss: 1.325402021408081\n",
      "weights of  P086 :  [[array([[ 0.6571489 ,  0.15675342],\n",
      "       [ 0.38153863,  0.20078844],\n",
      "       [-0.62857944,  0.5733507 ],\n",
      "       [ 0.1162501 , -1.0073003 ],\n",
      "       [-0.23098546,  0.58599555]], dtype=float32), array([-0.16998498,  0.16998498], dtype=float32)]]\n",
      "Lowest loss of  P086 :  0.9416418\n",
      "Epoch 0, Loss: 1.1076661348342896\n",
      "Epoch 20, Loss: 1.0291119813919067\n",
      "Epoch 40, Loss: 1.0337039232254028\n",
      "Epoch 60, Loss: 1.0593620538711548\n",
      "Epoch 80, Loss: 1.082372784614563\n",
      "Epoch 100, Loss: 1.1096601486206055\n",
      "Epoch 120, Loss: 1.1418132781982422\n",
      "Epoch 140, Loss: 1.1772089004516602\n",
      "Epoch 160, Loss: 1.2150347232818604\n",
      "Epoch 180, Loss: 1.2550642490386963\n",
      "Epoch 200, Loss: 1.2970298528671265\n",
      "Epoch 220, Loss: 1.340590476989746\n",
      "Epoch 240, Loss: 1.3854098320007324\n",
      "Epoch 260, Loss: 1.4311354160308838\n",
      "Epoch 280, Loss: 1.4773848056793213\n",
      "Epoch 300, Loss: 1.5237622261047363\n",
      "Epoch 320, Loss: 1.5698745250701904\n",
      "Epoch 340, Loss: 1.615342617034912\n",
      "Epoch 360, Loss: 1.6598148345947266\n",
      "Epoch 380, Loss: 1.702976942062378\n",
      "Epoch 400, Loss: 1.744558572769165\n",
      "Epoch 420, Loss: 1.7843347787857056\n",
      "Epoch 440, Loss: 1.8221304416656494\n",
      "Epoch 460, Loss: 1.8578155040740967\n",
      "Epoch 480, Loss: 1.8913040161132812\n",
      "weights of  P087 :  [[array([[ 1.0241563 ,  0.5791657 ],\n",
      "       [-0.43617332, -0.6343804 ],\n",
      "       [-0.2424395 ,  0.56730205],\n",
      "       [ 0.45303124, -0.837977  ],\n",
      "       [-0.47963876, -0.3115661 ]], dtype=float32), array([ 0.09523848, -0.09523848], dtype=float32)]]\n",
      "Lowest loss of  P087 :  1.0245725\n",
      "Epoch 0, Loss: 1.1665399074554443\n",
      "Epoch 20, Loss: 1.091484546661377\n",
      "Epoch 40, Loss: 1.0712213516235352\n",
      "Epoch 60, Loss: 1.0658578872680664\n",
      "Epoch 80, Loss: 1.078001856803894\n",
      "Epoch 100, Loss: 1.1081149578094482\n",
      "Epoch 120, Loss: 1.1550679206848145\n",
      "Epoch 140, Loss: 1.2172714471817017\n",
      "Epoch 160, Loss: 1.293146014213562\n",
      "Epoch 180, Loss: 1.381256103515625\n",
      "Epoch 200, Loss: 1.480339765548706\n",
      "Epoch 220, Loss: 1.5893036127090454\n",
      "Epoch 240, Loss: 1.7072043418884277\n",
      "Epoch 260, Loss: 1.833222508430481\n",
      "Epoch 280, Loss: 1.9666407108306885\n",
      "Epoch 300, Loss: 2.106825351715088\n",
      "Epoch 320, Loss: 2.2532119750976562\n",
      "Epoch 340, Loss: 2.4052910804748535\n",
      "Epoch 360, Loss: 2.562600612640381\n",
      "Epoch 380, Loss: 2.7247185707092285\n",
      "Epoch 400, Loss: 2.89125657081604\n",
      "Epoch 420, Loss: 3.0618557929992676\n",
      "Epoch 440, Loss: 3.2361819744110107\n",
      "Epoch 460, Loss: 3.4139232635498047\n",
      "Epoch 480, Loss: 3.5947885513305664\n",
      "weights of  P088 :  [[array([[-0.36497694,  0.53482354],\n",
      "       [-0.9397125 , -0.6516728 ],\n",
      "       [ 0.73556495, -0.40259686],\n",
      "       [-1.1422645 , -0.39864045],\n",
      "       [ 0.31618196, -0.38454485]], dtype=float32), array([ 0.02459479, -0.02459478], dtype=float32)]]\n",
      "Lowest loss of  P088 :  1.065622\n",
      "Epoch 0, Loss: 4.063183307647705\n",
      "Epoch 20, Loss: 1.011104702949524\n",
      "Epoch 40, Loss: 0.9970437288284302\n",
      "Epoch 60, Loss: 0.9838986396789551\n",
      "Epoch 80, Loss: 0.975665807723999\n",
      "Epoch 100, Loss: 0.9734889268875122\n",
      "Epoch 120, Loss: 0.9714213609695435\n",
      "Epoch 140, Loss: 0.9692613482475281\n",
      "Epoch 160, Loss: 0.9669546484947205\n",
      "Epoch 180, Loss: 0.9645101428031921\n",
      "Epoch 200, Loss: 0.9619396924972534\n",
      "Epoch 220, Loss: 0.9592540264129639\n",
      "Epoch 240, Loss: 0.9564627408981323\n",
      "Epoch 260, Loss: 0.9535740613937378\n",
      "Epoch 280, Loss: 0.9505962133407593\n",
      "Epoch 300, Loss: 0.9475365877151489\n",
      "Epoch 320, Loss: 0.944402277469635\n",
      "Epoch 340, Loss: 0.9412001967430115\n",
      "Epoch 360, Loss: 0.9379367828369141\n",
      "Epoch 380, Loss: 0.9346180558204651\n",
      "Epoch 400, Loss: 0.9312503337860107\n",
      "Epoch 420, Loss: 0.9278390407562256\n",
      "Epoch 440, Loss: 0.9243900775909424\n",
      "Epoch 460, Loss: 0.9209088087081909\n",
      "Epoch 480, Loss: 0.9174007177352905\n",
      "weights of  P089 :  [[array([[-0.50000864,  0.4971052 ],\n",
      "       [ 0.7255757 ,  0.23308827],\n",
      "       [-0.2191016 , -0.6445815 ],\n",
      "       [ 0.40660146, -0.1391333 ],\n",
      "       [-0.11360221,  0.53403676]], dtype=float32), array([-0.17430288,  0.17430288], dtype=float32)]]\n",
      "Lowest loss of  P089 :  0.9140477\n",
      "Epoch 0, Loss: 1.2082550525665283\n",
      "Epoch 20, Loss: 1.0819271802902222\n",
      "Epoch 40, Loss: 0.998014509677887\n",
      "Epoch 60, Loss: 0.9401752948760986\n",
      "Epoch 80, Loss: 0.8954770565032959\n",
      "Epoch 100, Loss: 0.8657088279724121\n",
      "Epoch 120, Loss: 0.8479740023612976\n",
      "Epoch 140, Loss: 0.8394418358802795\n",
      "Epoch 160, Loss: 0.837647557258606\n",
      "Epoch 180, Loss: 0.8408538103103638\n",
      "Epoch 200, Loss: 0.8478559255599976\n",
      "Epoch 220, Loss: 0.8578351140022278\n",
      "Epoch 240, Loss: 0.8702281713485718\n",
      "Epoch 260, Loss: 0.8846226930618286\n",
      "Epoch 280, Loss: 0.9006940126419067\n",
      "Epoch 300, Loss: 0.918166995048523\n",
      "Epoch 320, Loss: 0.936795711517334\n",
      "Epoch 340, Loss: 0.9563519358634949\n",
      "Epoch 360, Loss: 0.9766199588775635\n",
      "Epoch 380, Loss: 0.9973953366279602\n",
      "Epoch 400, Loss: 1.0184853076934814\n",
      "Epoch 420, Loss: 1.039709210395813\n",
      "Epoch 440, Loss: 1.0609018802642822\n",
      "Epoch 460, Loss: 1.0819131135940552\n",
      "Epoch 480, Loss: 1.1026091575622559\n",
      "weights of  P090 :  [[array([[-0.25541055, -0.3741969 ],\n",
      "       [ 0.40421683, -0.18501082],\n",
      "       [ 0.7816654 ,  0.5564393 ],\n",
      "       [ 0.34448043,  0.2148775 ],\n",
      "       [-0.25736928, -0.06231225]], dtype=float32), array([ 0.45323747, -0.4532374 ], dtype=float32)]]\n",
      "Lowest loss of  P090 :  0.83755463\n",
      "Epoch 0, Loss: 1.0213007926940918\n",
      "Epoch 20, Loss: 1.0591859817504883\n",
      "Epoch 40, Loss: 1.1573225259780884\n",
      "Epoch 60, Loss: 1.2740740776062012\n",
      "Epoch 80, Loss: 1.3907657861709595\n",
      "Epoch 100, Loss: 1.492631435394287\n",
      "Epoch 120, Loss: 1.5722393989562988\n",
      "Epoch 140, Loss: 1.6291172504425049\n",
      "Epoch 160, Loss: 1.667881727218628\n",
      "Epoch 180, Loss: 1.69392991065979\n",
      "Epoch 200, Loss: 1.7117177248001099\n",
      "Epoch 220, Loss: 1.724393367767334\n",
      "Epoch 240, Loss: 1.733964443206787\n",
      "Epoch 260, Loss: 1.7415878772735596\n",
      "Epoch 280, Loss: 1.747879981994629\n",
      "Epoch 300, Loss: 1.7531615495681763\n",
      "Epoch 320, Loss: 1.757610559463501\n",
      "Epoch 340, Loss: 1.7613410949707031\n",
      "Epoch 360, Loss: 1.764445424079895\n",
      "Epoch 380, Loss: 1.7670056819915771\n",
      "Epoch 400, Loss: 1.7690972089767456\n",
      "Epoch 420, Loss: 1.7707908153533936\n",
      "Epoch 440, Loss: 1.7721493244171143\n",
      "Epoch 460, Loss: 1.7732295989990234\n",
      "Epoch 480, Loss: 1.7740806341171265\n",
      "weights of  P091 :  [[array([[-0.47641337,  0.7002176 ],\n",
      "       [-0.05969661, -0.47358128],\n",
      "       [ 0.30794126,  0.8046994 ],\n",
      "       [-0.8372739 ,  0.44565076],\n",
      "       [ 0.2901632 , -0.8318092 ]], dtype=float32), array([ 0.03783634, -0.03783634], dtype=float32)]]\n",
      "Lowest loss of  P091 :  1.0072349\n",
      "Epoch 0, Loss: 0.9365308880805969\n",
      "Epoch 20, Loss: 0.8332604765892029\n",
      "Epoch 40, Loss: 0.828985333442688\n",
      "Epoch 60, Loss: 0.8566936254501343\n",
      "Epoch 80, Loss: 0.9060097932815552\n",
      "Epoch 100, Loss: 0.9703541398048401\n",
      "Epoch 120, Loss: 1.042852759361267\n",
      "Epoch 140, Loss: 1.120307207107544\n",
      "Epoch 160, Loss: 1.1996500492095947\n",
      "Epoch 180, Loss: 1.2786474227905273\n",
      "Epoch 200, Loss: 1.3556056022644043\n",
      "Epoch 220, Loss: 1.4293982982635498\n",
      "Epoch 240, Loss: 1.4994006156921387\n",
      "Epoch 260, Loss: 1.565382719039917\n",
      "Epoch 280, Loss: 1.6274182796478271\n",
      "Epoch 300, Loss: 1.685782790184021\n",
      "Epoch 320, Loss: 1.7408668994903564\n",
      "Epoch 340, Loss: 1.7931106090545654\n",
      "Epoch 360, Loss: 1.8429518938064575\n",
      "Epoch 380, Loss: 1.8907926082611084\n",
      "Epoch 400, Loss: 1.9369821548461914\n",
      "Epoch 420, Loss: 1.9818053245544434\n",
      "Epoch 440, Loss: 2.0254836082458496\n",
      "Epoch 460, Loss: 2.0681769847869873\n",
      "Epoch 480, Loss: 2.1099929809570312\n",
      "weights of  P092 :  [[array([[ 0.56906265, -0.97528976],\n",
      "       [ 0.01950822,  0.30873755],\n",
      "       [ 0.21254802, -0.00503633],\n",
      "       [-0.41277605,  0.26490894],\n",
      "       [ 0.40699664, -0.1346783 ]], dtype=float32), array([-0.23405349,  0.23405349], dtype=float32)]]\n",
      "Lowest loss of  P092 :  0.8260352\n",
      "Epoch 0, Loss: 1.059234380722046\n",
      "Epoch 20, Loss: 0.9297523498535156\n",
      "Epoch 40, Loss: 0.9041080474853516\n",
      "Epoch 60, Loss: 0.8860894441604614\n",
      "Epoch 80, Loss: 0.8765682578086853\n",
      "Epoch 100, Loss: 0.877875566482544\n",
      "Epoch 120, Loss: 0.8906704187393188\n",
      "Epoch 140, Loss: 0.9146841764450073\n",
      "Epoch 160, Loss: 0.949110746383667\n",
      "Epoch 180, Loss: 0.9928797483444214\n",
      "Epoch 200, Loss: 1.0448380708694458\n",
      "Epoch 220, Loss: 1.1038538217544556\n",
      "Epoch 240, Loss: 1.1688742637634277\n",
      "Epoch 260, Loss: 1.238950252532959\n",
      "Epoch 280, Loss: 1.3132426738739014\n",
      "Epoch 300, Loss: 1.391019582748413\n",
      "Epoch 320, Loss: 1.4716455936431885\n",
      "Epoch 340, Loss: 1.5545709133148193\n",
      "Epoch 360, Loss: 1.6393216848373413\n",
      "Epoch 380, Loss: 1.725486397743225\n",
      "Epoch 400, Loss: 1.8127106428146362\n",
      "Epoch 420, Loss: 1.9006857872009277\n",
      "Epoch 440, Loss: 1.9891424179077148\n",
      "Epoch 460, Loss: 2.0778470039367676\n",
      "Epoch 480, Loss: 2.1665942668914795\n",
      "weights of  P093 :  [[array([[-0.27215877, -0.723195  ],\n",
      "       [-1.138139  , -0.02315811],\n",
      "       [ 0.14284143,  0.4514352 ],\n",
      "       [ 0.19516672, -0.24124458],\n",
      "       [ 0.01457618, -0.79864645]], dtype=float32), array([-0.0740125,  0.0740125], dtype=float32)]]\n",
      "Lowest loss of  P093 :  0.87568676\n",
      "Epoch 0, Loss: 0.990909993648529\n",
      "Epoch 20, Loss: 0.8623533248901367\n",
      "Epoch 40, Loss: 0.8618134260177612\n",
      "Epoch 60, Loss: 0.9179999828338623\n",
      "Epoch 80, Loss: 1.0095072984695435\n",
      "Epoch 100, Loss: 1.1179507970809937\n",
      "Epoch 120, Loss: 1.2365206480026245\n",
      "Epoch 140, Loss: 1.362402319908142\n",
      "Epoch 160, Loss: 1.4941468238830566\n",
      "Epoch 180, Loss: 1.6309441328048706\n",
      "Epoch 200, Loss: 1.772226333618164\n",
      "Epoch 220, Loss: 1.9176790714263916\n",
      "Epoch 240, Loss: 2.0671117305755615\n",
      "Epoch 260, Loss: 2.220409631729126\n",
      "Epoch 280, Loss: 2.377488136291504\n",
      "Epoch 300, Loss: 2.538276195526123\n",
      "Epoch 320, Loss: 2.7026994228363037\n",
      "Epoch 340, Loss: 2.870677947998047\n",
      "Epoch 360, Loss: 3.042123556137085\n",
      "Epoch 380, Loss: 3.216939687728882\n",
      "Epoch 400, Loss: 3.3950185775756836\n",
      "Epoch 420, Loss: 3.576245069503784\n",
      "Epoch 440, Loss: 3.7605016231536865\n",
      "Epoch 460, Loss: 3.947661876678467\n",
      "Epoch 480, Loss: 4.137597560882568\n",
      "weights of  P094 :  [[array([[ 0.66383564, -0.4209127 ],\n",
      "       [-0.23000431,  0.6389874 ],\n",
      "       [-0.14690425,  0.64780444],\n",
      "       [-0.00614771,  0.08559468],\n",
      "       [ 0.7359819 , -0.31289586]], dtype=float32), array([ 0.14004996, -0.14004996], dtype=float32)]]\n",
      "Lowest loss of  P094 :  0.85412806\n",
      "Epoch 0, Loss: 1.1448389291763306\n",
      "Epoch 20, Loss: 1.0354143381118774\n",
      "Epoch 40, Loss: 0.9374088644981384\n",
      "Epoch 60, Loss: 0.8607889413833618\n",
      "Epoch 80, Loss: 0.8267770409584045\n",
      "Epoch 100, Loss: 0.8255302309989929\n",
      "Epoch 120, Loss: 0.8475163578987122\n",
      "Epoch 140, Loss: 0.8858533501625061\n",
      "Epoch 160, Loss: 0.9356412887573242\n",
      "Epoch 180, Loss: 0.9935030937194824\n",
      "Epoch 200, Loss: 1.0570788383483887\n",
      "Epoch 220, Loss: 1.124636173248291\n",
      "Epoch 240, Loss: 1.1948072910308838\n",
      "Epoch 260, Loss: 1.2664775848388672\n",
      "Epoch 280, Loss: 1.3387300968170166\n",
      "Epoch 300, Loss: 1.4108166694641113\n",
      "Epoch 320, Loss: 1.482140302658081\n",
      "Epoch 340, Loss: 1.5522385835647583\n",
      "Epoch 360, Loss: 1.6207656860351562\n",
      "Epoch 380, Loss: 1.6874711513519287\n",
      "Epoch 400, Loss: 1.7521839141845703\n",
      "Epoch 420, Loss: 1.8147937059402466\n",
      "Epoch 440, Loss: 1.8752355575561523\n",
      "Epoch 460, Loss: 1.9334816932678223\n",
      "Epoch 480, Loss: 1.989525556564331\n",
      "weights of  P095 :  [[array([[ 0.09519147, -0.20404239],\n",
      "       [-1.0115303 , -0.0295753 ],\n",
      "       [-0.24811178,  0.01974211],\n",
      "       [ 0.3245648 , -0.36322784],\n",
      "       [-0.33283597, -0.5057162 ]], dtype=float32), array([-0.521913,  0.521913], dtype=float32)]]\n",
      "Lowest loss of  P095 :  0.82253695\n",
      "Epoch 0, Loss: 0.9491910934448242\n",
      "Epoch 20, Loss: 0.9103911519050598\n",
      "Epoch 40, Loss: 0.9022916555404663\n",
      "Epoch 60, Loss: 0.902853786945343\n",
      "Epoch 80, Loss: 0.9138007164001465\n",
      "Epoch 100, Loss: 0.9348486661911011\n",
      "Epoch 120, Loss: 0.9606537222862244\n",
      "Epoch 140, Loss: 0.9845728874206543\n",
      "Epoch 160, Loss: 1.0040831565856934\n",
      "Epoch 180, Loss: 1.018950343132019\n",
      "Epoch 200, Loss: 1.0298421382904053\n",
      "Epoch 220, Loss: 1.0375984907150269\n",
      "Epoch 240, Loss: 1.042999267578125\n",
      "Epoch 260, Loss: 1.046691656112671\n",
      "Epoch 280, Loss: 1.049174189567566\n",
      "Epoch 300, Loss: 1.0508151054382324\n",
      "Epoch 320, Loss: 1.0518804788589478\n",
      "Epoch 340, Loss: 1.0525585412979126\n",
      "Epoch 360, Loss: 1.0529807806015015\n",
      "Epoch 380, Loss: 1.0532374382019043\n",
      "Epoch 400, Loss: 1.053389072418213\n",
      "Epoch 420, Loss: 1.0534765720367432\n",
      "Epoch 440, Loss: 1.0535252094268799\n",
      "Epoch 460, Loss: 1.0535515546798706\n",
      "Epoch 480, Loss: 1.0535653829574585\n",
      "weights of  P096 :  [[array([[ 0.07139602, -0.3881593 ],\n",
      "       [ 0.36832246,  0.33225834],\n",
      "       [ 1.1207684 , -0.36323184],\n",
      "       [-0.6730564 , -0.34481812],\n",
      "       [-0.02662427, -0.2864835 ]], dtype=float32), array([ 0.08151152, -0.0815116 ], dtype=float32)]]\n",
      "Lowest loss of  P096 :  0.9019444\n",
      "Epoch 0, Loss: 1.2375603914260864\n",
      "Epoch 20, Loss: 1.0796908140182495\n",
      "Epoch 40, Loss: 1.0201668739318848\n",
      "Epoch 60, Loss: 0.9996683597564697\n",
      "Epoch 80, Loss: 1.0037646293640137\n",
      "Epoch 100, Loss: 1.020628809928894\n",
      "Epoch 120, Loss: 1.041538953781128\n",
      "Epoch 140, Loss: 1.0610361099243164\n",
      "Epoch 160, Loss: 1.0773749351501465\n",
      "Epoch 180, Loss: 1.0903443098068237\n",
      "Epoch 200, Loss: 1.1006983518600464\n",
      "Epoch 220, Loss: 1.1093173027038574\n",
      "Epoch 240, Loss: 1.1169365644454956\n",
      "Epoch 260, Loss: 1.1240599155426025\n",
      "Epoch 280, Loss: 1.1309690475463867\n",
      "Epoch 300, Loss: 1.137780785560608\n",
      "Epoch 320, Loss: 1.1445083618164062\n",
      "Epoch 340, Loss: 1.1511096954345703\n",
      "Epoch 360, Loss: 1.1575207710266113\n",
      "Epoch 380, Loss: 1.1636784076690674\n",
      "Epoch 400, Loss: 1.1695290803909302\n",
      "Epoch 420, Loss: 1.1750344038009644\n",
      "Epoch 440, Loss: 1.1801708936691284\n",
      "Epoch 460, Loss: 1.1849288940429688\n",
      "Epoch 480, Loss: 1.189310073852539\n",
      "weights of  P097 :  [[array([[-0.06752707,  0.37514597],\n",
      "       [-0.5228339 , -0.02569624],\n",
      "       [-0.6249142 , -0.74859065],\n",
      "       [-1.0233445 , -0.45202678],\n",
      "       [ 0.7356951 ,  0.38439438]], dtype=float32), array([-0.29895738,  0.29895732], dtype=float32)]]\n",
      "Lowest loss of  P097 :  0.9986553\n",
      "Epoch 0, Loss: 1.7899142503738403\n",
      "Epoch 20, Loss: 1.2103791236877441\n",
      "Epoch 40, Loss: 1.1494628190994263\n",
      "Epoch 60, Loss: 1.1284455060958862\n",
      "Epoch 80, Loss: 1.111064076423645\n",
      "Epoch 100, Loss: 1.092847466468811\n",
      "Epoch 120, Loss: 1.07468843460083\n",
      "Epoch 140, Loss: 1.0569939613342285\n",
      "Epoch 160, Loss: 1.0401707887649536\n",
      "Epoch 180, Loss: 1.0245306491851807\n",
      "Epoch 200, Loss: 1.0102956295013428\n",
      "Epoch 220, Loss: 0.9976117610931396\n",
      "Epoch 240, Loss: 0.9865622520446777\n",
      "Epoch 260, Loss: 0.9771771430969238\n",
      "Epoch 280, Loss: 0.9694448113441467\n",
      "Epoch 300, Loss: 0.9633215665817261\n",
      "Epoch 320, Loss: 0.958740770816803\n",
      "Epoch 340, Loss: 0.9556195139884949\n",
      "Epoch 360, Loss: 0.9538662433624268\n",
      "Epoch 380, Loss: 0.9533848762512207\n",
      "Epoch 400, Loss: 0.9540799856185913\n",
      "Epoch 420, Loss: 0.9558583498001099\n",
      "Epoch 440, Loss: 0.9586317539215088\n",
      "Epoch 460, Loss: 0.9623185396194458\n",
      "Epoch 480, Loss: 0.9668439626693726\n",
      "weights of  P098 :  [[array([[ 0.08953629,  0.22867903],\n",
      "       [ 0.20666012,  0.2612838 ],\n",
      "       [-0.25121063, -1.2228796 ],\n",
      "       [-0.554165  , -0.68513346],\n",
      "       [-0.17412022,  0.47800413]], dtype=float32), array([ 0.38044152, -0.38044155], dtype=float32)]]\n",
      "Lowest loss of  P098 :  0.9533786\n",
      "Epoch 0, Loss: 1.0067638158798218\n",
      "Epoch 20, Loss: 0.8737013339996338\n",
      "Epoch 40, Loss: 0.8028730154037476\n",
      "Epoch 60, Loss: 0.7811086773872375\n",
      "Epoch 80, Loss: 0.7907627820968628\n",
      "Epoch 100, Loss: 0.8239596486091614\n",
      "Epoch 120, Loss: 0.8739168047904968\n",
      "Epoch 140, Loss: 0.936002254486084\n",
      "Epoch 160, Loss: 1.0070562362670898\n",
      "Epoch 180, Loss: 1.0844013690948486\n",
      "Epoch 200, Loss: 1.1657172441482544\n",
      "Epoch 220, Loss: 1.2488832473754883\n",
      "Epoch 240, Loss: 1.3320366144180298\n",
      "Epoch 260, Loss: 1.413599967956543\n",
      "Epoch 280, Loss: 1.492300033569336\n",
      "Epoch 300, Loss: 1.567159652709961\n",
      "Epoch 320, Loss: 1.637471318244934\n",
      "Epoch 340, Loss: 1.702768325805664\n",
      "Epoch 360, Loss: 1.762786865234375\n",
      "Epoch 380, Loss: 1.8174340724945068\n",
      "Epoch 400, Loss: 1.8667559623718262\n",
      "Epoch 420, Loss: 1.9109052419662476\n",
      "Epoch 440, Loss: 1.9501185417175293\n",
      "Epoch 460, Loss: 1.9846906661987305\n",
      "Epoch 480, Loss: 2.0149548053741455\n",
      "weights of  P099 :  [[array([[-0.15692215, -0.22478849],\n",
      "       [-0.01427903, -0.20186222],\n",
      "       [ 0.6998713 ,  0.24215487],\n",
      "       [ 0.16328834, -0.30856952],\n",
      "       [-0.42283934, -0.20200635]], dtype=float32), array([ 0.04546417, -0.04546401], dtype=float32)]]\n",
      "Lowest loss of  P099 :  0.7807958\n",
      "Epoch 0, Loss: 1.1072050333023071\n",
      "Epoch 20, Loss: 0.9340651035308838\n",
      "Epoch 40, Loss: 0.8494237661361694\n",
      "Epoch 60, Loss: 0.8117941617965698\n",
      "Epoch 80, Loss: 0.7999743223190308\n",
      "Epoch 100, Loss: 0.803440511226654\n",
      "Epoch 120, Loss: 0.8119360208511353\n",
      "Epoch 140, Loss: 0.8226069808006287\n",
      "Epoch 160, Loss: 0.8329402208328247\n",
      "Epoch 180, Loss: 0.8418686389923096\n",
      "Epoch 200, Loss: 0.8490755558013916\n",
      "Epoch 220, Loss: 0.8547512292861938\n",
      "Epoch 240, Loss: 0.8591348528862\n",
      "Epoch 260, Loss: 0.8624695539474487\n",
      "Epoch 280, Loss: 0.8649597764015198\n",
      "Epoch 300, Loss: 0.8667645454406738\n",
      "Epoch 320, Loss: 0.8680107593536377\n",
      "Epoch 340, Loss: 0.8688037395477295\n",
      "Epoch 360, Loss: 0.8692337274551392\n",
      "Epoch 380, Loss: 0.8693788051605225\n",
      "Epoch 400, Loss: 0.8693065643310547\n",
      "Epoch 420, Loss: 0.869074285030365\n",
      "Epoch 440, Loss: 0.8687291145324707\n",
      "Epoch 460, Loss: 0.8683091402053833\n",
      "Epoch 480, Loss: 0.8678435683250427\n",
      "weights of  P100 :  [[array([[ 0.3424215 ,  0.10888843],\n",
      "       [-0.3920863 , -0.45331606],\n",
      "       [ 0.0680623 , -0.02094947],\n",
      "       [-0.46133947,  0.52760893],\n",
      "       [-0.14787512,  0.30969247]], dtype=float32), array([-0.2631622 ,  0.26316217], dtype=float32)]]\n",
      "Lowest loss of  P100 :  0.7998162\n",
      "Epoch 0, Loss: 0.876564621925354\n",
      "Epoch 20, Loss: 0.8580688238143921\n",
      "Epoch 40, Loss: 0.8579872250556946\n",
      "Epoch 60, Loss: 0.8599686622619629\n",
      "Epoch 80, Loss: 0.8637320399284363\n",
      "Epoch 100, Loss: 0.8693896532058716\n",
      "Epoch 120, Loss: 0.8769403696060181\n",
      "Epoch 140, Loss: 0.8862581849098206\n",
      "Epoch 160, Loss: 0.8971632719039917\n",
      "Epoch 180, Loss: 0.909462034702301\n",
      "Epoch 200, Loss: 0.9229488372802734\n",
      "Epoch 220, Loss: 0.9374281764030457\n",
      "Epoch 240, Loss: 0.9527108669281006\n",
      "Epoch 260, Loss: 0.968618631362915\n",
      "Epoch 280, Loss: 0.9849845170974731\n",
      "Epoch 300, Loss: 1.0016533136367798\n",
      "Epoch 320, Loss: 1.0184838771820068\n",
      "Epoch 340, Loss: 1.0353492498397827\n",
      "Epoch 360, Loss: 1.0521376132965088\n",
      "Epoch 380, Loss: 1.068752408027649\n",
      "Epoch 400, Loss: 1.0851110219955444\n",
      "Epoch 420, Loss: 1.1011450290679932\n",
      "Epoch 440, Loss: 1.1167982816696167\n",
      "Epoch 460, Loss: 1.1320257186889648\n",
      "Epoch 480, Loss: 1.146791934967041\n",
      "weights of  P101 :  [[array([[-0.12271421,  0.7525764 ],\n",
      "       [ 0.33975118,  0.5483999 ],\n",
      "       [ 0.4110051 , -0.22786598],\n",
      "       [ 0.2279321 , -0.35175344],\n",
      "       [-0.5453295 , -0.11803796]], dtype=float32), array([ 0.03421292, -0.03421293], dtype=float32)]]\n",
      "Lowest loss of  P101 :  0.8571828\n",
      "Epoch 0, Loss: 4.91575288772583\n",
      "Epoch 20, Loss: 1.3150503635406494\n",
      "Epoch 40, Loss: 1.029687523841858\n",
      "Epoch 60, Loss: 0.9576265811920166\n",
      "Epoch 80, Loss: 0.9479568004608154\n",
      "Epoch 100, Loss: 0.9475486278533936\n",
      "Epoch 120, Loss: 0.947649359703064\n",
      "Epoch 140, Loss: 0.947699248790741\n",
      "Epoch 160, Loss: 0.947820782661438\n",
      "Epoch 180, Loss: 0.9479779005050659\n",
      "Epoch 200, Loss: 0.9481738805770874\n",
      "Epoch 220, Loss: 0.9484161138534546\n",
      "Epoch 240, Loss: 0.9487090110778809\n",
      "Epoch 260, Loss: 0.9490568041801453\n",
      "Epoch 280, Loss: 0.9494643211364746\n",
      "Epoch 300, Loss: 0.9499364495277405\n",
      "Epoch 320, Loss: 0.9504776000976562\n",
      "Epoch 340, Loss: 0.9510926008224487\n",
      "Epoch 360, Loss: 0.9517858624458313\n",
      "Epoch 380, Loss: 0.9525622129440308\n",
      "Epoch 400, Loss: 0.9534258842468262\n",
      "Epoch 420, Loss: 0.9543814659118652\n",
      "Epoch 440, Loss: 0.9554333686828613\n",
      "Epoch 460, Loss: 0.9565858840942383\n",
      "Epoch 480, Loss: 0.9578433632850647\n",
      "weights of  P102 :  [[array([[ 0.45738268, -0.56072825],\n",
      "       [ 0.17599319, -0.04705326],\n",
      "       [-0.56043136,  0.06063863],\n",
      "       [-0.45675004,  0.6350176 ],\n",
      "       [-0.6379189 , -0.8892438 ]], dtype=float32), array([-0.2461653,  0.2461653], dtype=float32)]]\n",
      "Lowest loss of  P102 :  0.9474807\n",
      "Epoch 0, Loss: 0.972480058670044\n",
      "Epoch 20, Loss: 0.9873303174972534\n",
      "Epoch 40, Loss: 1.0651644468307495\n",
      "Epoch 60, Loss: 1.1904478073120117\n",
      "Epoch 80, Loss: 1.3402760028839111\n",
      "Epoch 100, Loss: 1.5056030750274658\n",
      "Epoch 120, Loss: 1.6754350662231445\n",
      "Epoch 140, Loss: 1.8440836668014526\n",
      "Epoch 160, Loss: 2.007171392440796\n",
      "Epoch 180, Loss: 2.161813735961914\n",
      "Epoch 200, Loss: 2.306164503097534\n",
      "Epoch 220, Loss: 2.4390978813171387\n",
      "Epoch 240, Loss: 2.5600779056549072\n",
      "Epoch 260, Loss: 2.669006586074829\n",
      "Epoch 280, Loss: 2.766120672225952\n",
      "Epoch 300, Loss: 2.8519067764282227\n",
      "Epoch 320, Loss: 2.9270293712615967\n",
      "Epoch 340, Loss: 2.9922709465026855\n",
      "Epoch 360, Loss: 3.048480987548828\n",
      "Epoch 380, Loss: 3.09653902053833\n",
      "Epoch 400, Loss: 3.1373178958892822\n",
      "Epoch 420, Loss: 3.171668291091919\n",
      "Epoch 440, Loss: 3.200394630432129\n",
      "Epoch 460, Loss: 3.22424578666687\n",
      "Epoch 480, Loss: 3.2439093589782715\n",
      "weights of  P103 :  [[array([[ 0.82015496, -0.3218913 ],\n",
      "       [-0.8305997 ,  0.0195457 ],\n",
      "       [ 0.63038   ,  0.0061738 ],\n",
      "       [ 0.08742632,  0.93505687],\n",
      "       [-0.7359317 ,  0.27138335]], dtype=float32), array([-0.07830725,  0.07830724], dtype=float32)]]\n",
      "Lowest loss of  P103 :  0.9658555\n",
      "Epoch 0, Loss: 1.12723970413208\n",
      "Epoch 20, Loss: 1.0375243425369263\n",
      "Epoch 40, Loss: 1.0364323854446411\n",
      "Epoch 60, Loss: 1.0411150455474854\n",
      "Epoch 80, Loss: 1.048600196838379\n",
      "Epoch 100, Loss: 1.0593963861465454\n",
      "Epoch 120, Loss: 1.0738755464553833\n",
      "Epoch 140, Loss: 1.0923020839691162\n",
      "Epoch 160, Loss: 1.1148210763931274\n",
      "Epoch 180, Loss: 1.141463279724121\n",
      "Epoch 200, Loss: 1.172166347503662\n",
      "Epoch 220, Loss: 1.2067910432815552\n",
      "Epoch 240, Loss: 1.2451376914978027\n",
      "Epoch 260, Loss: 1.2869571447372437\n",
      "Epoch 280, Loss: 1.3319661617279053\n",
      "Epoch 300, Loss: 1.3798575401306152\n",
      "Epoch 320, Loss: 1.4303100109100342\n",
      "Epoch 340, Loss: 1.4829974174499512\n",
      "Epoch 360, Loss: 1.537594199180603\n",
      "Epoch 380, Loss: 1.5937825441360474\n",
      "Epoch 400, Loss: 1.6512537002563477\n",
      "Epoch 420, Loss: 1.7097129821777344\n",
      "Epoch 440, Loss: 1.768882393836975\n",
      "Epoch 460, Loss: 1.8285012245178223\n",
      "Epoch 480, Loss: 1.8883261680603027\n",
      "weights of  P104 :  [[array([[ 0.60585046,  0.8568549 ],\n",
      "       [-0.66375965, -0.8433424 ],\n",
      "       [ 0.4598961 , -0.11208959],\n",
      "       [-0.78738767, -0.5651101 ],\n",
      "       [-0.34706834,  0.1463605 ]], dtype=float32), array([-0.05379228,  0.05379227], dtype=float32)]]\n",
      "Lowest loss of  P104 :  1.0330272\n",
      "Epoch 0, Loss: 4.6078948974609375\n",
      "Epoch 20, Loss: 2.716876268386841\n",
      "Epoch 40, Loss: 1.1850736141204834\n",
      "Epoch 60, Loss: 0.9181045889854431\n",
      "Epoch 80, Loss: 0.9009652733802795\n",
      "Epoch 100, Loss: 0.8980567455291748\n",
      "Epoch 120, Loss: 0.8929156064987183\n",
      "Epoch 140, Loss: 0.8890240788459778\n",
      "Epoch 160, Loss: 0.8849097490310669\n",
      "Epoch 180, Loss: 0.8806080222129822\n",
      "Epoch 200, Loss: 0.8762329816818237\n",
      "Epoch 220, Loss: 0.8717674612998962\n",
      "Epoch 240, Loss: 0.8672512769699097\n",
      "Epoch 260, Loss: 0.862700879573822\n",
      "Epoch 280, Loss: 0.8581350445747375\n",
      "Epoch 300, Loss: 0.853568971157074\n",
      "Epoch 320, Loss: 0.8490161895751953\n",
      "Epoch 340, Loss: 0.844488263130188\n",
      "Epoch 360, Loss: 0.8399953842163086\n",
      "Epoch 380, Loss: 0.8355466723442078\n",
      "Epoch 400, Loss: 0.8311501145362854\n",
      "Epoch 420, Loss: 0.8268131017684937\n",
      "Epoch 440, Loss: 0.8225421905517578\n",
      "Epoch 460, Loss: 0.8183438181877136\n",
      "Epoch 480, Loss: 0.8142237663269043\n",
      "weights of  P105 :  [[array([[ 0.3012086 , -0.1709413 ],\n",
      "       [ 0.14615184, -0.28827137],\n",
      "       [-0.7272347 ,  0.41941145],\n",
      "       [ 0.21080446, -0.17824702],\n",
      "       [-0.27762872, -0.19509658]], dtype=float32), array([ 0.02906304, -0.02906305], dtype=float32)]]\n",
      "Lowest loss of  P105 :  0.8103876\n",
      "Epoch 0, Loss: 1.1934301853179932\n",
      "Epoch 20, Loss: 0.982725977897644\n",
      "Epoch 40, Loss: 0.961486279964447\n",
      "Epoch 60, Loss: 0.9504633545875549\n",
      "Epoch 80, Loss: 0.9369994401931763\n",
      "Epoch 100, Loss: 0.9306461811065674\n",
      "Epoch 120, Loss: 0.9320108890533447\n",
      "Epoch 140, Loss: 0.9402956962585449\n",
      "Epoch 160, Loss: 0.9558477401733398\n",
      "Epoch 180, Loss: 0.9786108732223511\n",
      "Epoch 200, Loss: 1.0081812143325806\n",
      "Epoch 220, Loss: 1.0440192222595215\n",
      "Epoch 240, Loss: 1.0854899883270264\n",
      "Epoch 260, Loss: 1.1319000720977783\n",
      "Epoch 280, Loss: 1.1825370788574219\n",
      "Epoch 300, Loss: 1.2366926670074463\n",
      "Epoch 320, Loss: 1.2936832904815674\n",
      "Epoch 340, Loss: 1.3528615236282349\n",
      "Epoch 360, Loss: 1.4136252403259277\n",
      "Epoch 380, Loss: 1.47542142868042\n",
      "Epoch 400, Loss: 1.5377492904663086\n",
      "Epoch 420, Loss: 1.6001601219177246\n",
      "Epoch 440, Loss: 1.6622551679611206\n",
      "Epoch 460, Loss: 1.7236852645874023\n",
      "Epoch 480, Loss: 1.7841472625732422\n",
      "weights of  P106 :  [[array([[-0.7744975 , -0.8578904 ],\n",
      "       [ 0.23010951,  0.21321425],\n",
      "       [ 0.58893126,  0.3526627 ],\n",
      "       [-0.6616216 ,  0.20140329],\n",
      "       [ 0.11263284,  0.5052143 ]], dtype=float32), array([-0.00479901,  0.00479899], dtype=float32)]]\n",
      "Lowest loss of  P106 :  0.9303093\n",
      "Epoch 0, Loss: 1.1116204261779785\n",
      "Epoch 20, Loss: 1.0458807945251465\n",
      "Epoch 40, Loss: 1.0002849102020264\n",
      "Epoch 60, Loss: 0.9805233478546143\n",
      "Epoch 80, Loss: 0.9803704023361206\n",
      "Epoch 100, Loss: 0.9942864179611206\n",
      "Epoch 120, Loss: 1.0179357528686523\n",
      "Epoch 140, Loss: 1.047559380531311\n",
      "Epoch 160, Loss: 1.080369472503662\n",
      "Epoch 180, Loss: 1.114422082901001\n",
      "Epoch 200, Loss: 1.1483105421066284\n",
      "Epoch 220, Loss: 1.18096923828125\n",
      "Epoch 240, Loss: 1.211609959602356\n",
      "Epoch 260, Loss: 1.2397024631500244\n",
      "Epoch 280, Loss: 1.2649471759796143\n",
      "Epoch 300, Loss: 1.2872345447540283\n",
      "Epoch 320, Loss: 1.30660080909729\n",
      "Epoch 340, Loss: 1.3231850862503052\n",
      "Epoch 360, Loss: 1.3371970653533936\n",
      "Epoch 380, Loss: 1.3488863706588745\n",
      "Epoch 400, Loss: 1.3585214614868164\n",
      "Epoch 420, Loss: 1.366373062133789\n",
      "Epoch 440, Loss: 1.3727009296417236\n",
      "Epoch 460, Loss: 1.3777467012405396\n",
      "Epoch 480, Loss: 1.3817282915115356\n",
      "weights of  P107 :  [[array([[ 0.7421723 ,  0.47339442],\n",
      "       [ 0.474298  ,  0.25495315],\n",
      "       [-0.6076802 , -0.5668434 ],\n",
      "       [ 0.8003858 ,  0.68511754],\n",
      "       [ 0.3626422 , -0.26606408]], dtype=float32), array([ 0.5669546, -0.5669546], dtype=float32)]]\n",
      "Lowest loss of  P107 :  0.97846174\n",
      "Epoch 0, Loss: 1.288300633430481\n",
      "Epoch 20, Loss: 0.9595440626144409\n",
      "Epoch 40, Loss: 0.9704633951187134\n",
      "Epoch 60, Loss: 0.9693955183029175\n",
      "Epoch 80, Loss: 0.9788100719451904\n",
      "Epoch 100, Loss: 0.9933712482452393\n",
      "Epoch 120, Loss: 1.0133439302444458\n",
      "Epoch 140, Loss: 1.0396318435668945\n",
      "Epoch 160, Loss: 1.0720186233520508\n",
      "Epoch 180, Loss: 1.1102850437164307\n",
      "Epoch 200, Loss: 1.1539909839630127\n",
      "Epoch 220, Loss: 1.2026405334472656\n",
      "Epoch 240, Loss: 1.2557432651519775\n",
      "Epoch 260, Loss: 1.3128528594970703\n",
      "Epoch 280, Loss: 1.3735790252685547\n",
      "Epoch 300, Loss: 1.4375803470611572\n",
      "Epoch 320, Loss: 1.5045560598373413\n",
      "Epoch 340, Loss: 1.5742334127426147\n",
      "Epoch 360, Loss: 1.6463630199432373\n",
      "Epoch 380, Loss: 1.7207109928131104\n",
      "Epoch 400, Loss: 1.7970587015151978\n",
      "Epoch 420, Loss: 1.8752002716064453\n",
      "Epoch 440, Loss: 1.954941749572754\n",
      "Epoch 460, Loss: 2.0361008644104004\n",
      "Epoch 480, Loss: 2.1185059547424316\n",
      "weights of  P108 :  [[array([[ 1.0599285 , -1.1088088 ],\n",
      "       [-0.5179024 , -0.39012867],\n",
      "       [ 0.07373156,  0.43754056],\n",
      "       [ 0.08707507, -0.10887039],\n",
      "       [-0.6715156 , -0.1582762 ]], dtype=float32), array([-0.20991865,  0.20991863], dtype=float32)]]\n",
      "Lowest loss of  P108 :  0.9566351\n",
      "Epoch 0, Loss: 2.6773083209991455\n",
      "Epoch 20, Loss: 1.1517646312713623\n",
      "Epoch 40, Loss: 1.0382659435272217\n",
      "Epoch 60, Loss: 0.9940289855003357\n",
      "Epoch 80, Loss: 0.986236035823822\n",
      "Epoch 100, Loss: 0.9789732694625854\n",
      "Epoch 120, Loss: 0.9717230796813965\n",
      "Epoch 140, Loss: 0.9652209281921387\n",
      "Epoch 160, Loss: 0.9592524170875549\n",
      "Epoch 180, Loss: 0.9539705514907837\n",
      "Epoch 200, Loss: 0.9493147730827332\n",
      "Epoch 220, Loss: 0.945248544216156\n",
      "Epoch 240, Loss: 0.941703200340271\n",
      "Epoch 260, Loss: 0.9386025667190552\n",
      "Epoch 280, Loss: 0.9358705878257751\n",
      "Epoch 300, Loss: 0.933434247970581\n",
      "Epoch 320, Loss: 0.9312284588813782\n",
      "Epoch 340, Loss: 0.9291976094245911\n",
      "Epoch 360, Loss: 0.9272956848144531\n",
      "Epoch 380, Loss: 0.9254868626594543\n",
      "Epoch 400, Loss: 0.9237440228462219\n",
      "Epoch 420, Loss: 0.9220484495162964\n",
      "Epoch 440, Loss: 0.9203880429267883\n",
      "Epoch 460, Loss: 0.9187560081481934\n",
      "Epoch 480, Loss: 0.9171502590179443\n",
      "weights of  P109 :  [[array([[-0.2986525 ,  0.13780601],\n",
      "       [-0.36260888, -0.01257715],\n",
      "       [ 0.5485051 ,  0.54091805],\n",
      "       [-0.665479  , -0.40201697],\n",
      "       [ 0.80683786,  0.34171206]], dtype=float32), array([ 0.32551074, -0.3255109 ], dtype=float32)]]\n",
      "Lowest loss of  P109 :  0.91564983\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Custom loss function\n",
    "class CustomLossLL1(tf.keras.losses.Loss):\n",
    "    def __init__(self, lambda_t, model):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        ws = self.get_weights_from_model()\n",
    "        reg_term = self.regularization_term(ws)\n",
    "        return ce_loss + self.lambda_t * reg_term\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        # return tf.concat([tf.reshape(w, [-1]) for w in model_weights], axis=0)\n",
    "        return model_weights\n",
    "\n",
    "    def regularization_term(self, ws):\n",
    "        reg_term = tf.pow(tf.norm(ws, ord='euclidean'),2)\n",
    "        return reg_term\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True) # Perform a forward pass and compute the predictions\n",
    "        loss = custom_loss(y, y_pred) # Compute the custom loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_weight_LL(X_train, y_train, lambd, num_tier=10, learning_rate = 0.01):\n",
    "    n_classes = np.unique(y_train).size\n",
    "    _, sequential_indices = np.unique(y_train, return_inverse=True)\n",
    "    y_one_hot = tf.keras.utils.to_categorical(sequential_indices, num_classes=n_classes)\n",
    "\n",
    "    lambda_t = lambd  # Regularization parameter\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_classes, input_shape=(X_train.shape[1],), activation='softmax')  # Adjust input_shape to match the number of features in X\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate)\n",
    "    custom_loss = CustomLossLL1(lambda_t, model)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = num_tier\n",
    "    lowest_loss = float('inf')\n",
    "    best_weights = None\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X_train, y_one_hot, custom_loss)\n",
    "        loss_value = loss.numpy()\n",
    "        if loss_value < lowest_loss:\n",
    "            lowest_loss = loss_value\n",
    "            best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    # best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "    return best_weights, lowest_loss\n",
    "\n",
    "def build_clf_params(data, target_subjects ,condition, load_weight):\n",
    "\n",
    "    for sub in data.keys():\n",
    "        if (sub  != target_subjects) and  (sub != target_data_0): #Don't apply weight to target subject\n",
    "            # Where the tranining data is stored\n",
    "            if condition == \"noEA\":\n",
    "                X = data[sub]['Raw_csp']\n",
    "                y = data[sub]['Raw_csp_label']\n",
    "                store_ws = 'ws_Raw'\n",
    "                with open(noEA_wLTL_weight, 'rb') as file:\n",
    "                    loaded_weight = pickle.load(file)\n",
    "\n",
    "            else:\n",
    "                X = data[sub]['EA_csp']\n",
    "                y = data[sub]['EA_csp_label']\n",
    "                store_ws = 'ws_EA'\n",
    "                with open(EA_wLTL_weight, 'rb') as file:\n",
    "                    loaded_weight = pickle.load(file)\n",
    "\n",
    "            if load_weight == False:\n",
    "                weights, loss = train_weight_LL(X_train=X, y_train=y, lambd= 0.1, num_tier=500, learning_rate= 0.01)\n",
    "                print(\"weights of \", str(sub), \": \", weights)\n",
    "                print(\"Lowest loss of \", str(sub), \": \", loss)\n",
    "                data[sub][store_ws] = weights\n",
    "\n",
    "            else:\n",
    "                for sub in data.keys():\n",
    "                    if (sub  != target_subjects) and  (sub != target_data_0): #Don't apply weight to target subject\n",
    "                        data[sub][store_ws] = loaded_weight[sub][store_ws]\n",
    "\n",
    "build_clf_params(CSP2D_Epoch, target_subjects= target_data ,condition = condition_wLTL, load_weight = load_wLTL_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_22216\\3339135240.py:12: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  sigma_P = np.cov(P, rowvar=False)\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2705: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2705: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_22216\\3339135240.py:13: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  sigma_Q = np.cov(Q, rowvar=False)\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2139: RuntimeWarning: invalid value encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n"
     ]
    }
   ],
   "source": [
    "# First define the kl divergence\n",
    "def KL_div(P, Q):\n",
    "    # First convert to np array\n",
    "    P = np.array(P)\n",
    "    Q = np.array(Q)\n",
    "    \n",
    "    # Then compute their means, datain shape of samples x feat\n",
    "    mu_P = np.mean(P, axis=0)\n",
    "    mu_Q = np.mean(Q, axis=0)    \n",
    "\n",
    "    # Compute their covariance\n",
    "    sigma_P = np.cov(P, rowvar=False)\n",
    "    sigma_Q = np.cov(Q, rowvar=False)  \n",
    "\n",
    "    diff = mu_Q - mu_P\n",
    "\n",
    "    inv_sigma_Q = np.linalg.inv(sigma_Q)\n",
    "    term1 = np.dot(np.dot(diff.T, inv_sigma_Q), diff)\n",
    "    \n",
    "    # Calculate the trace term trace(Sigma_Q^{-1} * Sigma_P)\n",
    "    term2 = np.trace(np.dot(inv_sigma_Q, sigma_P))\n",
    "    \n",
    "    # Calculate the determinant term ln(det(Sigma_P) / det(Sigma_Q))\n",
    "    det_sigma0 = np.linalg.det(sigma_P)\n",
    "    det_sigma1 = np.linalg.det(sigma_Q)\n",
    "\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    term3 = np.log((det_sigma0+epsilon) / (det_sigma1+epsilon))\n",
    "    \n",
    "    # print(term3)\n",
    "    \n",
    "    # Dimensionality of the data\n",
    "    K = mu_P.shape[0]\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_div = 0.5 * (term1 + term2 - term3 - K)\n",
    "    \n",
    "    return kl_div\n",
    "\n",
    "# Compute kl divergence of target subject to each source subject\n",
    "def compute_all_kl_div(data, target_subjects , condition):\n",
    "    '''\n",
    "    Parameter:\n",
    "    data, is the whole data containing target and source data\n",
    "    '''\n",
    "    kl_div_score = []\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        target_data = 'Raw_csp'\n",
    "        label_name = 'Raw_csp_label'\n",
    "\n",
    "    else:\n",
    "        target_data = 'EA_csp'\n",
    "        label_name = 'EA_csp_label'\n",
    "        \n",
    "    # cal P from target data\n",
    "    label_tgt =  data[target_subjects][label_name]\n",
    "    P_left =  data[target_subjects][target_data][np.where(label_tgt == 0)]\n",
    "    P_right = data[target_subjects][target_data][np.where(label_tgt == 1)]\n",
    "    P_non = data[target_subjects][target_data][np.where(label_tgt == 2)]\n",
    "    P_feet = data[target_subjects][target_data][np.where(label_tgt == 3)]\n",
    "\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "\n",
    "    #cal Q from each source subject\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            label_src =  data[sub][label_name]\n",
    "            Q_left =  data[sub][target_data][np.where(label_src == 0)]\n",
    "            Q_right = data[sub][target_data][np.where(label_src == 1)]\n",
    "            Q_non = data[sub][target_data][np.where(label_src == 2)]\n",
    "            Q_feet = data[sub][target_data][np.where(label_src == 3)]\n",
    "\n",
    "            kl_left = KL_div(P_left, Q_left)\n",
    "            kl_right = KL_div(P_right, Q_right)\n",
    "            kl_non = KL_div(P_non, Q_non)\n",
    "            kl_feet = KL_div(P_feet, Q_feet)\n",
    "\n",
    "            kl_div_temp = [kl_left, kl_right, kl_non, kl_feet]\n",
    "\n",
    "            kl_div_score.append(kl_div_temp)\n",
    "\n",
    "    data[target_subjects]['kl_div'] = kl_div_score\n",
    "\n",
    "\n",
    "compute_all_kl_div(CSP2D_Epoch, target_subjects=target_data_0 ,condition = condition_wLTL) #target_sub for cal KL is calibrate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.628025485519769e-09, 6.628419833565168e-09, 4.4174225957524534e-10, 8.291452137555778e-09, 5.346648327628779e-09, 2.823794570879222e-08, 1.78903755658172e-08, 3.659235646264757e-09, 1.5140387340438572e-08, 1.9639259232460108e-10, 5.326229051597959e-10, 8.225203872449291e-10, 2.890576109488273e-09, 1.2898771690661835e-10, 2.013570723147923e-09, 9.564049014911657e-09, 8.365726466701272e-10, 5.360088321050377e-11, 9.633163877799854e-11, 1.6380235025780643e-09, 3.0776977567634203e-09, 1.0548389465619387e-09, 3.824617387573564e-08, 3.3804408440165823e-10, 1.814955440502483e-08, 2.500978932388916e-09, 2.9072136933382863e-09, 3.715607909736661e-09, 6.808789695455736e-09, 8.390600179373703e-10, 9.008458122801353e-10, 9.292305876749893e-09, 3.634574072433045e-09, 4.767643636278344e-10, 5.026984462926968e-09, 2.9530464733090455e-08, 2.5793958069482285e-09, 3.3710051054192887e-06, 1.1783470878460895e-08, 1.7227647258671477e-09, 1.3830036901963572e-10, 3.824517785283558e-09, 6.158706193379177e-07, 4.410645965388356e-10, 5.271050813015711e-10, 1.3451122933139095e-10, 1.7835224701569543e-09, 2.3454000528438534e-09, 7.050774275984284e-10, 5.896050909877861e-09, 1.3909543072482245e-06, 4.751113866551717e-12, 9.919929803684823e-08, 1.0967193442242768e-09, 3.975492884625825e-11, 4.902118088964827e-09, 5.36769864231216e-11, 1.4816920398532746e-11, 2.3375947671793066e-09, 3.796697299760146e-10, 3.914057040920057e-10, 4.133179654884275e-12, 4.406846140517098e-09, 1.528327412540966e-09, 1.0940464604894493e-09, 5.032372283603479e-11, 6.208908087688958e-12, 8.233420174680323e-10, 1.0216233002935652e-09, 1.4902846529148494e-10, 7.103877197043049e-10, 7.082520855768647e-12, 3.467644767326215e-08, 5.551654554255767e-09, 4.623577010997911e-08, 5.7555418537090445e-08, 7.23947024373746e-11, 3.811116096815211e-09, 2.7565036733655734e-10, 5.054584054155244e-11, 5.104432609776786e-07, 4.306434920930371e-09, 4.949614705084417e-10, 1.0621128391389534e-09, 4.2990495658876094e-09, 5.317535321953708e-10, 9.815686293995287e-08, 1.0285148898946665e-09, 1.9235725825158282e-11, 4.1041252084454e-10, 5.4593093099292135e-09, 3.5500097063816136e-10, 9.957849747735553e-11, 4.7400219099305135e-09, 1.8268097480072896e-09, 2.535801600528505e-10, 1.0050848912611498e-08, 1.940892801193981e-10, 3.081139117103519e-10, 5.7857411469489523e-11, 4.033915563091133e-11, 4.008934115638528e-10, 3.787010986087595e-10, 1.091298572955986e-11, 1.4586274069828325e-09, 1.708861574805784e-10, 2.0889140716313882e-07, 6.516675574650182e-10, 7.986889648861388e-11, 8.17720122400622e-11, 8.851523915982617e-10, 4.80427909232428e-09, 2.107192186931062e-09, 2.5040913216753082e-08, 5.449528971757392e-09, 3.3552456015199675e-10, 1.2155509414032425e-08, 5.10569890464114e-14]\n",
      "[1.679693122818867e-09, 7.127974599877952e-08, 8.771983095657747e-09, 1.765025022351711e-08, 1.9950642460799264e-08, 9.112848246587914e-09, 1.738912458920889e-08, 7.49374173634258e-09, 4.0106486192570616e-08, 1.1827495580021488e-10, 2.8208965791428475e-10, 4.964190404083262e-11, 6.168310009995116e-10, 1.62168238823405e-09, 6.225285868633069e-09, 3.978834960633439e-09, 6.189655663584742e-10, 2.793550430332357e-11, 1.4523370059763806e-11, 7.055790904849403e-09, 4.7907100513962866e-09, 2.846670378458991e-10, 4.373448270395322e-09, 3.312971753710301e-10, 1.542354659251011e-09, 1.7322899137031743e-09, 1.5917257561205857e-10, 4.801472994711118e-10, 7.200279450481682e-09, 1.8412736929408338e-10, 9.426163837563833e-13, 1.778249205283554e-08, 1.715454797021532e-10, 1.2490138450452886e-08, 1.1936341983876199e-09, 4.670612959068546e-09, 1.3512349454560457e-10, 1.0411740072554845e-06, 3.0542506335241955e-08, 1.3669225126421446e-09, 2.1317234405347013e-10, 1.3600107520708514e-09, 4.028215654404727e-09, 6.500740651912743e-10, 5.4823595926195985e-11, 3.6451211682755397e-11, 6.882965710054197e-11, 5.970961133248046e-10, 2.5806098715202793e-09, 3.303505726987102e-08, 5.539000694828867e-07, 1.4871227794397558e-10, 1.4861001873611382e-08, 9.771987165402854e-09, 3.4185598866042985e-11, 1.165542689993433e-09, 9.989878923952174e-09, 2.0661417522425043e-10, 1.8730932252575443e-10, 1.8987837117980794e-09, 1.2779911588223773e-10, 2.7206206452534945e-11, 4.700496691784261e-08, 6.622553554866719e-08, 2.8751188666796115e-10, 1.8591671754410716e-10, 1.525826820223876e-10, 4.634227834511955e-10, 1.0774306481964767e-09, 2.583417951545865e-10, 3.277983651462535e-08, 1.0644497669206384e-10, 2.335885665528634e-09, 5.691488913127534e-09, 4.4315374247361374e-09, 1.4158178843148129e-07, 1.7503452082046446e-09, 1.9367990246290784e-09, 3.113778974878375e-12, 3.756078587681788e-10, 5.351927123180378e-07, 4.4045090893848045e-10, 6.588436160093489e-09, 5.813986139371155e-09, 2.715344801862724e-09, 5.2946931100136476e-11, 1.0442984967011713e-06, 1.7012735386048477e-09, 5.825964034367197e-11, 7.477324019567357e-11, 7.655413794870252e-10, 1.443699439259602e-09, 1.213506309301813e-10, 1.823843293109456e-10, 1.894112068142977e-10, 1.24815352400585e-11, 3.078733555027198e-10, 8.15519477743208e-11, 3.75891573778924e-10, 8.409396926748294e-11, 4.617545832324197e-11, 1.4638063459165592e-09, 2.15850566132606e-10, 6.796299663015948e-12, 6.339409048047465e-10, 3.5177354230137685e-10, 5.151864879268126e-07, 6.921269940206309e-10, 7.566834951642169e-10, 2.166731315051454e-11, 1.2673304286092017e-08, 1.4357435613577995e-10, 2.8674153373593024e-10, 2.0352462106883863e-07, 8.032386601791377e-09, 1.871423136899875e-10, 2.4835202197887998e-08, 4.589299229857368e-13]\n",
      "[[3.85525276e-04 3.60774137e-04]\n",
      " [9.72373898e-04 1.53098733e-02]\n",
      " [6.48025704e-05 1.88409692e-03]\n",
      " [1.21633690e-03 3.79102213e-03]\n",
      " [7.84340975e-04 4.28511359e-03]\n",
      " [4.14244149e-03 1.95730989e-03]\n",
      " [2.62447682e-03 3.73493607e-03]\n",
      " [5.36801428e-04 1.60954890e-03]\n",
      " [2.22105989e-03 8.61430148e-03]\n",
      " [2.88103402e-05 2.54037744e-05]\n",
      " [7.81345514e-05 6.05888371e-05]\n",
      " [1.20661843e-04 1.06623733e-05]\n",
      " [4.24040847e-04 1.32486506e-04]\n",
      " [1.89222005e-05 3.48314259e-04]\n",
      " [2.95386180e-04 1.33710266e-03]\n",
      " [1.40302393e-03 8.54597028e-04]\n",
      " [1.22723277e-04 1.32944980e-04]\n",
      " [7.86312591e-06 6.00014808e-06]\n",
      " [1.41316292e-05 3.11941285e-06]\n",
      " [2.40294269e-04 1.51548330e-03]\n",
      " [4.51491161e-04 1.02897622e-03]\n",
      " [1.54742440e-04 6.11424216e-05]\n",
      " [5.61062547e-03 9.39354341e-04]\n",
      " [4.95902873e-05 7.11579103e-05]\n",
      " [2.66249776e-03 3.31275793e-04]\n",
      " [3.66887840e-04 3.72071178e-04]\n",
      " [4.26481542e-04 3.41880001e-05]\n",
      " [5.45071109e-04 1.03128795e-04]\n",
      " [9.98833741e-04 1.54651738e-03]\n",
      " [1.23088169e-04 3.95479340e-05]\n",
      " [1.32152002e-04 2.02460561e-07]\n",
      " [1.36315984e-03 3.81942579e-03]\n",
      " [5.33183632e-04 3.68455235e-05]\n",
      " [6.99402323e-05 2.68270368e-03]\n",
      " [7.37447024e-04 2.56375610e-04]\n",
      " [4.33205105e-03 1.00318108e-03]\n",
      " [3.78391414e-04 2.90226003e-05]\n",
      " [4.94518672e-01 2.23629334e-01]\n",
      " [1.72860799e-03 6.56009495e-03]\n",
      " [2.52725611e-04 2.93595469e-04]\n",
      " [2.02883451e-05 4.57863806e-05]\n",
      " [5.61047936e-04 2.92110921e-04]\n",
      " [9.03467991e-02 8.65203297e-04]\n",
      " [6.47031588e-05 1.39626642e-04]\n",
      " [7.73250994e-05 1.17753268e-05]\n",
      " [1.97324870e-05 7.82920063e-06]\n",
      " [2.61638632e-04 1.47836291e-05]\n",
      " [3.44064777e-04 1.28247733e-04]\n",
      " [1.03433232e-04 5.54278212e-04]\n",
      " [8.64937066e-04 7.09545936e-03]\n",
      " [2.04049788e-01 1.18969839e-01]\n",
      " [6.96977444e-07 3.19412773e-05]\n",
      " [1.45523082e-02 3.19193135e-03]\n",
      " [1.60886198e-04 2.09888354e-03]\n",
      " [5.83195635e-06 7.34257929e-06]\n",
      " [7.19129415e-04 2.50341954e-04]\n",
      " [7.87429008e-06 2.14568358e-03]\n",
      " [2.17360804e-06 4.43777794e-05]\n",
      " [3.42919760e-04 4.02313722e-05]\n",
      " [5.56966735e-05 4.07831673e-04]\n",
      " [5.74183138e-05 2.74494282e-05]\n",
      " [6.06327920e-07 5.84350530e-06]\n",
      " [6.46474162e-04 1.00959968e-02]\n",
      " [2.24202106e-04 1.42243009e-02]\n",
      " [1.60494092e-04 6.17534545e-05]\n",
      " [7.38237404e-06 3.99322605e-05]\n",
      " [9.10832493e-07 3.27725849e-05]\n",
      " [1.20782375e-04 9.95366074e-05]\n",
      " [1.49869781e-04 2.31416744e-04]\n",
      " [2.18621321e-05 5.54881347e-05]\n",
      " [1.04212240e-04 7.04064158e-03]\n",
      " [1.03898947e-06 2.28628635e-05]\n",
      " [5.08695488e-03 5.01714941e-04]\n",
      " [8.14414916e-04 1.22245068e-03]\n",
      " [6.78268081e-03 9.51831064e-04]\n",
      " [8.44324712e-03 3.04097498e-02]\n",
      " [1.06201358e-05 3.75949198e-04]\n",
      " [5.59081939e-04 4.15996820e-04]\n",
      " [4.04372730e-05 6.68795334e-07]\n",
      " [7.41495820e-06 8.06752135e-05]\n",
      " [7.48808488e-02 1.14951765e-01]\n",
      " [6.31744068e-04 9.46025762e-05]\n",
      " [7.26097059e-05 1.41510216e-03]\n",
      " [1.55809503e-04 1.24876134e-03]\n",
      " [6.30660654e-04 5.83217354e-04]\n",
      " [7.80070164e-05 1.13722460e-05]\n",
      " [1.43993853e-02 2.24300430e-01]\n",
      " [1.50880762e-04 3.65409303e-04]\n",
      " [2.82183660e-06 1.25133402e-05]\n",
      " [6.02065700e-05 1.60602260e-05]\n",
      " [8.00868081e-04 1.64427375e-04]\n",
      " [5.20778234e-05 3.10086058e-04]\n",
      " [1.46079359e-05 2.60643856e-05]\n",
      " [6.95350279e-04 3.91735539e-05]\n",
      " [2.67988776e-04 4.06828270e-05]\n",
      " [3.71996245e-05 2.68085583e-06]\n",
      " [1.47443635e-03 6.61268078e-05]\n",
      " [2.84724496e-05 1.75161958e-05]\n",
      " [4.51996000e-05 8.07361514e-05]\n",
      " [8.48754879e-06 1.80621858e-05]\n",
      " [5.91766107e-06 9.91783018e-06]\n",
      " [5.88101386e-05 3.14404735e-04]\n",
      " [5.55545775e-05 4.63616245e-05]\n",
      " [1.60090983e-06 1.45974828e-06]\n",
      " [2.13977276e-04 1.36161469e-04]\n",
      " [2.50686051e-05 7.55559421e-05]\n",
      " [3.06438875e-02 1.10654713e-01]\n",
      " [9.55981273e-05 1.48659011e-04]\n",
      " [1.17165829e-05 1.62524828e-04]\n",
      " [1.19957656e-05 4.65382998e-06]\n",
      " [1.29849814e-04 2.72204509e-03]\n",
      " [7.04776660e-04 3.08377249e-05]\n",
      " [3.09120233e-04 6.15879937e-05]\n",
      " [3.67344420e-03 4.37141872e-02]\n",
      " [7.99433328e-04 1.72524213e-03]\n",
      " [4.92206789e-05 4.01955011e-05]\n",
      " [1.78318519e-03 5.33424739e-03]\n",
      " [7.48994251e-09 9.85716050e-08]]\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_weights(data, target_subjects):\n",
    "    kl = data[target_subjects]['kl_div']\n",
    "    KL_inv_left = []\n",
    "    KL_inv_right = []\n",
    "    KL_inv_non = []\n",
    "    KL_inv_feet = []\n",
    "\n",
    "    alpha_s = []\n",
    "    eps = 0.0001\n",
    "    \n",
    "    #equation (9)\n",
    "    for val in kl:\n",
    "        if val != 0: \n",
    "            KL_inv_left.append(1/((val[0] + eps)**4))\n",
    "            KL_inv_right.append(1/((val[1] + eps)**4))\n",
    "            KL_inv_non.append(1/((val[2] + eps)**4))\n",
    "            KL_inv_feet.append(1/((val[3] + eps)**4))\n",
    "\n",
    "    print(KL_inv_left)\n",
    "    print(KL_inv_right)\n",
    "    \n",
    "    for i in range(0,len(KL_inv_left)):\n",
    "        temp = [KL_inv_left[i]/sum(KL_inv_left), KL_inv_right[i]/sum(KL_inv_right), KL_inv_non[i]/sum(KL_inv_non), KL_inv_feet[i]/sum(KL_inv_feet)]\n",
    "        alpha_s.append(temp)\n",
    "\n",
    "    alpha_s = np.array(alpha_s)\n",
    "    print(np.array(alpha_s[:, ~np.isnan(alpha_s).any(axis=0)]))\n",
    "    data[target_subjects]['alpha_s'] = alpha_s[:, ~np.isnan(alpha_s).any(axis=0)]\n",
    "\n",
    "compute_similarity_weights(CSP2D_Epoch, target_subjects=target_data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.54487333 -0.08484746]\n",
      " [-0.35785604  0.07279549]\n",
      " [-0.01531437 -0.05386171]\n",
      " [ 0.27785029 -0.05203541]\n",
      " [ 0.00302888  0.18399436]]\n",
      "[[ 35.43430115 -23.42980642  -0.45842971  18.11544951  -1.62082829]\n",
      " [-23.42980642  15.55118557   0.17073299 -11.99511116   1.42163   ]\n",
      " [ -0.45842971   0.17073299   0.41970051  -0.15791036  -1.17509388]\n",
      " [ 18.11544951 -11.99511116  -0.15791036   9.30937201  -1.03345245]\n",
      " [ -1.62082829   1.42163     -1.17509388  -1.03345245   3.97061088]]\n"
     ]
    }
   ],
   "source": [
    "def compute_ETL_and_mu_ws(data, target_subjects, condition):\n",
    "\n",
    "    mu_ws = 0\n",
    "    temp_ws = 0\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        ws_name = 'ws_Raw'\n",
    "    else:\n",
    "        ws_name = 'ws_EA'\n",
    "\n",
    "    alpha_s = np.array(data[target_subjects]['alpha_s'])\n",
    "\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "    index_count = 0\n",
    "\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            ws = data[sub][ws_name][0][0]\n",
    "            mu_ws += ws * alpha_s[index_count]\n",
    "            index_count += 1\n",
    "\n",
    "    print(np.array(mu_ws))\n",
    "\n",
    "    index_count = 0\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            ws = data[sub][ws_name][0][0]\n",
    "            ws_min_mu = np.dot(((ws * alpha_s[index_count]) - mu_ws), np.transpose((ws * alpha_s[index_count]) - mu_ws))\n",
    "            temp_ws += ws_min_mu #equation (11)\n",
    "            index_count += 1\n",
    "\n",
    "    print(np.array(temp_ws))\n",
    "\n",
    "    den = temp_ws\n",
    "    nom = np.trace(temp_ws) #Return the sum along diagonals of the array.\n",
    "    Sigma_TL = den/nom\n",
    "\n",
    "    data[target_subjects]['Sigma_TL'] = Sigma_TL\n",
    "    data[target_subjects]['mu_ws'] = mu_ws\n",
    "\n",
    "compute_ETL_and_mu_ws(CSP2D_Epoch, target_subjects = target_data_0, condition=condition_wLTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(5, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(CSP2D_Epoch[target_data_0]['Sigma_TL']).shape)\n",
    "print(np.array(CSP2D_Epoch[target_data_0]['mu_ws']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: -1289.6744384765625\n",
      "Epoch 20, Loss: -689.5081176757812\n",
      "Epoch 40, Loss: -746.0029907226562\n",
      "Epoch 60, Loss: -717.7683715820312\n",
      "Epoch 80, Loss: -731.6109619140625\n",
      "Epoch 100, Loss: -721.7869873046875\n",
      "Epoch 120, Loss: -719.4027099609375\n",
      "Epoch 140, Loss: -716.6469116210938\n",
      "Epoch 160, Loss: -713.1688232421875\n",
      "Epoch 180, Loss: -709.8173217773438\n",
      "Epoch 200, Loss: -706.6131591796875\n",
      "Epoch 220, Loss: -703.5489501953125\n",
      "Epoch 240, Loss: -700.6712646484375\n",
      "Epoch 260, Loss: -698.0281982421875\n",
      "Epoch 280, Loss: -695.6593627929688\n",
      "Epoch 300, Loss: -693.5963134765625\n",
      "Epoch 320, Loss: -691.864990234375\n",
      "Epoch 340, Loss: -690.4857177734375\n",
      "Epoch 360, Loss: -689.4723510742188\n",
      "Epoch 380, Loss: -688.8333129882812\n",
      "Epoch 400, Loss: -688.5718383789062\n",
      "Epoch 420, Loss: -688.68505859375\n",
      "Epoch 440, Loss: -689.1675415039062\n",
      "Epoch 460, Loss: -690.0081176757812\n",
      "Epoch 480, Loss: -691.1929931640625\n",
      "Epoch 500, Loss: -692.7039794921875\n",
      "Epoch 520, Loss: -694.5206298828125\n",
      "Epoch 540, Loss: -696.6209716796875\n",
      "Epoch 560, Loss: -698.9779663085938\n",
      "Epoch 580, Loss: -701.5660400390625\n",
      "Epoch 600, Loss: -704.356689453125\n",
      "Epoch 620, Loss: -707.3208618164062\n",
      "Epoch 640, Loss: -710.4283447265625\n",
      "Epoch 660, Loss: -713.6486206054688\n",
      "Epoch 680, Loss: -716.9511108398438\n",
      "Epoch 700, Loss: -720.3035888671875\n",
      "Epoch 720, Loss: -723.6766357421875\n",
      "Epoch 740, Loss: -727.0388793945312\n",
      "Epoch 760, Loss: -730.3597412109375\n",
      "Epoch 780, Loss: -733.6094970703125\n",
      "Epoch 800, Loss: -736.7587280273438\n",
      "Epoch 820, Loss: -739.7786865234375\n",
      "Epoch 840, Loss: -742.6409912109375\n",
      "Epoch 860, Loss: -745.3189697265625\n",
      "Epoch 880, Loss: -747.785888671875\n",
      "Epoch 900, Loss: -750.0174560546875\n",
      "Epoch 920, Loss: -751.9859619140625\n",
      "Epoch 940, Loss: -753.6707763671875\n",
      "Epoch 960, Loss: -755.0472412109375\n",
      "Epoch 980, Loss: -756.0945434570312\n",
      "Epoch 1000, Loss: -756.7926635742188\n",
      "Epoch 1020, Loss: -757.1190795898438\n",
      "Epoch 1040, Loss: -757.05615234375\n",
      "Epoch 1060, Loss: -756.5857543945312\n",
      "Epoch 1080, Loss: -755.6905517578125\n",
      "Epoch 1100, Loss: -754.3538208007812\n",
      "Epoch 1120, Loss: -752.5609130859375\n",
      "Epoch 1140, Loss: -750.2960815429688\n",
      "Epoch 1160, Loss: -747.5471801757812\n",
      "Epoch 1180, Loss: -744.29931640625\n",
      "Epoch 1200, Loss: -740.5419921875\n",
      "Epoch 1220, Loss: -736.26318359375\n",
      "Epoch 1240, Loss: -731.45166015625\n",
      "Epoch 1260, Loss: -726.098388671875\n",
      "Epoch 1280, Loss: -720.1962890625\n",
      "Epoch 1300, Loss: -713.7326049804688\n",
      "Epoch 1320, Loss: -706.7022094726562\n",
      "Epoch 1340, Loss: -699.0965576171875\n",
      "Epoch 1360, Loss: -690.9119873046875\n",
      "Epoch 1380, Loss: -682.1410522460938\n",
      "Epoch 1400, Loss: -672.7804565429688\n",
      "Epoch 1420, Loss: -662.8245849609375\n",
      "Epoch 1440, Loss: -652.2677612304688\n",
      "Epoch 1460, Loss: -641.1095581054688\n",
      "Epoch 1480, Loss: -629.3460083007812\n",
      "Epoch 1500, Loss: -616.9758911132812\n",
      "Epoch 1520, Loss: -603.9971923828125\n",
      "Epoch 1540, Loss: -590.4085083007812\n",
      "Epoch 1560, Loss: -576.2083740234375\n",
      "Epoch 1580, Loss: -561.3995361328125\n",
      "Epoch 1600, Loss: -545.9784545898438\n",
      "Epoch 1620, Loss: -529.9490356445312\n",
      "Epoch 1640, Loss: -513.31298828125\n",
      "Epoch 1660, Loss: -496.0694885253906\n",
      "Epoch 1680, Loss: -478.2220764160156\n",
      "Epoch 1700, Loss: -459.7763671875\n",
      "Epoch 1720, Loss: -440.73272705078125\n",
      "Epoch 1740, Loss: -421.09033203125\n",
      "Epoch 1760, Loss: -400.8584899902344\n",
      "Epoch 1780, Loss: -380.041015625\n",
      "Epoch 1800, Loss: -358.64013671875\n",
      "Epoch 1820, Loss: -336.66180419921875\n",
      "Epoch 1840, Loss: -314.11016845703125\n",
      "Epoch 1860, Loss: -290.99432373046875\n",
      "Epoch 1880, Loss: -267.3179626464844\n",
      "Epoch 1900, Loss: -243.0856475830078\n",
      "Epoch 1920, Loss: -218.30517578125\n",
      "Epoch 1940, Loss: -192.9822235107422\n",
      "Epoch 1960, Loss: -167.1277313232422\n",
      "Epoch 1980, Loss: -140.74618530273438\n",
      "Epoch 2000, Loss: -113.84501647949219\n",
      "Epoch 2020, Loss: -86.4320297241211\n",
      "Epoch 2040, Loss: -58.521583557128906\n",
      "Epoch 2060, Loss: -30.10986328125\n",
      "Epoch 2080, Loss: -1.213964819908142\n",
      "Epoch 2100, Loss: 28.160058975219727\n",
      "Epoch 2120, Loss: 57.999610900878906\n",
      "Epoch 2140, Loss: 88.3021469116211\n",
      "Epoch 2160, Loss: 119.0537109375\n",
      "Epoch 2180, Loss: 150.24667358398438\n",
      "Epoch 2200, Loss: 181.8689422607422\n",
      "Epoch 2220, Loss: 213.9138641357422\n",
      "Epoch 2240, Loss: 246.365234375\n",
      "Epoch 2260, Loss: 279.21502685546875\n",
      "Epoch 2280, Loss: 312.46014404296875\n",
      "Epoch 2300, Loss: 346.08380126953125\n",
      "Epoch 2320, Loss: 380.07049560546875\n",
      "Epoch 2340, Loss: 414.42010498046875\n",
      "Epoch 2360, Loss: 449.1146545410156\n",
      "Epoch 2380, Loss: 484.15118408203125\n",
      "Epoch 2400, Loss: 519.5054931640625\n",
      "Epoch 2420, Loss: 555.1804809570312\n",
      "Epoch 2440, Loss: 591.1554565429688\n",
      "Epoch 2460, Loss: 627.42578125\n",
      "Epoch 2480, Loss: 663.9675903320312\n",
      "Epoch 2500, Loss: 700.7799072265625\n",
      "Epoch 2520, Loss: 737.8568115234375\n",
      "Epoch 2540, Loss: 775.1810302734375\n",
      "Epoch 2560, Loss: 812.7445068359375\n",
      "Epoch 2580, Loss: 850.5208740234375\n",
      "Epoch 2600, Loss: 888.5260009765625\n",
      "Epoch 2620, Loss: 926.7222900390625\n",
      "Epoch 2640, Loss: 965.1062622070312\n",
      "Epoch 2660, Loss: 1003.6652221679688\n",
      "Epoch 2680, Loss: 1042.3870849609375\n",
      "Epoch 2700, Loss: 1081.2626953125\n",
      "Epoch 2720, Loss: 1120.2913818359375\n",
      "Epoch 2740, Loss: 1159.4495849609375\n",
      "Epoch 2760, Loss: 1198.7301025390625\n",
      "Epoch 2780, Loss: 1238.1129150390625\n",
      "Epoch 2800, Loss: 1277.61328125\n",
      "Epoch 2820, Loss: 1317.203125\n",
      "Epoch 2840, Loss: 1356.848388671875\n",
      "Epoch 2860, Loss: 1396.5679931640625\n",
      "Epoch 2880, Loss: 1436.3355712890625\n",
      "Epoch 2900, Loss: 1476.1585693359375\n",
      "Epoch 2920, Loss: 1516.008544921875\n",
      "Epoch 2940, Loss: 1555.9000244140625\n",
      "Epoch 2960, Loss: 1595.790283203125\n",
      "Epoch 2980, Loss: 1635.68359375\n",
      "weights of  pipo :  [[array([[ 3.184887  , -2.2072887 ],\n",
      "       [-3.4972446 ,  2.82359   ],\n",
      "       [ 0.98292065, -2.0011332 ],\n",
      "       [ 0.04168822,  0.76558346],\n",
      "       [ 0.10138874, -0.53327566]], dtype=float32), array([ 0.52768487, -0.5276851 ], dtype=float32)]]\n",
      "loss of  pipo :  0.24316406\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84        10\n",
      "           1       0.82      0.90      0.86        10\n",
      "\n",
      "    accuracy                           0.85        20\n",
      "   macro avg       0.85      0.85      0.85        20\n",
      "weighted avg       0.85      0.85      0.85        20\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[8 2]\n",
      " [1 9]]\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.80      0.86        20\n",
      "           1       0.83      0.95      0.88        20\n",
      "\n",
      "    accuracy                           0.88        40\n",
      "   macro avg       0.88      0.88      0.87        40\n",
      "weighted avg       0.88      0.88      0.87        40\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[16  4]\n",
      " [ 1 19]]\n"
     ]
    }
   ],
   "source": [
    "# Custom loss function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "class CustomLossLL2(tf.keras.losses.Loss):\n",
    "    \n",
    "    def __init__(self, lambda_t, model, mu, sigma_TL):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "        self.mu = tf.convert_to_tensor(mu, dtype=tf.float32)\n",
    "        self.sigma_TL = tf.convert_to_tensor(sigma_TL, dtype=tf.float32)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        wt = self.get_weights_from_model()\n",
    "        reg_term = self.regularization_term(wt)\n",
    "\n",
    "        return ce_loss + (self.lambda_t * tf.linalg.matmul(reg_term, wt))\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        return model_weights\n",
    "\n",
    "    def regularization_term(self, wt):\n",
    "        diff = wt - self.mu\n",
    "        reg_term = 0.5 * tf.linalg.matmul(tf.linalg.matmul(tf.linalg.inv(self.sigma_TL), diff[0]), tf.transpose(diff[0]))\n",
    "        reg_term += 0.5 * tf.math.log(tf.linalg.det(self.sigma_TL))\n",
    "        return reg_term\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True) # Perform a forward pass and compute the predictions\n",
    "        loss = custom_loss(y, y_pred) # Compute the custom loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def train_weight_LL2(X_train, y_train, lambd, mu, sigma_TL, num_tier=10, learning_rate = 0.01):\n",
    "    n_classes = np.unique(y_train).size\n",
    "    _, sequential_indices = np.unique(y_train, return_inverse=True)\n",
    "    y_one_hot = tf.keras.utils.to_categorical(sequential_indices, num_classes=n_classes)\n",
    "\n",
    "    lambda_t = lambd  # Regularization parameter\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_classes, input_shape=(X_train.shape[1],), activation='softmax')  # Adjust input_shape to match the number of features in X\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate)\n",
    "    custom_loss = CustomLossLL2(lambda_t, model, mu, sigma_TL)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = num_tier\n",
    "    lowest_loss = float('inf')\n",
    "    best_weights = None\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X_train, y_one_hot, custom_loss)\n",
    "        loss_value = loss.numpy()\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        if (abs(loss_value) < lowest_loss):\n",
    "            lowest_loss = abs(loss_value)\n",
    "            best_model = model\n",
    "            best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "    return best_model, best_weights, lowest_loss\n",
    "\n",
    "def GetConfusionMatrix(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_prob = model.predict(X_train)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    _, y_train = np.unique(y_train, return_inverse=True)\n",
    "\n",
    "\n",
    "    print(\"Classification TRAIN DATA \\n=======================\")\n",
    "    print(classification_report(y_true= y_train, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true= y_train, y_pred=y_pred))\n",
    "\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    _, y_test = np.unique(y_test, return_inverse=True)\n",
    "    \n",
    "    print(\"Classification TEST DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "\n",
    "def tgt_test_wLTL(data, target_subjects ,condition):\n",
    "        tgt_data = target_subjects + \"_test\"\n",
    "\n",
    "        if condition == \"noEA\":\n",
    "            X = data[target_subjects]['Raw_csp']\n",
    "            y = data[target_subjects]['Raw_csp_label']\n",
    "            X_test = data[tgt_data]['Raw_csp']\n",
    "            y_test = data[tgt_data]['Raw_csp_label']\n",
    "            store_ws = 'wt_Raw'\n",
    "\n",
    "        else:\n",
    "            X = data[target_subjects]['EA_csp']\n",
    "            y = data[target_subjects]['EA_csp_label']\n",
    "            X_test = data[tgt_data]['EA_csp']\n",
    "            y_test = data[tgt_data]['EA_csp_label']\n",
    "            store_ws = 'wt_EA'\n",
    "\n",
    "        mu = data[target_subjects]['mu_ws']\n",
    "        sigma_TL = data[target_subjects]['Sigma_TL']\n",
    "\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "        \n",
    "        model, weights, loss = train_weight_LL2(X_train=X_train, y_train=y_train, mu =mu, sigma_TL=sigma_TL, lambd= 1, num_tier=3000, learning_rate= 0.01)\n",
    "        print(\"weights of \", str(target_subjects), \": \", weights)\n",
    "        print(\"loss of \", str(target_subjects), \": \", loss)\n",
    "        data[target_subjects][store_ws] = weights\n",
    "\n",
    "        GetConfusionMatrix(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "tgt_test_wLTL(CSP2D_Epoch, target_subjects= target_data_0 ,condition = condition_wLTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save wLTL weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del CSP2D_Epoch[target_data]\n",
    "# del CSP2D_Epoch[target_data_0]\n",
    "\n",
    "# # Save the dictionary\n",
    "# with open('noEA_wLTL_LR.pkl', 'wb') as file:\n",
    "#     pickle.dump(CSP2D_Epoch, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
