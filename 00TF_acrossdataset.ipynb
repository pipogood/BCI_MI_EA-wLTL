{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_utilize import Unicorn\n",
    "from OtherData_utilize import Physionet, BCIcompet2a\n",
    "\n",
    "target_class = [\"Left\", \"Right\"]  #adjust selective classes\n",
    "num_subject_physionet = 109\n",
    "\n",
    "load_wLTL_weight = False  #saved weight of 109 physionet subject\n",
    "condition_wLTL = \"EA\"\n",
    "EA_wLTL_weight = \"EA_wLTL_LR.pkl\"\n",
    "noEA_wLTL_weight = \"noEA_wLTL_LR.pkl\"\n",
    "\n",
    "AllBCIClass = Unicorn(selectclass = target_class, desired_fz = 128, ch_pick = ['Fz','C3','Cz','C4','Pz'])\n",
    "Class_compet = BCIcompet2a(selectclass = target_class, desired_fz = 128, ch_pick = ['EEG-Fz', 'EEG-Cz', 'EEG-C3', 'EEG-C4', 'EEG-Pz'])\n",
    "Class_Physio = Physionet(selectclass = target_class, desired_fz = 128, ch_pick =  ['Fz..','C3..', 'Cz..','C4..','Pz..'])\n",
    "\n",
    "target_data_0 = \"AJpang\" #this subject will be test_set otherwise are train_set\n",
    "calibrate_size = 20 # (trial)\n",
    "train_svm = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicorn hybrid black dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful to create Data of ['AJpang']\n",
      "Used Annotations descriptions: ['OVTK_GDF_Cross_On_Screen', 'OVTK_GDF_Left', 'OVTK_GDF_Right', 'OVTK_GDF_Tongue', 'OVTK_GDF_Up']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\main_utilize.py:97: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].get_data()\n"
     ]
    }
   ],
   "source": [
    "subjectForTargetData = [target_data_0]\n",
    "# subjectForTargetData = \"all\"\n",
    "\n",
    "EEG_data = AllBCIClass.GetRawEDF(target_subjects= subjectForTargetData, condition=\"Offline_Experiment\")\n",
    "Unicorn_Epochs = AllBCIClass.GetEpoch(EEG_data ,tmin= -2.0, tmax= 6.0, crop = (0,4) ,baseline= (-0.5,0.0), band_pass=(6,32),trial_removal_th = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCI Compettition 2a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n",
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n",
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:259: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:259: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '32766', '768', '769', '770', '771', '772']\n",
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:259: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:259: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n",
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:259: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:259: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:259: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:259: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:259: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    }
   ],
   "source": [
    "EEG_data = Class_compet.GetRaw(target_subjects=\"all\", preload=True)\n",
    "EEG_compet_Epochs = Class_compet.GetEpoch(EEG_data ,tmin= -1.0, tmax= 4.0, crop = (0,4) ,baseline=(-0.5,0.0),band_pass=(6,32),trial_removal_th = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physionet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing subject number:  1\n",
      "processing subject number:  2\n",
      "processing subject number:  3\n",
      "processing subject number:  4\n",
      "processing subject number:  5\n",
      "processing subject number:  6\n",
      "processing subject number:  7\n",
      "processing subject number:  8\n",
      "processing subject number:  9\n",
      "processing subject number:  10\n",
      "processing subject number:  11\n",
      "processing subject number:  12\n",
      "processing subject number:  13\n",
      "processing subject number:  14\n",
      "processing subject number:  15\n",
      "processing subject number:  16\n",
      "processing subject number:  17\n",
      "processing subject number:  18\n",
      "processing subject number:  19\n",
      "processing subject number:  20\n",
      "processing subject number:  21\n",
      "processing subject number:  22\n",
      "processing subject number:  23\n",
      "processing subject number:  24\n",
      "processing subject number:  25\n",
      "processing subject number:  26\n",
      "processing subject number:  27\n",
      "processing subject number:  28\n",
      "processing subject number:  29\n",
      "processing subject number:  30\n",
      "processing subject number:  31\n",
      "processing subject number:  32\n",
      "processing subject number:  33\n",
      "processing subject number:  34\n",
      "processing subject number:  35\n",
      "processing subject number:  36\n",
      "processing subject number:  37\n",
      "processing subject number:  38\n",
      "processing subject number:  39\n",
      "processing subject number:  40\n",
      "processing subject number:  41\n",
      "processing subject number:  42\n",
      "processing subject number:  43\n",
      "processing subject number:  44\n",
      "processing subject number:  45\n",
      "processing subject number:  46\n",
      "processing subject number:  47\n",
      "processing subject number:  48\n",
      "processing subject number:  49\n",
      "processing subject number:  50\n",
      "processing subject number:  51\n",
      "processing subject number:  52\n",
      "processing subject number:  53\n",
      "processing subject number:  54\n",
      "processing subject number:  55\n",
      "processing subject number:  56\n",
      "processing subject number:  57\n",
      "processing subject number:  58\n",
      "processing subject number:  59\n",
      "processing subject number:  60\n",
      "processing subject number:  61\n",
      "processing subject number:  62\n",
      "processing subject number:  63\n",
      "processing subject number:  64\n",
      "processing subject number:  65\n",
      "processing subject number:  66\n",
      "processing subject number:  67\n",
      "processing subject number:  68\n",
      "processing subject number:  69\n",
      "processing subject number:  70\n",
      "processing subject number:  71\n",
      "processing subject number:  72\n",
      "processing subject number:  73\n",
      "processing subject number:  74\n",
      "processing subject number:  75\n",
      "processing subject number:  76\n",
      "processing subject number:  77\n",
      "processing subject number:  78\n",
      "processing subject number:  79\n",
      "processing subject number:  80\n",
      "processing subject number:  81\n",
      "processing subject number:  82\n",
      "processing subject number:  83\n",
      "processing subject number:  84\n",
      "processing subject number:  85\n",
      "processing subject number:  86\n",
      "processing subject number:  87\n",
      "processing subject number:  88\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "processing subject number:  89\n",
      "processing subject number:  90\n",
      "processing subject number:  91\n",
      "processing subject number:  92\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "processing subject number:  93\n",
      "processing subject number:  94\n",
      "processing subject number:  95\n",
      "processing subject number:  96\n",
      "processing subject number:  97\n",
      "processing subject number:  98\n",
      "processing subject number:  99\n",
      "processing subject number:  100\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "Sampling frequency of the instance is already 128.0, returning unmodified.\n",
      "processing subject number:  101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:65: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw_RorL1 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R04.edf\",preload = True, verbose=False)\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:66: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw_RorL2 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R08.edf\",preload = True, verbose=False)\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:67: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw_RorL3 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R12.edf\",preload = True, verbose=False)\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:78: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw_Both1 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R06.edf\",preload = True, verbose=False)\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:79: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw_Both2 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R10.edf\",preload = True, verbose=False)\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:80: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw_Both3 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R14.edf\",preload = True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing subject number:  102\n",
      "processing subject number:  103\n",
      "processing subject number:  104\n",
      "processing subject number:  105\n",
      "processing subject number:  106\n",
      "processing subject number:  107\n",
      "processing subject number:  108\n",
      "processing subject number:  109\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "148 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "148 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "P059 -318.4516872532656 15\n",
      "P059 1038.9442350925492 15\n",
      "P059 -1074.055918632143 16\n",
      "P059 299.1623359364581 16\n",
      "P059 -1196.6689362296906 20\n",
      "P059 274.9148497256546 20\n",
      "P059 -1147.5470456986693 70\n",
      "P059 178.30133611108454 70\n",
      "P059 -1104.3980624949231 97\n",
      "P059 81.10133720571451 97\n",
      "P059 -1091.8251635226216 131\n",
      "P059 20.68441134035783 131\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "148 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "147 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "148 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "P084 -1019.3821703246222 141\n",
      "P084 774.6617274743861 141\n",
      "P084 -1070.1889576620583 142\n",
      "P084 1020.7164674663519 142\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "188 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "187 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "P095 -142.26261475457747 118\n",
      "P095 1047.8370281991654 118\n",
      "P095 -148.75865329411093 119\n",
      "P095 1006.5569166385028 119\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "120 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "148 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:138: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "c:\\git\\BCI_MI_Study\\OtherData_utilize.py:140: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    }
   ],
   "source": [
    "RAW_data_RorL, RAW_data_Both = Class_Physio.GetRaw(num_subject= num_subject_physionet)\n",
    "EEG_physio_Epochs = Class_Physio.Get_epoch(RAW_data_RorL, RAW_data_Both, tmin=-2.0, tmax=4.0, crop=(0,4),baseline = (-0.5,0.0),band_pass= (6,32),trial_removal_th = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all dictionaries into a single dictionary\n",
    "combined_epochs = {**Unicorn_Epochs, **EEG_compet_Epochs, **EEG_physio_Epochs}\n",
    "calibrate_size = calibrate_size / combined_epochs[target_data_0]['Raw_Epoch'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear Memory\n",
    "del Unicorn_Epochs\n",
    "del EEG_compet_Epochs\n",
    "del EEG_physio_Epochs\n",
    "del RAW_data_RorL\n",
    "del RAW_data_Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllBCIClass.ComputeEA(combined_epochs, target_subject= target_data_0, calibrate_size=calibrate_size)\n",
    "if calibrate_size != 0:\n",
    "    target_data = target_data_0 + \"_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "CSP2D_Epoch = AllBCIClass.computeCSPFeatures(combined_epochs, target_subject = target_data , target_subject_0= target_data_0) #For wLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train wLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't make the python class yet because I need to check the correction of this purpose later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 8.447275161743164\n",
      "Epoch 20, Loss: 3.3128178119659424\n",
      "Epoch 40, Loss: 1.121471643447876\n",
      "Epoch 60, Loss: 0.8851298093795776\n",
      "Epoch 80, Loss: 0.8437283039093018\n",
      "Epoch 100, Loss: 0.8390390276908875\n",
      "Epoch 120, Loss: 0.839455246925354\n",
      "Epoch 140, Loss: 0.840056836605072\n",
      "Epoch 160, Loss: 0.8408677577972412\n",
      "Epoch 180, Loss: 0.84173983335495\n",
      "Epoch 200, Loss: 0.8426751494407654\n",
      "Epoch 220, Loss: 0.8436890840530396\n",
      "Epoch 240, Loss: 0.8447816967964172\n",
      "Epoch 260, Loss: 0.8459533452987671\n",
      "Epoch 280, Loss: 0.8472057580947876\n",
      "Epoch 300, Loss: 0.8485410213470459\n",
      "Epoch 320, Loss: 0.8499611616134644\n",
      "Epoch 340, Loss: 0.8514683842658997\n",
      "Epoch 360, Loss: 0.8530647158622742\n",
      "Epoch 380, Loss: 0.8547523617744446\n",
      "Epoch 400, Loss: 0.8565333485603333\n",
      "Epoch 420, Loss: 0.8584100008010864\n",
      "Epoch 440, Loss: 0.860384464263916\n",
      "Epoch 460, Loss: 0.8624587059020996\n",
      "Epoch 480, Loss: 0.8646349906921387\n",
      "weights of  A01 :  [array([[ 0.5022021 , -0.5772616 ],\n",
      "       [-0.6905646 ,  0.21191064],\n",
      "       [ 0.40474373, -0.06322531],\n",
      "       [-0.28040874,  0.46766186],\n",
      "       [ 0.44552186, -0.4245998 ]], dtype=float32), array([ 0.3326747, -0.3326747], dtype=float32)]\n",
      "Lowest loss of  A01 :  0.8369114\n",
      "Epoch 0, Loss: 1.544693112373352\n",
      "Epoch 20, Loss: 0.8621342778205872\n",
      "Epoch 40, Loss: 0.8432715535163879\n",
      "Epoch 60, Loss: 0.8420392870903015\n",
      "Epoch 80, Loss: 0.8405337333679199\n",
      "Epoch 100, Loss: 0.8390308618545532\n",
      "Epoch 120, Loss: 0.8373945951461792\n",
      "Epoch 140, Loss: 0.8356516361236572\n",
      "Epoch 160, Loss: 0.833820104598999\n",
      "Epoch 180, Loss: 0.8319154381752014\n",
      "Epoch 200, Loss: 0.8299537897109985\n",
      "Epoch 220, Loss: 0.8279486298561096\n",
      "Epoch 240, Loss: 0.8259128928184509\n",
      "Epoch 260, Loss: 0.8238586783409119\n",
      "Epoch 280, Loss: 0.8217973709106445\n",
      "Epoch 300, Loss: 0.8197394013404846\n",
      "Epoch 320, Loss: 0.817694902420044\n",
      "Epoch 340, Loss: 0.8156734108924866\n",
      "Epoch 360, Loss: 0.8136836290359497\n",
      "Epoch 380, Loss: 0.8117340207099915\n",
      "Epoch 400, Loss: 0.8098324537277222\n",
      "Epoch 420, Loss: 0.8079863786697388\n",
      "Epoch 440, Loss: 0.8062028288841248\n",
      "Epoch 460, Loss: 0.8044881224632263\n",
      "Epoch 480, Loss: 0.8028483986854553\n",
      "weights of  A02 :  [array([[ 0.10945864, -0.26673204],\n",
      "       [-0.47182247, -0.08160455],\n",
      "       [ 0.3059964 ,  0.46422255],\n",
      "       [ 0.31021404, -0.203893  ],\n",
      "       [ 0.026336  ,  0.5684702 ]], dtype=float32), array([-0.01766341,  0.0176634 ], dtype=float32)]\n",
      "Lowest loss of  A02 :  0.8013653\n",
      "Epoch 0, Loss: 6.079708576202393\n",
      "Epoch 20, Loss: 1.2302830219268799\n",
      "Epoch 40, Loss: 1.0382407903671265\n",
      "Epoch 60, Loss: 1.013982892036438\n",
      "Epoch 80, Loss: 1.0024391412734985\n",
      "Epoch 100, Loss: 0.9874022603034973\n",
      "Epoch 120, Loss: 0.973902702331543\n",
      "Epoch 140, Loss: 0.9602421522140503\n",
      "Epoch 160, Loss: 0.9467242956161499\n",
      "Epoch 180, Loss: 0.933558464050293\n",
      "Epoch 200, Loss: 0.9209519624710083\n",
      "Epoch 220, Loss: 0.9090778827667236\n",
      "Epoch 240, Loss: 0.8980809450149536\n",
      "Epoch 260, Loss: 0.888079047203064\n",
      "Epoch 280, Loss: 0.879165768623352\n",
      "Epoch 300, Loss: 0.8714123964309692\n",
      "Epoch 320, Loss: 0.8648704290390015\n",
      "Epoch 340, Loss: 0.859573245048523\n",
      "Epoch 360, Loss: 0.8555391430854797\n",
      "Epoch 380, Loss: 0.8527727127075195\n",
      "Epoch 400, Loss: 0.8512672185897827\n",
      "Epoch 420, Loss: 0.8510066270828247\n",
      "Epoch 440, Loss: 0.8519670963287354\n",
      "Epoch 460, Loss: 0.8541184663772583\n",
      "Epoch 480, Loss: 0.8574259877204895\n",
      "weights of  A03 :  [array([[-0.45865384, -0.44537416],\n",
      "       [-0.35945502,  0.5527093 ],\n",
      "       [-0.1699614 , -1.08937   ],\n",
      "       [-0.4615102 , -0.6277625 ],\n",
      "       [ 0.7102756 ,  0.6058463 ]], dtype=float32), array([ 0.25589007, -0.25588998], dtype=float32)]\n",
      "Lowest loss of  A03 :  0.85095537\n",
      "Epoch 0, Loss: 1.8206547498703003\n",
      "Epoch 20, Loss: 0.9188038110733032\n",
      "Epoch 40, Loss: 0.9230664968490601\n",
      "Epoch 60, Loss: 0.9166877269744873\n",
      "Epoch 80, Loss: 0.9145140051841736\n",
      "Epoch 100, Loss: 0.9132861495018005\n",
      "Epoch 120, Loss: 0.9122416973114014\n",
      "Epoch 140, Loss: 0.911328136920929\n",
      "Epoch 160, Loss: 0.9105743169784546\n",
      "Epoch 180, Loss: 0.9100210666656494\n",
      "Epoch 200, Loss: 0.9097087383270264\n",
      "Epoch 220, Loss: 0.909675657749176\n",
      "Epoch 240, Loss: 0.909957230091095\n",
      "Epoch 260, Loss: 0.9105850458145142\n",
      "Epoch 280, Loss: 0.9115885496139526\n",
      "Epoch 300, Loss: 0.9129934906959534\n",
      "Epoch 320, Loss: 0.9148232936859131\n",
      "Epoch 340, Loss: 0.9170982837677002\n",
      "Epoch 360, Loss: 0.9198365211486816\n",
      "Epoch 380, Loss: 0.9230530261993408\n",
      "Epoch 400, Loss: 0.9267604947090149\n",
      "Epoch 420, Loss: 0.93096923828125\n",
      "Epoch 440, Loss: 0.9356870651245117\n",
      "Epoch 460, Loss: 0.9409198760986328\n",
      "Epoch 480, Loss: 0.9466710686683655\n",
      "weights of  A04 :  [array([[-0.66581666, -0.15125193],\n",
      "       [ 0.46413413,  0.80797005],\n",
      "       [ 0.7197322 ,  0.07973078],\n",
      "       [ 0.15943064, -0.4233749 ],\n",
      "       [-0.01586694,  0.6016456 ]], dtype=float32), array([ 0.03473688, -0.03473687], dtype=float32)]\n",
      "Lowest loss of  A04 :  0.90965295\n",
      "Epoch 0, Loss: 6.255564212799072\n",
      "Epoch 20, Loss: 1.2870347499847412\n",
      "Epoch 40, Loss: 1.0037176609039307\n",
      "Epoch 60, Loss: 0.9658256769180298\n",
      "Epoch 80, Loss: 0.966483473777771\n",
      "Epoch 100, Loss: 0.9633798599243164\n",
      "Epoch 120, Loss: 0.9617167115211487\n",
      "Epoch 140, Loss: 0.9601157903671265\n",
      "Epoch 160, Loss: 0.9584177732467651\n",
      "Epoch 180, Loss: 0.9566494822502136\n",
      "Epoch 200, Loss: 0.9548230767250061\n",
      "Epoch 220, Loss: 0.9529489278793335\n",
      "Epoch 240, Loss: 0.9510376453399658\n",
      "Epoch 260, Loss: 0.9490997195243835\n",
      "Epoch 280, Loss: 0.9471449255943298\n",
      "Epoch 300, Loss: 0.9451825618743896\n",
      "Epoch 320, Loss: 0.9432210326194763\n",
      "Epoch 340, Loss: 0.941268801689148\n",
      "Epoch 360, Loss: 0.9393336772918701\n",
      "Epoch 380, Loss: 0.9374227523803711\n",
      "Epoch 400, Loss: 0.9355431795120239\n",
      "Epoch 420, Loss: 0.9337015748023987\n",
      "Epoch 440, Loss: 0.9319040179252625\n",
      "Epoch 460, Loss: 0.9301563501358032\n",
      "Epoch 480, Loss: 0.9284640550613403\n",
      "weights of  A05 :  [array([[ 0.59774625,  0.60018253],\n",
      "       [ 0.28469282,  1.0725695 ],\n",
      "       [ 0.22771095, -0.11273468],\n",
      "       [ 0.42144305,  0.0321035 ],\n",
      "       [ 0.2803962 ,  0.18430926]], dtype=float32), array([ 0.08396949, -0.08396952], dtype=float32)]\n",
      "Lowest loss of  A05 :  0.9269125\n",
      "Epoch 0, Loss: 5.810476779937744\n",
      "Epoch 20, Loss: 1.0024399757385254\n",
      "Epoch 40, Loss: 0.8669566512107849\n",
      "Epoch 60, Loss: 0.8575348258018494\n",
      "Epoch 80, Loss: 0.8561344146728516\n",
      "Epoch 100, Loss: 0.8520039916038513\n",
      "Epoch 120, Loss: 0.8498383164405823\n",
      "Epoch 140, Loss: 0.8475522398948669\n",
      "Epoch 160, Loss: 0.845166802406311\n",
      "Epoch 180, Loss: 0.8426910042762756\n",
      "Epoch 200, Loss: 0.8401498794555664\n",
      "Epoch 220, Loss: 0.837561845779419\n",
      "Epoch 240, Loss: 0.8349447846412659\n",
      "Epoch 260, Loss: 0.832315981388092\n",
      "Epoch 280, Loss: 0.8296915888786316\n",
      "Epoch 300, Loss: 0.8270872831344604\n",
      "Epoch 320, Loss: 0.8245176672935486\n",
      "Epoch 340, Loss: 0.8219969868659973\n",
      "Epoch 360, Loss: 0.8195384740829468\n",
      "Epoch 380, Loss: 0.8171549439430237\n",
      "Epoch 400, Loss: 0.8148585557937622\n",
      "Epoch 420, Loss: 0.8126609325408936\n",
      "Epoch 440, Loss: 0.8105732202529907\n",
      "Epoch 460, Loss: 0.8086056113243103\n",
      "Epoch 480, Loss: 0.8067682385444641\n",
      "weights of  A06 :  [array([[ 0.10100115,  0.16123566],\n",
      "       [-0.22092505, -0.21184303],\n",
      "       [ 0.47597912, -0.12082172],\n",
      "       [ 0.29355475,  0.6541938 ],\n",
      "       [-0.3364164 , -0.418724  ]], dtype=float32), array([ 0.13973062, -0.1397306 ], dtype=float32)]\n",
      "Lowest loss of  A06 :  0.8051519\n",
      "Epoch 0, Loss: 2.4574813842773438\n",
      "Epoch 20, Loss: 0.9838064908981323\n",
      "Epoch 40, Loss: 0.9078390598297119\n",
      "Epoch 60, Loss: 0.9055836200714111\n",
      "Epoch 80, Loss: 0.9035274982452393\n",
      "Epoch 100, Loss: 0.9022175073623657\n",
      "Epoch 120, Loss: 0.9010869860649109\n",
      "Epoch 140, Loss: 0.8999108076095581\n",
      "Epoch 160, Loss: 0.8986809253692627\n",
      "Epoch 180, Loss: 0.8974077701568604\n",
      "Epoch 200, Loss: 0.8961024284362793\n",
      "Epoch 220, Loss: 0.8947754502296448\n",
      "Epoch 240, Loss: 0.8934367895126343\n",
      "Epoch 260, Loss: 0.8920958638191223\n",
      "Epoch 280, Loss: 0.8907618522644043\n",
      "Epoch 300, Loss: 0.8894434571266174\n",
      "Epoch 320, Loss: 0.8881491422653198\n",
      "Epoch 340, Loss: 0.8868869543075562\n",
      "Epoch 360, Loss: 0.8856648206710815\n",
      "Epoch 380, Loss: 0.8844902515411377\n",
      "Epoch 400, Loss: 0.8833707571029663\n",
      "Epoch 420, Loss: 0.8823136687278748\n",
      "Epoch 440, Loss: 0.8813256025314331\n",
      "Epoch 460, Loss: 0.8804135918617249\n",
      "Epoch 480, Loss: 0.8795839548110962\n",
      "weights of  A07 :  [array([[-0.4606274 , -0.58153653],\n",
      "       [-0.7754549 , -0.01523668],\n",
      "       [ 0.4300907 , -0.51276875],\n",
      "       [-0.02234533, -0.29524243],\n",
      "       [-0.14094934,  0.4920457 ]], dtype=float32), array([-0.1267671 ,  0.12676705], dtype=float32)]\n",
      "Lowest loss of  A07 :  0.87887794\n",
      "Epoch 0, Loss: 1.845492959022522\n",
      "Epoch 20, Loss: 1.4058116674423218\n",
      "Epoch 40, Loss: 1.3368455171585083\n",
      "Epoch 60, Loss: 1.281915545463562\n",
      "Epoch 80, Loss: 1.2232398986816406\n",
      "Epoch 100, Loss: 1.1621915102005005\n",
      "Epoch 120, Loss: 1.1006550788879395\n",
      "Epoch 140, Loss: 1.0402824878692627\n",
      "Epoch 160, Loss: 0.9824883341789246\n",
      "Epoch 180, Loss: 0.9284340143203735\n",
      "Epoch 200, Loss: 0.8789995908737183\n",
      "Epoch 220, Loss: 0.8348362445831299\n",
      "Epoch 240, Loss: 0.7963767051696777\n",
      "Epoch 260, Loss: 0.763861894607544\n",
      "Epoch 280, Loss: 0.7373653650283813\n",
      "Epoch 300, Loss: 0.7168221473693848\n",
      "Epoch 320, Loss: 0.7020576000213623\n",
      "Epoch 340, Loss: 0.6928166747093201\n",
      "Epoch 360, Loss: 0.6887901425361633\n",
      "Epoch 380, Loss: 0.6896365880966187\n",
      "Epoch 400, Loss: 0.6950017213821411\n",
      "Epoch 420, Loss: 0.7045316100120544\n",
      "Epoch 440, Loss: 0.7178831696510315\n",
      "Epoch 460, Loss: 0.7347304821014404\n",
      "Epoch 480, Loss: 0.7547687292098999\n",
      "weights of  A08 :  [array([[-0.10179693,  0.43036905],\n",
      "       [-0.44761664,  0.3194678 ],\n",
      "       [ 0.66390985, -0.22601256],\n",
      "       [ 0.00753875, -0.30707726],\n",
      "       [ 0.08838595, -0.01006454]], dtype=float32), array([-0.04705816,  0.04705818], dtype=float32)]\n",
      "Lowest loss of  A08 :  0.68854845\n",
      "Epoch 0, Loss: 0.98710036277771\n",
      "Epoch 20, Loss: 0.965995192527771\n",
      "Epoch 40, Loss: 0.9513053894042969\n",
      "Epoch 60, Loss: 0.9404726028442383\n",
      "Epoch 80, Loss: 0.9354299306869507\n",
      "Epoch 100, Loss: 0.9375866651535034\n",
      "Epoch 120, Loss: 0.947364091873169\n",
      "Epoch 140, Loss: 0.9645828604698181\n",
      "Epoch 160, Loss: 0.9887081384658813\n",
      "Epoch 180, Loss: 1.0190300941467285\n",
      "Epoch 200, Loss: 1.0547747611999512\n",
      "Epoch 220, Loss: 1.0951695442199707\n",
      "Epoch 240, Loss: 1.1394808292388916\n",
      "Epoch 260, Loss: 1.1870313882827759\n",
      "Epoch 280, Loss: 1.2372084856033325\n",
      "Epoch 300, Loss: 1.289463758468628\n",
      "Epoch 320, Loss: 1.343312382698059\n",
      "Epoch 340, Loss: 1.3983275890350342\n",
      "Epoch 360, Loss: 1.4541356563568115\n",
      "Epoch 380, Loss: 1.5104115009307861\n",
      "Epoch 400, Loss: 1.5668721199035645\n",
      "Epoch 420, Loss: 1.623273253440857\n",
      "Epoch 440, Loss: 1.6794023513793945\n",
      "Epoch 460, Loss: 1.735076665878296\n",
      "Epoch 480, Loss: 1.7901395559310913\n",
      "weights of  A09 :  [array([[ 0.5689726 ,  0.01434405],\n",
      "       [-0.06061462,  1.0498654 ],\n",
      "       [-0.32019454,  0.55902874],\n",
      "       [ 0.59035397, -0.06901871],\n",
      "       [ 0.9413046 ,  0.14619857]], dtype=float32), array([-0.0096097 ,  0.00960965], dtype=float32)]\n",
      "Lowest loss of  A09 :  0.93526495\n",
      "Epoch 0, Loss: 3.0107669830322266\n",
      "Epoch 20, Loss: 1.122133731842041\n",
      "Epoch 40, Loss: 0.926170825958252\n",
      "Epoch 60, Loss: 0.9222209453582764\n",
      "Epoch 80, Loss: 0.9204933643341064\n",
      "Epoch 100, Loss: 0.921065092086792\n",
      "Epoch 120, Loss: 0.9216235280036926\n",
      "Epoch 140, Loss: 0.9223061800003052\n",
      "Epoch 160, Loss: 0.9230784177780151\n",
      "Epoch 180, Loss: 0.9239321947097778\n",
      "Epoch 200, Loss: 0.9248678088188171\n",
      "Epoch 220, Loss: 0.9258869886398315\n",
      "Epoch 240, Loss: 0.9269925355911255\n",
      "Epoch 260, Loss: 0.9281870722770691\n",
      "Epoch 280, Loss: 0.9294736385345459\n",
      "Epoch 300, Loss: 0.9308549165725708\n",
      "Epoch 320, Loss: 0.9323341846466064\n",
      "Epoch 340, Loss: 0.9339141845703125\n",
      "Epoch 360, Loss: 0.9355980157852173\n",
      "Epoch 380, Loss: 0.9373890161514282\n",
      "Epoch 400, Loss: 0.9392898082733154\n",
      "Epoch 420, Loss: 0.9413034915924072\n",
      "Epoch 440, Loss: 0.9434328079223633\n",
      "Epoch 460, Loss: 0.9456807374954224\n",
      "Epoch 480, Loss: 0.948049783706665\n",
      "weights of  P001 :  [array([[-0.23965997, -0.6831846 ],\n",
      "       [-0.48603025,  0.1938992 ],\n",
      "       [ 0.35566887, -0.66222084],\n",
      "       [-0.23278865,  0.3138726 ],\n",
      "       [ 0.78913534,  0.60547316]], dtype=float32), array([-0.11093602,  0.11093602], dtype=float32)]\n",
      "Lowest loss of  P001 :  0.91970116\n",
      "Epoch 0, Loss: 1.8856182098388672\n",
      "Epoch 20, Loss: 0.8930295705795288\n",
      "Epoch 40, Loss: 0.9034924507141113\n",
      "Epoch 60, Loss: 0.8972897529602051\n",
      "Epoch 80, Loss: 0.8990629315376282\n",
      "Epoch 100, Loss: 0.9019267559051514\n",
      "Epoch 120, Loss: 0.9052470922470093\n",
      "Epoch 140, Loss: 0.9090051054954529\n",
      "Epoch 160, Loss: 0.9132053852081299\n",
      "Epoch 180, Loss: 0.9178568124771118\n",
      "Epoch 200, Loss: 0.9229714870452881\n",
      "Epoch 220, Loss: 0.928561806678772\n",
      "Epoch 240, Loss: 0.9346375465393066\n",
      "Epoch 260, Loss: 0.9412081241607666\n",
      "Epoch 280, Loss: 0.9482816457748413\n",
      "Epoch 300, Loss: 0.9558655023574829\n",
      "Epoch 320, Loss: 0.9639654159545898\n",
      "Epoch 340, Loss: 0.9725862741470337\n",
      "Epoch 360, Loss: 0.9817311763763428\n",
      "Epoch 380, Loss: 0.9914019107818604\n",
      "Epoch 400, Loss: 1.0015994310379028\n",
      "Epoch 420, Loss: 1.0123227834701538\n",
      "Epoch 440, Loss: 1.0235700607299805\n",
      "Epoch 460, Loss: 1.0353386402130127\n",
      "Epoch 480, Loss: 1.0476242303848267\n",
      "weights of  P002 :  [array([[-0.0984475 , -0.00424861],\n",
      "       [-0.8824581 ,  0.37033334],\n",
      "       [ 0.3813947 , -0.7104041 ],\n",
      "       [-0.70815784, -0.42544514],\n",
      "       [ 0.65510136, -0.32076672]], dtype=float32), array([ 0.07432427, -0.07432427], dtype=float32)]\n",
      "Lowest loss of  P002 :  0.89283866\n",
      "Epoch 0, Loss: 6.669065475463867\n",
      "Epoch 20, Loss: 1.7219072580337524\n",
      "Epoch 40, Loss: 0.9550857543945312\n",
      "Epoch 60, Loss: 0.8235711455345154\n",
      "Epoch 80, Loss: 0.8167747259140015\n",
      "Epoch 100, Loss: 0.8157615661621094\n",
      "Epoch 120, Loss: 0.8144074082374573\n",
      "Epoch 140, Loss: 0.8133622407913208\n",
      "Epoch 160, Loss: 0.8122348785400391\n",
      "Epoch 180, Loss: 0.8110687136650085\n",
      "Epoch 200, Loss: 0.8098602890968323\n",
      "Epoch 220, Loss: 0.8086152672767639\n",
      "Epoch 240, Loss: 0.8073406219482422\n",
      "Epoch 260, Loss: 0.8060426115989685\n",
      "Epoch 280, Loss: 0.8047274947166443\n",
      "Epoch 300, Loss: 0.8034007549285889\n",
      "Epoch 320, Loss: 0.8020676970481873\n",
      "Epoch 340, Loss: 0.8007333278656006\n",
      "Epoch 360, Loss: 0.7994027137756348\n",
      "Epoch 380, Loss: 0.7980802059173584\n",
      "Epoch 400, Loss: 0.796770453453064\n",
      "Epoch 420, Loss: 0.7954775094985962\n",
      "Epoch 440, Loss: 0.7942054867744446\n",
      "Epoch 460, Loss: 0.7929580807685852\n",
      "Epoch 480, Loss: 0.7917391061782837\n",
      "weights of  P003 :  [array([[-0.1478468 ,  0.4716714 ],\n",
      "       [-0.3030062 , -0.15782964],\n",
      "       [-0.03035434, -0.5959312 ],\n",
      "       [-0.16283391, -0.31741637],\n",
      "       [ 0.1054243 ,  0.28600815]], dtype=float32), array([-0.30692148,  0.30692154], dtype=float32)]\n",
      "Lowest loss of  P003 :  0.7906106\n",
      "Epoch 0, Loss: 0.9935737252235413\n",
      "Epoch 20, Loss: 0.9510408639907837\n",
      "Epoch 40, Loss: 0.9468206167221069\n",
      "Epoch 60, Loss: 0.943827748298645\n",
      "Epoch 80, Loss: 0.9417681694030762\n",
      "Epoch 100, Loss: 0.9409414529800415\n",
      "Epoch 120, Loss: 0.9417306184768677\n",
      "Epoch 140, Loss: 0.9444724321365356\n",
      "Epoch 160, Loss: 0.9494308233261108\n",
      "Epoch 180, Loss: 0.956818699836731\n",
      "Epoch 200, Loss: 0.9667871594429016\n",
      "Epoch 220, Loss: 0.9794356226921082\n",
      "Epoch 240, Loss: 0.994815468788147\n",
      "Epoch 260, Loss: 1.0129365921020508\n",
      "Epoch 280, Loss: 1.0337717533111572\n",
      "Epoch 300, Loss: 1.0572633743286133\n",
      "Epoch 320, Loss: 1.083326816558838\n",
      "Epoch 340, Loss: 1.1118568181991577\n",
      "Epoch 360, Loss: 1.1427310705184937\n",
      "Epoch 380, Loss: 1.175813913345337\n",
      "Epoch 400, Loss: 1.2109615802764893\n",
      "Epoch 420, Loss: 1.2480227947235107\n",
      "Epoch 440, Loss: 1.28684401512146\n",
      "Epoch 460, Loss: 1.3272700309753418\n",
      "Epoch 480, Loss: 1.3691469430923462\n",
      "weights of  P004 :  [array([[ 0.89095896,  0.04295447],\n",
      "       [-0.06841695, -0.14226732],\n",
      "       [-0.7314271 , -0.08693257],\n",
      "       [ 0.0100842 ,  0.7999922 ],\n",
      "       [ 0.7845858 , -0.20638601]], dtype=float32), array([ 0.0220854 , -0.02208543], dtype=float32)]\n",
      "Lowest loss of  P004 :  0.9409363\n",
      "Epoch 0, Loss: 1.237621784210205\n",
      "Epoch 20, Loss: 0.989679217338562\n",
      "Epoch 40, Loss: 0.9846580028533936\n",
      "Epoch 60, Loss: 0.9803858995437622\n",
      "Epoch 80, Loss: 0.9754780530929565\n",
      "Epoch 100, Loss: 0.9701980352401733\n",
      "Epoch 120, Loss: 0.9646856784820557\n",
      "Epoch 140, Loss: 0.9589786529541016\n",
      "Epoch 160, Loss: 0.9531903862953186\n",
      "Epoch 180, Loss: 0.9473972320556641\n",
      "Epoch 200, Loss: 0.9416730999946594\n",
      "Epoch 220, Loss: 0.9360804557800293\n",
      "Epoch 240, Loss: 0.9306724071502686\n",
      "Epoch 260, Loss: 0.9254942536354065\n",
      "Epoch 280, Loss: 0.920583188533783\n",
      "Epoch 300, Loss: 0.9159693717956543\n",
      "Epoch 320, Loss: 0.911676287651062\n",
      "Epoch 340, Loss: 0.9077210426330566\n",
      "Epoch 360, Loss: 0.9041153192520142\n",
      "Epoch 380, Loss: 0.900865375995636\n",
      "Epoch 400, Loss: 0.8979730010032654\n",
      "Epoch 420, Loss: 0.8954353332519531\n",
      "Epoch 440, Loss: 0.893246591091156\n",
      "Epoch 460, Loss: 0.8913969993591309\n",
      "Epoch 480, Loss: 0.8898746371269226\n",
      "weights of  P005 :  [array([[ 0.9421194 ,  0.4659667 ],\n",
      "       [-0.28362855,  0.5037938 ],\n",
      "       [ 0.3395072 ,  0.38921565],\n",
      "       [ 0.47492743, -0.1682508 ],\n",
      "       [ 0.00404908,  0.21927431]], dtype=float32), array([ 0.3586344, -0.3586345], dtype=float32)]\n",
      "Lowest loss of  P005 :  0.8887187\n",
      "Epoch 0, Loss: 3.8324058055877686\n",
      "Epoch 20, Loss: 1.2298177480697632\n",
      "Epoch 40, Loss: 1.0180003643035889\n",
      "Epoch 60, Loss: 0.9762218594551086\n",
      "Epoch 80, Loss: 0.9752293825149536\n",
      "Epoch 100, Loss: 0.9733672142028809\n",
      "Epoch 120, Loss: 0.9721736907958984\n",
      "Epoch 140, Loss: 0.9708946943283081\n",
      "Epoch 160, Loss: 0.96954345703125\n",
      "Epoch 180, Loss: 0.968126654624939\n",
      "Epoch 200, Loss: 0.9666544198989868\n",
      "Epoch 220, Loss: 0.9651360511779785\n",
      "Epoch 240, Loss: 0.9635792970657349\n",
      "Epoch 260, Loss: 0.9619911313056946\n",
      "Epoch 280, Loss: 0.9603780508041382\n",
      "Epoch 300, Loss: 0.9587461948394775\n",
      "Epoch 320, Loss: 0.9571007490158081\n",
      "Epoch 340, Loss: 0.9554471969604492\n",
      "Epoch 360, Loss: 0.9537904858589172\n",
      "Epoch 380, Loss: 0.9521346092224121\n",
      "Epoch 400, Loss: 0.9504837989807129\n",
      "Epoch 420, Loss: 0.94884192943573\n",
      "Epoch 440, Loss: 0.9472124576568604\n",
      "Epoch 460, Loss: 0.9455986022949219\n",
      "Epoch 480, Loss: 0.9440031051635742\n",
      "weights of  P006 :  [array([[-0.10824282, -0.02102511],\n",
      "       [ 0.2621732 , -0.6709033 ],\n",
      "       [ 0.662807  ,  0.92427397],\n",
      "       [-0.17811804, -0.36147273],\n",
      "       [-0.5090161 ,  0.5049509 ]], dtype=float32), array([ 0.3709264 , -0.37092638], dtype=float32)]\n",
      "Lowest loss of  P006 :  0.942507\n",
      "Epoch 0, Loss: 6.000302314758301\n",
      "Epoch 20, Loss: 1.2210031747817993\n",
      "Epoch 40, Loss: 0.9610955119132996\n",
      "Epoch 60, Loss: 0.9322922229766846\n",
      "Epoch 80, Loss: 0.9225733280181885\n",
      "Epoch 100, Loss: 0.9078624844551086\n",
      "Epoch 120, Loss: 0.8945707678794861\n",
      "Epoch 140, Loss: 0.8807581067085266\n",
      "Epoch 160, Loss: 0.8665932416915894\n",
      "Epoch 180, Loss: 0.8522676229476929\n",
      "Epoch 200, Loss: 0.8379601836204529\n",
      "Epoch 220, Loss: 0.8238256573677063\n",
      "Epoch 240, Loss: 0.810005784034729\n",
      "Epoch 260, Loss: 0.7966287732124329\n",
      "Epoch 280, Loss: 0.7838107347488403\n",
      "Epoch 300, Loss: 0.7716561555862427\n",
      "Epoch 320, Loss: 0.7602581977844238\n",
      "Epoch 340, Loss: 0.7496992349624634\n",
      "Epoch 360, Loss: 0.7400513887405396\n",
      "Epoch 380, Loss: 0.731377124786377\n",
      "Epoch 400, Loss: 0.7237297296524048\n",
      "Epoch 420, Loss: 0.7171533703804016\n",
      "Epoch 440, Loss: 0.7116843461990356\n",
      "Epoch 460, Loss: 0.7073508501052856\n",
      "Epoch 480, Loss: 0.704174280166626\n",
      "weights of  P007 :  [array([[ 0.64770275, -0.1893092 ],\n",
      "       [-0.7215629 , -0.19743305],\n",
      "       [-0.43709671,  0.30748314],\n",
      "       [ 0.16306311, -0.04409504],\n",
      "       [-0.07612873, -0.1903494 ]], dtype=float32), array([-0.34331092,  0.3433109 ], dtype=float32)]\n",
      "Lowest loss of  P007 :  0.7022412\n",
      "Epoch 0, Loss: 2.6520347595214844\n",
      "Epoch 20, Loss: 1.122981071472168\n",
      "Epoch 40, Loss: 1.014381766319275\n",
      "Epoch 60, Loss: 1.0133259296417236\n",
      "Epoch 80, Loss: 1.0093499422073364\n",
      "Epoch 100, Loss: 1.0067046880722046\n",
      "Epoch 120, Loss: 1.0039610862731934\n",
      "Epoch 140, Loss: 1.0010095834732056\n",
      "Epoch 160, Loss: 0.9978882074356079\n",
      "Epoch 180, Loss: 0.9946248531341553\n",
      "Epoch 200, Loss: 0.9912425875663757\n",
      "Epoch 220, Loss: 0.9877619743347168\n",
      "Epoch 240, Loss: 0.9842027425765991\n",
      "Epoch 260, Loss: 0.9805834293365479\n",
      "Epoch 280, Loss: 0.9769212603569031\n",
      "Epoch 300, Loss: 0.9732327461242676\n",
      "Epoch 320, Loss: 0.9695337414741516\n",
      "Epoch 340, Loss: 0.9658389091491699\n",
      "Epoch 360, Loss: 0.9621627330780029\n",
      "Epoch 380, Loss: 0.9585191011428833\n",
      "Epoch 400, Loss: 0.954920768737793\n",
      "Epoch 420, Loss: 0.9513804912567139\n",
      "Epoch 440, Loss: 0.9479104280471802\n",
      "Epoch 460, Loss: 0.9445218443870544\n",
      "Epoch 480, Loss: 0.9412262439727783\n",
      "weights of  P008 :  [array([[-0.76714575,  0.26640278],\n",
      "       [ 0.36742985,  0.03130017],\n",
      "       [-0.11071404,  0.93263984],\n",
      "       [ 0.7776552 , -0.37215644],\n",
      "       [ 0.02355858,  0.01636821]], dtype=float32), array([ 0.19698551, -0.19698554], dtype=float32)]\n",
      "Lowest loss of  P008 :  0.9381908\n",
      "Epoch 0, Loss: 4.121641159057617\n",
      "Epoch 20, Loss: 1.1374050378799438\n",
      "Epoch 40, Loss: 0.9754030704498291\n",
      "Epoch 60, Loss: 0.9308287501335144\n",
      "Epoch 80, Loss: 0.927121102809906\n",
      "Epoch 100, Loss: 0.9270790219306946\n",
      "Epoch 120, Loss: 0.9268947243690491\n",
      "Epoch 140, Loss: 0.926802933216095\n",
      "Epoch 160, Loss: 0.926763653755188\n",
      "Epoch 180, Loss: 0.926756739616394\n",
      "Epoch 200, Loss: 0.9267839193344116\n",
      "Epoch 220, Loss: 0.9268509149551392\n",
      "Epoch 240, Loss: 0.9269629716873169\n",
      "Epoch 260, Loss: 0.9271258115768433\n",
      "Epoch 280, Loss: 0.9273440837860107\n",
      "Epoch 300, Loss: 0.92762291431427\n",
      "Epoch 320, Loss: 0.9279670119285583\n",
      "Epoch 340, Loss: 0.9283807277679443\n",
      "Epoch 360, Loss: 0.9288686513900757\n",
      "Epoch 380, Loss: 0.929435133934021\n",
      "Epoch 400, Loss: 0.9300841093063354\n",
      "Epoch 420, Loss: 0.9308197498321533\n",
      "Epoch 440, Loss: 0.931645393371582\n",
      "Epoch 460, Loss: 0.9325650334358215\n",
      "Epoch 480, Loss: 0.9335819482803345\n",
      "weights of  P009 :  [array([[ 0.5797592 , -0.52292734],\n",
      "       [-0.22649713,  0.1726186 ],\n",
      "       [-0.4296757 ,  0.32446146],\n",
      "       [ 0.10161613,  0.7862258 ],\n",
      "       [ 0.6048058 , -0.72985375]], dtype=float32), array([ 0.13169312, -0.13169311], dtype=float32)]\n",
      "Lowest loss of  P009 :  0.92675567\n",
      "Epoch 0, Loss: 2.542188882827759\n",
      "Epoch 20, Loss: 1.0983408689498901\n",
      "Epoch 40, Loss: 1.0301098823547363\n",
      "Epoch 60, Loss: 1.0230904817581177\n",
      "Epoch 80, Loss: 1.0177010297775269\n",
      "Epoch 100, Loss: 1.0122681856155396\n",
      "Epoch 120, Loss: 1.006712794303894\n",
      "Epoch 140, Loss: 1.0009046792984009\n",
      "Epoch 160, Loss: 0.9948968887329102\n",
      "Epoch 180, Loss: 0.9887582063674927\n",
      "Epoch 200, Loss: 0.9825537204742432\n",
      "Epoch 220, Loss: 0.9763433933258057\n",
      "Epoch 240, Loss: 0.9701830148696899\n",
      "Epoch 260, Loss: 0.9641237854957581\n",
      "Epoch 280, Loss: 0.958213746547699\n",
      "Epoch 300, Loss: 0.9524965286254883\n",
      "Epoch 320, Loss: 0.9470135569572449\n",
      "Epoch 340, Loss: 0.9418021440505981\n",
      "Epoch 360, Loss: 0.9368969202041626\n",
      "Epoch 380, Loss: 0.9323297142982483\n",
      "Epoch 400, Loss: 0.9281289577484131\n",
      "Epoch 420, Loss: 0.9243211150169373\n",
      "Epoch 440, Loss: 0.9209293127059937\n",
      "Epoch 460, Loss: 0.9179744124412537\n",
      "Epoch 480, Loss: 0.9154748320579529\n",
      "weights of  P010 :  [array([[ 1.0032988 , -0.05311509],\n",
      "       [-0.36183536,  0.05646136],\n",
      "       [-0.60576236, -0.671524  ],\n",
      "       [-0.16533631, -0.29224363],\n",
      "       [-0.5048616 ,  0.37176332]], dtype=float32), array([-0.03492697,  0.03492698], dtype=float32)]\n",
      "Lowest loss of  P010 :  0.91353625\n",
      "Epoch 0, Loss: 2.9759328365325928\n",
      "Epoch 20, Loss: 1.189035177230835\n",
      "Epoch 40, Loss: 1.0210376977920532\n",
      "Epoch 60, Loss: 1.0204354524612427\n",
      "Epoch 80, Loss: 1.0156593322753906\n",
      "Epoch 100, Loss: 1.0139274597167969\n",
      "Epoch 120, Loss: 1.012043833732605\n",
      "Epoch 140, Loss: 1.0100178718566895\n",
      "Epoch 160, Loss: 1.0078588724136353\n",
      "Epoch 180, Loss: 1.0055800676345825\n",
      "Epoch 200, Loss: 1.0031949281692505\n",
      "Epoch 220, Loss: 1.0007154941558838\n",
      "Epoch 240, Loss: 0.9981517791748047\n",
      "Epoch 260, Loss: 0.9955142140388489\n",
      "Epoch 280, Loss: 0.9928113222122192\n",
      "Epoch 300, Loss: 0.9900519847869873\n",
      "Epoch 320, Loss: 0.9872442483901978\n",
      "Epoch 340, Loss: 0.9843955039978027\n",
      "Epoch 360, Loss: 0.9815133810043335\n",
      "Epoch 380, Loss: 0.978604793548584\n",
      "Epoch 400, Loss: 0.9756766557693481\n",
      "Epoch 420, Loss: 0.9727354049682617\n",
      "Epoch 440, Loss: 0.9697874784469604\n",
      "Epoch 460, Loss: 0.9668388366699219\n",
      "Epoch 480, Loss: 0.9638950824737549\n",
      "weights of  P011 :  [array([[-0.7244599 , -0.04863515],\n",
      "       [ 0.56540895,  0.01101226],\n",
      "       [-0.49739912,  0.22169226],\n",
      "       [-0.5506523 , -0.99751556],\n",
      "       [ 0.21174935,  0.23935017]], dtype=float32), array([-0.05651088,  0.05651085], dtype=float32)]\n",
      "Lowest loss of  P011 :  0.9611085\n",
      "Epoch 0, Loss: 5.089773654937744\n",
      "Epoch 20, Loss: 0.8457996845245361\n",
      "Epoch 40, Loss: 0.8297198414802551\n",
      "Epoch 60, Loss: 0.828163743019104\n",
      "Epoch 80, Loss: 0.8194244503974915\n",
      "Epoch 100, Loss: 0.8180215954780579\n",
      "Epoch 120, Loss: 0.8174565434455872\n",
      "Epoch 140, Loss: 0.8168936371803284\n",
      "Epoch 160, Loss: 0.8163144588470459\n",
      "Epoch 180, Loss: 0.8157152533531189\n",
      "Epoch 200, Loss: 0.815100371837616\n",
      "Epoch 220, Loss: 0.814475417137146\n",
      "Epoch 240, Loss: 0.8138450980186462\n",
      "Epoch 260, Loss: 0.813214123249054\n",
      "Epoch 280, Loss: 0.8125869631767273\n",
      "Epoch 300, Loss: 0.8119677305221558\n",
      "Epoch 320, Loss: 0.8113607168197632\n",
      "Epoch 340, Loss: 0.8107702136039734\n",
      "Epoch 360, Loss: 0.810200035572052\n",
      "Epoch 380, Loss: 0.8096543550491333\n",
      "Epoch 400, Loss: 0.8091367483139038\n",
      "Epoch 420, Loss: 0.8086514472961426\n",
      "Epoch 440, Loss: 0.8082016706466675\n",
      "Epoch 460, Loss: 0.8077914118766785\n",
      "Epoch 480, Loss: 0.8074243664741516\n",
      "weights of  P012 :  [array([[-0.24323495, -0.41328913],\n",
      "       [-0.5441709 ,  0.05976053],\n",
      "       [-0.5579866 , -0.06057949],\n",
      "       [ 0.25000054, -0.14400043],\n",
      "       [ 0.00270388, -0.52370036]], dtype=float32), array([-0.11692362,  0.11692359], dtype=float32)]\n",
      "Lowest loss of  P012 :  0.80711883\n",
      "Epoch 0, Loss: 3.068002223968506\n",
      "Epoch 20, Loss: 0.9977136850357056\n",
      "Epoch 40, Loss: 0.787702202796936\n",
      "Epoch 60, Loss: 0.774163007736206\n",
      "Epoch 80, Loss: 0.7724179625511169\n",
      "Epoch 100, Loss: 0.7719602584838867\n",
      "Epoch 120, Loss: 0.7716029286384583\n",
      "Epoch 140, Loss: 0.7713342308998108\n",
      "Epoch 160, Loss: 0.7711237668991089\n",
      "Epoch 180, Loss: 0.7709876298904419\n",
      "Epoch 200, Loss: 0.7709426879882812\n",
      "Epoch 220, Loss: 0.7710047960281372\n",
      "Epoch 240, Loss: 0.7711893916130066\n",
      "Epoch 260, Loss: 0.7715113162994385\n",
      "Epoch 280, Loss: 0.7719851732254028\n",
      "Epoch 300, Loss: 0.7726246118545532\n",
      "Epoch 320, Loss: 0.7734432816505432\n",
      "Epoch 340, Loss: 0.7744542956352234\n",
      "Epoch 360, Loss: 0.7756702899932861\n",
      "Epoch 380, Loss: 0.7771032452583313\n",
      "Epoch 400, Loss: 0.7787650227546692\n",
      "Epoch 420, Loss: 0.7806665897369385\n",
      "Epoch 440, Loss: 0.7828189134597778\n",
      "Epoch 460, Loss: 0.7852320671081543\n",
      "Epoch 480, Loss: 0.7879156470298767\n",
      "weights of  P013 :  [array([[ 0.05797737,  0.07288916],\n",
      "       [-0.3710217 ,  0.17580678],\n",
      "       [-0.3130771 , -0.445897  ],\n",
      "       [-0.07962637, -0.04978543],\n",
      "       [ 0.7671599 ,  0.0322054 ]], dtype=float32), array([ 0.15819778, -0.15819785], dtype=float32)]\n",
      "Lowest loss of  P013 :  0.7709425\n",
      "Epoch 0, Loss: 1.3464058637619019\n",
      "Epoch 20, Loss: 1.06977117061615\n",
      "Epoch 40, Loss: 1.0609790086746216\n",
      "Epoch 60, Loss: 1.0581508874893188\n",
      "Epoch 80, Loss: 1.0542049407958984\n",
      "Epoch 100, Loss: 1.0504086017608643\n",
      "Epoch 120, Loss: 1.0465757846832275\n",
      "Epoch 140, Loss: 1.0427855253219604\n",
      "Epoch 160, Loss: 1.0391769409179688\n",
      "Epoch 180, Loss: 1.0358433723449707\n",
      "Epoch 200, Loss: 1.0328787565231323\n",
      "Epoch 220, Loss: 1.0303699970245361\n",
      "Epoch 240, Loss: 1.028395414352417\n",
      "Epoch 260, Loss: 1.027026891708374\n",
      "Epoch 280, Loss: 1.0263299942016602\n",
      "Epoch 300, Loss: 1.0263643264770508\n",
      "Epoch 320, Loss: 1.027182698249817\n",
      "Epoch 340, Loss: 1.0288324356079102\n",
      "Epoch 360, Loss: 1.0313550233840942\n",
      "Epoch 380, Loss: 1.0347867012023926\n",
      "Epoch 400, Loss: 1.0391578674316406\n",
      "Epoch 420, Loss: 1.044493556022644\n",
      "Epoch 440, Loss: 1.050814151763916\n",
      "Epoch 460, Loss: 1.0581350326538086\n",
      "Epoch 480, Loss: 1.066467523574829\n",
      "weights of  P014 :  [array([[ 0.8557272 ,  0.5163851 ],\n",
      "       [-0.20669672, -0.12851968],\n",
      "       [ 0.70383346,  0.48747456],\n",
      "       [-0.54614186,  0.6250382 ],\n",
      "       [ 0.33127657, -0.99209124]], dtype=float32), array([-0.2304164 ,  0.23041639], dtype=float32)]\n",
      "Lowest loss of  P014 :  1.0262517\n",
      "Epoch 0, Loss: 1.4022715091705322\n",
      "Epoch 20, Loss: 0.8502961993217468\n",
      "Epoch 40, Loss: 0.8222709894180298\n",
      "Epoch 60, Loss: 0.8218243718147278\n",
      "Epoch 80, Loss: 0.8223926424980164\n",
      "Epoch 100, Loss: 0.8232837319374084\n",
      "Epoch 120, Loss: 0.8245993256568909\n",
      "Epoch 140, Loss: 0.8263477087020874\n",
      "Epoch 160, Loss: 0.8285712599754333\n",
      "Epoch 180, Loss: 0.8313196301460266\n",
      "Epoch 200, Loss: 0.8346423506736755\n",
      "Epoch 220, Loss: 0.8385831713676453\n",
      "Epoch 240, Loss: 0.843182384967804\n",
      "Epoch 260, Loss: 0.8484761714935303\n",
      "Epoch 280, Loss: 0.8544965982437134\n",
      "Epoch 300, Loss: 0.8612719178199768\n",
      "Epoch 320, Loss: 0.8688259124755859\n",
      "Epoch 340, Loss: 0.877178430557251\n",
      "Epoch 360, Loss: 0.886345624923706\n",
      "Epoch 380, Loss: 0.8963394165039062\n",
      "Epoch 400, Loss: 0.9071686863899231\n",
      "Epoch 420, Loss: 0.9188380241394043\n",
      "Epoch 440, Loss: 0.931349515914917\n",
      "Epoch 460, Loss: 0.9447013139724731\n",
      "Epoch 480, Loss: 0.9588887691497803\n",
      "weights of  P015 :  [array([[-1.6320817e-01, -5.3059608e-01],\n",
      "       [-1.8635708e-01,  2.8310528e-01],\n",
      "       [ 1.1099130e-04, -4.4264570e-01],\n",
      "       [ 6.3214433e-01,  5.9068727e-01],\n",
      "       [-4.8930320e-01, -2.3735794e-01]], dtype=float32), array([ 0.04314137, -0.04314137], dtype=float32)]\n",
      "Lowest loss of  P015 :  0.8211093\n",
      "Epoch 0, Loss: 3.0104498863220215\n",
      "Epoch 20, Loss: 1.1799588203430176\n",
      "Epoch 40, Loss: 0.9932130575180054\n",
      "Epoch 60, Loss: 0.9919178485870361\n",
      "Epoch 80, Loss: 0.9879050254821777\n",
      "Epoch 100, Loss: 0.9864799976348877\n",
      "Epoch 120, Loss: 0.9847973585128784\n",
      "Epoch 140, Loss: 0.982994794845581\n",
      "Epoch 160, Loss: 0.9810753464698792\n",
      "Epoch 180, Loss: 0.9790410995483398\n",
      "Epoch 200, Loss: 0.9768986701965332\n",
      "Epoch 220, Loss: 0.974655270576477\n",
      "Epoch 240, Loss: 0.9723174571990967\n",
      "Epoch 260, Loss: 0.9698919057846069\n",
      "Epoch 280, Loss: 0.9673837423324585\n",
      "Epoch 300, Loss: 0.9647988080978394\n",
      "Epoch 320, Loss: 0.9621423482894897\n",
      "Epoch 340, Loss: 0.9594191312789917\n",
      "Epoch 360, Loss: 0.9566337466239929\n",
      "Epoch 380, Loss: 0.9537907838821411\n",
      "Epoch 400, Loss: 0.9508942365646362\n",
      "Epoch 420, Loss: 0.9479485750198364\n",
      "Epoch 440, Loss: 0.9449574947357178\n",
      "Epoch 460, Loss: 0.9419249892234802\n",
      "Epoch 480, Loss: 0.9388545751571655\n",
      "weights of  P016 :  [array([[ 0.22418909,  0.12587222],\n",
      "       [ 0.18817507, -0.573249  ],\n",
      "       [-0.62979174, -0.26177582],\n",
      "       [ 0.16288222, -0.3472134 ],\n",
      "       [-0.88317394,  0.6626338 ]], dtype=float32), array([ 0.02468614, -0.02468612], dtype=float32)]\n",
      "Lowest loss of  P016 :  0.9359063\n",
      "Epoch 0, Loss: 0.9185876846313477\n",
      "Epoch 20, Loss: 0.8915745615959167\n",
      "Epoch 40, Loss: 0.8922222852706909\n",
      "Epoch 60, Loss: 0.8953803181648254\n",
      "Epoch 80, Loss: 0.899696946144104\n",
      "Epoch 100, Loss: 0.9050852060317993\n",
      "Epoch 120, Loss: 0.9115663766860962\n",
      "Epoch 140, Loss: 0.9191106557846069\n",
      "Epoch 160, Loss: 0.9277211427688599\n",
      "Epoch 180, Loss: 0.9373618364334106\n",
      "Epoch 200, Loss: 0.9479831457138062\n",
      "Epoch 220, Loss: 0.9595210552215576\n",
      "Epoch 240, Loss: 0.9718987941741943\n",
      "Epoch 260, Loss: 0.9850311875343323\n",
      "Epoch 280, Loss: 0.998826801776886\n",
      "Epoch 300, Loss: 1.0131912231445312\n",
      "Epoch 320, Loss: 1.0280284881591797\n",
      "Epoch 340, Loss: 1.0432438850402832\n",
      "Epoch 360, Loss: 1.0587449073791504\n",
      "Epoch 380, Loss: 1.074442982673645\n",
      "Epoch 400, Loss: 1.0902538299560547\n",
      "Epoch 420, Loss: 1.1060986518859863\n",
      "Epoch 440, Loss: 1.1219043731689453\n",
      "Epoch 460, Loss: 1.1376047134399414\n",
      "Epoch 480, Loss: 1.1531386375427246\n",
      "weights of  P017 :  [array([[ 0.26014033,  0.5092778 ],\n",
      "       [ 0.05133894,  0.6931866 ],\n",
      "       [ 0.48741934, -0.4778523 ],\n",
      "       [-0.67541313, -0.05117938],\n",
      "       [ 0.4858114 , -0.5400486 ]], dtype=float32), array([ 0.01652982, -0.01652982], dtype=float32)]\n",
      "Lowest loss of  P017 :  0.8880429\n",
      "Epoch 0, Loss: 8.95423698425293\n",
      "Epoch 20, Loss: 3.9212629795074463\n",
      "Epoch 40, Loss: 1.1634823083877563\n",
      "Epoch 60, Loss: 0.975817084312439\n",
      "Epoch 80, Loss: 0.945218026638031\n",
      "Epoch 100, Loss: 0.9362194538116455\n",
      "Epoch 120, Loss: 0.9349406361579895\n",
      "Epoch 140, Loss: 0.9341007471084595\n",
      "Epoch 160, Loss: 0.9331406354904175\n",
      "Epoch 180, Loss: 0.9321683645248413\n",
      "Epoch 200, Loss: 0.9311612844467163\n",
      "Epoch 220, Loss: 0.9301213026046753\n",
      "Epoch 240, Loss: 0.9290546178817749\n",
      "Epoch 260, Loss: 0.9279673099517822\n",
      "Epoch 280, Loss: 0.926864504814148\n",
      "Epoch 300, Loss: 0.9257509708404541\n",
      "Epoch 320, Loss: 0.924631655216217\n",
      "Epoch 340, Loss: 0.9235110878944397\n",
      "Epoch 360, Loss: 0.9223936796188354\n",
      "Epoch 380, Loss: 0.9212838411331177\n",
      "Epoch 400, Loss: 0.9201858639717102\n",
      "Epoch 420, Loss: 0.9191039800643921\n",
      "Epoch 440, Loss: 0.9180423021316528\n",
      "Epoch 460, Loss: 0.9170050621032715\n",
      "Epoch 480, Loss: 0.9159958362579346\n",
      "weights of  P018 :  [array([[-0.09021247,  0.24103354],\n",
      "       [ 0.4580181 ,  0.31191126],\n",
      "       [ 0.4790613 ,  0.39118782],\n",
      "       [ 0.22105394, -0.53874326],\n",
      "       [ 0.0579435 ,  1.0486504 ]], dtype=float32), array([ 0.35201997, -0.35202003], dtype=float32)]\n",
      "Lowest loss of  P018 :  0.9150669\n",
      "Epoch 0, Loss: 3.5663492679595947\n",
      "Epoch 20, Loss: 1.250551700592041\n",
      "Epoch 40, Loss: 1.0286741256713867\n",
      "Epoch 60, Loss: 0.9957026243209839\n",
      "Epoch 80, Loss: 0.9931237101554871\n",
      "Epoch 100, Loss: 0.9887365102767944\n",
      "Epoch 120, Loss: 0.9846680164337158\n",
      "Epoch 140, Loss: 0.9804019927978516\n",
      "Epoch 160, Loss: 0.975932240486145\n",
      "Epoch 180, Loss: 0.9713008403778076\n",
      "Epoch 200, Loss: 0.9665484428405762\n",
      "Epoch 220, Loss: 0.9617132544517517\n",
      "Epoch 240, Loss: 0.9568307399749756\n",
      "Epoch 260, Loss: 0.951934278011322\n",
      "Epoch 280, Loss: 0.9470561742782593\n",
      "Epoch 300, Loss: 0.9422262907028198\n",
      "Epoch 320, Loss: 0.9374741315841675\n",
      "Epoch 340, Loss: 0.9328274726867676\n",
      "Epoch 360, Loss: 0.9283128380775452\n",
      "Epoch 380, Loss: 0.9239561557769775\n",
      "Epoch 400, Loss: 0.9197822213172913\n",
      "Epoch 420, Loss: 0.9158143401145935\n",
      "Epoch 440, Loss: 0.9120756387710571\n",
      "Epoch 460, Loss: 0.9085874557495117\n",
      "Epoch 480, Loss: 0.905370831489563\n",
      "weights of  P019 :  [array([[-0.34284762,  0.084831  ],\n",
      "       [ 0.26225096,  0.53042704],\n",
      "       [-0.71569264,  0.09991164],\n",
      "       [ 0.7485388 , -0.08275141],\n",
      "       [ 0.71174103,  0.2726599 ]], dtype=float32), array([ 0.21564794, -0.21564795], dtype=float32)]\n",
      "Lowest loss of  P019 :  0.90258443\n",
      "Epoch 0, Loss: 2.626380681991577\n",
      "Epoch 20, Loss: 1.0800604820251465\n",
      "Epoch 40, Loss: 0.973459780216217\n",
      "Epoch 60, Loss: 0.974239706993103\n",
      "Epoch 80, Loss: 0.9717780351638794\n",
      "Epoch 100, Loss: 0.9709630012512207\n",
      "Epoch 120, Loss: 0.9702949523925781\n",
      "Epoch 140, Loss: 0.9695892333984375\n",
      "Epoch 160, Loss: 0.968853771686554\n",
      "Epoch 180, Loss: 0.9680975079536438\n",
      "Epoch 200, Loss: 0.9673281311988831\n",
      "Epoch 220, Loss: 0.9665529727935791\n",
      "Epoch 240, Loss: 0.9657785892486572\n",
      "Epoch 260, Loss: 0.9650115966796875\n",
      "Epoch 280, Loss: 0.9642583131790161\n",
      "Epoch 300, Loss: 0.9635244607925415\n",
      "Epoch 320, Loss: 0.9628158807754517\n",
      "Epoch 340, Loss: 0.9621378183364868\n",
      "Epoch 360, Loss: 0.9614958763122559\n",
      "Epoch 380, Loss: 0.960894763469696\n",
      "Epoch 400, Loss: 0.9603395462036133\n",
      "Epoch 420, Loss: 0.9598349332809448\n",
      "Epoch 440, Loss: 0.9593853950500488\n",
      "Epoch 460, Loss: 0.9589953422546387\n",
      "Epoch 480, Loss: 0.9586684703826904\n",
      "weights of  P020 :  [array([[-0.20104347, -0.21835186],\n",
      "       [ 0.8805315 ,  0.23887306],\n",
      "       [ 0.06453004, -0.1555469 ],\n",
      "       [-0.8323818 ,  0.22694546],\n",
      "       [ 0.93811053,  0.36006367]], dtype=float32), array([-0.02336352,  0.02336351], dtype=float32)]\n",
      "Lowest loss of  P020 :  0.9584206\n",
      "Epoch 0, Loss: 4.195092678070068\n",
      "Epoch 20, Loss: 1.3140690326690674\n",
      "Epoch 40, Loss: 1.1354721784591675\n",
      "Epoch 60, Loss: 1.0933423042297363\n",
      "Epoch 80, Loss: 1.082541823387146\n",
      "Epoch 100, Loss: 1.0758451223373413\n",
      "Epoch 120, Loss: 1.068792462348938\n",
      "Epoch 140, Loss: 1.0614292621612549\n",
      "Epoch 160, Loss: 1.0537968873977661\n",
      "Epoch 180, Loss: 1.045998454093933\n",
      "Epoch 200, Loss: 1.0381172895431519\n",
      "Epoch 220, Loss: 1.0302233695983887\n",
      "Epoch 240, Loss: 1.022379994392395\n",
      "Epoch 260, Loss: 1.0146427154541016\n",
      "Epoch 280, Loss: 1.0070605278015137\n",
      "Epoch 300, Loss: 0.9996763467788696\n",
      "Epoch 320, Loss: 0.9925273656845093\n",
      "Epoch 340, Loss: 0.9856444597244263\n",
      "Epoch 360, Loss: 0.9790534973144531\n",
      "Epoch 380, Loss: 0.9727752804756165\n",
      "Epoch 400, Loss: 0.9668254852294922\n",
      "Epoch 420, Loss: 0.9612151980400085\n",
      "Epoch 440, Loss: 0.9559518694877625\n",
      "Epoch 460, Loss: 0.9510383605957031\n",
      "Epoch 480, Loss: 0.9464746713638306\n",
      "weights of  P021 :  [array([[-1.0390477 , -0.45887315],\n",
      "       [-0.31256396, -0.02239227],\n",
      "       [ 0.26578286,  0.502336  ],\n",
      "       [ 0.6241463 , -0.41319078],\n",
      "       [-0.31945193,  0.00258907]], dtype=float32), array([ 0.25482914, -0.25482917], dtype=float32)]\n",
      "Lowest loss of  P021 :  0.9424599\n",
      "Epoch 0, Loss: 2.7570371627807617\n",
      "Epoch 20, Loss: 1.0434143543243408\n",
      "Epoch 40, Loss: 0.8896216750144958\n",
      "Epoch 60, Loss: 0.8922867178916931\n",
      "Epoch 80, Loss: 0.8892126679420471\n",
      "Epoch 100, Loss: 0.8891093730926514\n",
      "Epoch 120, Loss: 0.8890942335128784\n",
      "Epoch 140, Loss: 0.8890883326530457\n",
      "Epoch 160, Loss: 0.8890966176986694\n",
      "Epoch 180, Loss: 0.8891205191612244\n",
      "Epoch 200, Loss: 0.8891626596450806\n",
      "Epoch 220, Loss: 0.8892256617546082\n",
      "Epoch 240, Loss: 0.8893125057220459\n",
      "Epoch 260, Loss: 0.88942551612854\n",
      "Epoch 280, Loss: 0.8895675539970398\n",
      "Epoch 300, Loss: 0.8897411823272705\n",
      "Epoch 320, Loss: 0.8899491429328918\n",
      "Epoch 340, Loss: 0.8901939392089844\n",
      "Epoch 360, Loss: 0.8904781341552734\n",
      "Epoch 380, Loss: 0.8908042907714844\n",
      "Epoch 400, Loss: 0.8911749124526978\n",
      "Epoch 420, Loss: 0.8915923833847046\n",
      "Epoch 440, Loss: 0.8920594453811646\n",
      "Epoch 460, Loss: 0.892578125\n",
      "Epoch 480, Loss: 0.8931512832641602\n",
      "weights of  P022 :  [array([[ 0.07482407,  0.00667329],\n",
      "       [-0.62137896, -0.51683086],\n",
      "       [-0.28202713, -0.6383246 ],\n",
      "       [ 0.5917319 ,  0.5758714 ],\n",
      "       [ 0.14036877,  0.40252376]], dtype=float32), array([ 0.12767126, -0.12767127], dtype=float32)]\n",
      "Lowest loss of  P022 :  0.88908833\n",
      "Epoch 0, Loss: 1.2575269937515259\n",
      "Epoch 20, Loss: 1.1537718772888184\n",
      "Epoch 40, Loss: 1.136768102645874\n",
      "Epoch 60, Loss: 1.1209715604782104\n",
      "Epoch 80, Loss: 1.1045725345611572\n",
      "Epoch 100, Loss: 1.087655782699585\n",
      "Epoch 120, Loss: 1.0708441734313965\n",
      "Epoch 140, Loss: 1.054707407951355\n",
      "Epoch 160, Loss: 1.0396978855133057\n",
      "Epoch 180, Loss: 1.0261814594268799\n",
      "Epoch 200, Loss: 1.0144469738006592\n",
      "Epoch 220, Loss: 1.0047134160995483\n",
      "Epoch 240, Loss: 0.9971358776092529\n",
      "Epoch 260, Loss: 0.9918146729469299\n",
      "Epoch 280, Loss: 0.9888007640838623\n",
      "Epoch 300, Loss: 0.9881023168563843\n",
      "Epoch 320, Loss: 0.9896925687789917\n",
      "Epoch 340, Loss: 0.9935142993927002\n",
      "Epoch 360, Loss: 0.9994869232177734\n",
      "Epoch 380, Loss: 1.0075111389160156\n",
      "Epoch 400, Loss: 1.0174742937088013\n",
      "Epoch 420, Loss: 1.0292534828186035\n",
      "Epoch 440, Loss: 1.042720913887024\n",
      "Epoch 460, Loss: 1.0577456951141357\n",
      "Epoch 480, Loss: 1.0741965770721436\n",
      "weights of  P023 :  [array([[ 0.7078453 ,  0.5364337 ],\n",
      "       [-0.77626634, -0.25693482],\n",
      "       [-0.02642746,  0.3086634 ],\n",
      "       [-0.42798   , -0.6943252 ],\n",
      "       [ 0.81039435,  0.57818013]], dtype=float32), array([-0.44756582,  0.44756567], dtype=float32)]\n",
      "Lowest loss of  P023 :  0.9880576\n",
      "Epoch 0, Loss: 3.4517152309417725\n",
      "Epoch 20, Loss: 1.0971624851226807\n",
      "Epoch 40, Loss: 0.8813812136650085\n",
      "Epoch 60, Loss: 0.8491696715354919\n",
      "Epoch 80, Loss: 0.8497096300125122\n",
      "Epoch 100, Loss: 0.8491561412811279\n",
      "Epoch 120, Loss: 0.8491530418395996\n",
      "Epoch 140, Loss: 0.8491934537887573\n",
      "Epoch 160, Loss: 0.8492428064346313\n",
      "Epoch 180, Loss: 0.8493038415908813\n",
      "Epoch 200, Loss: 0.8493776917457581\n",
      "Epoch 220, Loss: 0.8494654297828674\n",
      "Epoch 240, Loss: 0.8495681881904602\n",
      "Epoch 260, Loss: 0.8496873378753662\n",
      "Epoch 280, Loss: 0.8498239517211914\n",
      "Epoch 300, Loss: 0.8499791622161865\n",
      "Epoch 320, Loss: 0.8501541614532471\n",
      "Epoch 340, Loss: 0.8503501415252686\n",
      "Epoch 360, Loss: 0.8505679965019226\n",
      "Epoch 380, Loss: 0.8508089184761047\n",
      "Epoch 400, Loss: 0.8510738015174866\n",
      "Epoch 420, Loss: 0.8513636589050293\n",
      "Epoch 440, Loss: 0.8516796827316284\n",
      "Epoch 460, Loss: 0.8520222902297974\n",
      "Epoch 480, Loss: 0.852392852306366\n",
      "weights of  P024 :  [array([[ 0.28781617,  0.03567832],\n",
      "       [-0.02287748,  0.74214715],\n",
      "       [ 0.48185733,  0.4870886 ],\n",
      "       [-0.20623097, -0.6414049 ],\n",
      "       [-0.0954004 , -0.21908876]], dtype=float32), array([ 0.12380778, -0.12380778], dtype=float32)]\n",
      "Lowest loss of  P024 :  0.84910053\n",
      "Epoch 0, Loss: 2.015512228012085\n",
      "Epoch 20, Loss: 1.150025486946106\n",
      "Epoch 40, Loss: 1.1516990661621094\n",
      "Epoch 60, Loss: 1.1462746858596802\n",
      "Epoch 80, Loss: 1.1439570188522339\n",
      "Epoch 100, Loss: 1.14240562915802\n",
      "Epoch 120, Loss: 1.1408872604370117\n",
      "Epoch 140, Loss: 1.1393170356750488\n",
      "Epoch 160, Loss: 1.1376967430114746\n",
      "Epoch 180, Loss: 1.1360430717468262\n",
      "Epoch 200, Loss: 1.1343737840652466\n",
      "Epoch 220, Loss: 1.1327087879180908\n",
      "Epoch 240, Loss: 1.1310653686523438\n",
      "Epoch 260, Loss: 1.129460334777832\n",
      "Epoch 280, Loss: 1.1279101371765137\n",
      "Epoch 300, Loss: 1.1264307498931885\n",
      "Epoch 320, Loss: 1.125037431716919\n",
      "Epoch 340, Loss: 1.1237452030181885\n",
      "Epoch 360, Loss: 1.1225688457489014\n",
      "Epoch 380, Loss: 1.1215230226516724\n",
      "Epoch 400, Loss: 1.1206213235855103\n",
      "Epoch 420, Loss: 1.119877576828003\n",
      "Epoch 440, Loss: 1.1193056106567383\n",
      "Epoch 460, Loss: 1.1189184188842773\n",
      "Epoch 480, Loss: 1.1187286376953125\n",
      "weights of  P025 :  [array([[-0.36601806, -0.8778608 ],\n",
      "       [-0.6489328 ,  0.9949425 ],\n",
      "       [-0.77035606,  0.4702419 ],\n",
      "       [ 0.7191992 , -0.700255  ],\n",
      "       [ 0.04936687, -0.53690535]], dtype=float32), array([-0.22859998,  0.22859997], dtype=float32)]\n",
      "Lowest loss of  P025 :  1.1187109\n",
      "Epoch 0, Loss: 5.65619421005249\n",
      "Epoch 20, Loss: 1.0158804655075073\n",
      "Epoch 40, Loss: 1.005571722984314\n",
      "Epoch 60, Loss: 1.0035326480865479\n",
      "Epoch 80, Loss: 0.996139645576477\n",
      "Epoch 100, Loss: 0.9884092211723328\n",
      "Epoch 120, Loss: 0.9817029237747192\n",
      "Epoch 140, Loss: 0.9746807217597961\n",
      "Epoch 160, Loss: 0.9674888849258423\n",
      "Epoch 180, Loss: 0.9601768255233765\n",
      "Epoch 200, Loss: 0.9528228640556335\n",
      "Epoch 220, Loss: 0.945500373840332\n",
      "Epoch 240, Loss: 0.9382755756378174\n",
      "Epoch 260, Loss: 0.9312069416046143\n",
      "Epoch 280, Loss: 0.9243470430374146\n",
      "Epoch 300, Loss: 0.9177424311637878\n",
      "Epoch 320, Loss: 0.9114336967468262\n",
      "Epoch 340, Loss: 0.9054564833641052\n",
      "Epoch 360, Loss: 0.8998411893844604\n",
      "Epoch 380, Loss: 0.8946132659912109\n",
      "Epoch 400, Loss: 0.8897936344146729\n",
      "Epoch 420, Loss: 0.8853988647460938\n",
      "Epoch 440, Loss: 0.881441593170166\n",
      "Epoch 460, Loss: 0.8779308795928955\n",
      "Epoch 480, Loss: 0.8748719096183777\n",
      "weights of  P026 :  [array([[ 0.13375036, -0.42172614],\n",
      "       [ 0.17456096, -0.16538572],\n",
      "       [-0.89080125, -0.27016765],\n",
      "       [-0.07044999, -0.4155405 ],\n",
      "       [-0.8188099 , -0.01376529]], dtype=float32), array([ 0.32949606, -0.32949603], dtype=float32)]\n",
      "Lowest loss of  P026 :  0.872387\n",
      "Epoch 0, Loss: 1.966545820236206\n",
      "Epoch 20, Loss: 1.0410099029541016\n",
      "Epoch 40, Loss: 1.0480015277862549\n",
      "Epoch 60, Loss: 1.039773941040039\n",
      "Epoch 80, Loss: 1.037095069885254\n",
      "Epoch 100, Loss: 1.0352765321731567\n",
      "Epoch 120, Loss: 1.0334739685058594\n",
      "Epoch 140, Loss: 1.0316269397735596\n",
      "Epoch 160, Loss: 1.0297482013702393\n",
      "Epoch 180, Loss: 1.0278582572937012\n",
      "Epoch 200, Loss: 1.025978446006775\n",
      "Epoch 220, Loss: 1.0241308212280273\n",
      "Epoch 240, Loss: 1.0223366022109985\n",
      "Epoch 260, Loss: 1.020614743232727\n",
      "Epoch 280, Loss: 1.0189824104309082\n",
      "Epoch 300, Loss: 1.017455816268921\n",
      "Epoch 320, Loss: 1.0160497426986694\n",
      "Epoch 340, Loss: 1.014777660369873\n",
      "Epoch 360, Loss: 1.0136522054672241\n",
      "Epoch 380, Loss: 1.0126848220825195\n",
      "Epoch 400, Loss: 1.011885643005371\n",
      "Epoch 420, Loss: 1.0112648010253906\n",
      "Epoch 440, Loss: 1.0108306407928467\n",
      "Epoch 460, Loss: 1.0105912685394287\n",
      "Epoch 480, Loss: 1.0105539560317993\n",
      "weights of  P027 :  [array([[-0.4562356 , -0.79780376],\n",
      "       [-0.535172  , -0.12843817],\n",
      "       [ 0.39700916,  0.9687825 ],\n",
      "       [ 0.22010155, -0.8156801 ],\n",
      "       [-0.3831408 ,  0.56736344]], dtype=float32), array([-0.11877623,  0.11877618], dtype=float32)]\n",
      "Lowest loss of  P027 :  1.0105435\n",
      "Epoch 0, Loss: 10.81838607788086\n",
      "Epoch 20, Loss: 5.532915115356445\n",
      "Epoch 40, Loss: 0.9544879794120789\n",
      "Epoch 60, Loss: 0.9443636536598206\n",
      "Epoch 80, Loss: 0.9368693232536316\n",
      "Epoch 100, Loss: 0.9372644424438477\n",
      "Epoch 120, Loss: 0.9363244771957397\n",
      "Epoch 140, Loss: 0.9357539415359497\n",
      "Epoch 160, Loss: 0.9354569911956787\n",
      "Epoch 180, Loss: 0.9351488351821899\n",
      "Epoch 200, Loss: 0.9348242878913879\n",
      "Epoch 220, Loss: 0.934489905834198\n",
      "Epoch 240, Loss: 0.9341469407081604\n",
      "Epoch 260, Loss: 0.9337966442108154\n",
      "Epoch 280, Loss: 0.9334405660629272\n",
      "Epoch 300, Loss: 0.9330801963806152\n",
      "Epoch 320, Loss: 0.9327171444892883\n",
      "Epoch 340, Loss: 0.9323525428771973\n",
      "Epoch 360, Loss: 0.9319881796836853\n",
      "Epoch 380, Loss: 0.9316250681877136\n",
      "Epoch 400, Loss: 0.9312647581100464\n",
      "Epoch 420, Loss: 0.9309085607528687\n",
      "Epoch 440, Loss: 0.9305576682090759\n",
      "Epoch 460, Loss: 0.930213451385498\n",
      "Epoch 480, Loss: 0.9298771023750305\n",
      "weights of  P028 :  [array([[-0.3284524 , -0.83127195],\n",
      "       [-0.43330872, -0.16730137],\n",
      "       [ 0.85371256,  0.3762133 ],\n",
      "       [-0.43020636,  0.1507596 ],\n",
      "       [-0.36991897, -0.41403857]], dtype=float32), array([-0.45530155,  0.45530155], dtype=float32)]\n",
      "Lowest loss of  P028 :  0.92956614\n",
      "Epoch 0, Loss: 4.34389591217041\n",
      "Epoch 20, Loss: 1.1445388793945312\n",
      "Epoch 40, Loss: 1.001111388206482\n",
      "Epoch 60, Loss: 0.9535664916038513\n",
      "Epoch 80, Loss: 0.9418559074401855\n",
      "Epoch 100, Loss: 0.9352147579193115\n",
      "Epoch 120, Loss: 0.928507924079895\n",
      "Epoch 140, Loss: 0.9219655990600586\n",
      "Epoch 160, Loss: 0.9156993627548218\n",
      "Epoch 180, Loss: 0.9098595380783081\n",
      "Epoch 200, Loss: 0.9045820236206055\n",
      "Epoch 220, Loss: 0.8999840021133423\n",
      "Epoch 240, Loss: 0.8961656093597412\n",
      "Epoch 260, Loss: 0.8932122588157654\n",
      "Epoch 280, Loss: 0.8911953568458557\n",
      "Epoch 300, Loss: 0.8901733160018921\n",
      "Epoch 320, Loss: 0.8901926279067993\n",
      "Epoch 340, Loss: 0.8912888765335083\n",
      "Epoch 360, Loss: 0.8934876918792725\n",
      "Epoch 380, Loss: 0.8968054056167603\n",
      "Epoch 400, Loss: 0.9012512564659119\n",
      "Epoch 420, Loss: 0.9068266153335571\n",
      "Epoch 440, Loss: 0.9135273694992065\n",
      "Epoch 460, Loss: 0.9213431477546692\n",
      "Epoch 480, Loss: 0.9302600622177124\n",
      "weights of  P029 :  [array([[ 0.42953047, -0.18088812],\n",
      "       [ 0.34546036,  1.3640788 ],\n",
      "       [-0.38943914, -0.4458801 ],\n",
      "       [ 0.6411216 , -0.15332074],\n",
      "       [-0.8572055 , -0.25108293]], dtype=float32), array([ 0.04775076, -0.04775077], dtype=float32)]\n",
      "Lowest loss of  P029 :  0.8900503\n",
      "Epoch 0, Loss: 1.8697495460510254\n",
      "Epoch 20, Loss: 0.9480249881744385\n",
      "Epoch 40, Loss: 0.9521850943565369\n",
      "Epoch 60, Loss: 0.9471786618232727\n",
      "Epoch 80, Loss: 0.9461610913276672\n",
      "Epoch 100, Loss: 0.9460239410400391\n",
      "Epoch 120, Loss: 0.9459993839263916\n",
      "Epoch 140, Loss: 0.9459937810897827\n",
      "Epoch 160, Loss: 0.9459954500198364\n",
      "Epoch 180, Loss: 0.9460079669952393\n",
      "Epoch 200, Loss: 0.9460338354110718\n",
      "Epoch 220, Loss: 0.9460746049880981\n",
      "Epoch 240, Loss: 0.9461319446563721\n",
      "Epoch 260, Loss: 0.9462078809738159\n",
      "Epoch 280, Loss: 0.9463037848472595\n",
      "Epoch 300, Loss: 0.94642174243927\n",
      "Epoch 320, Loss: 0.9465631246566772\n",
      "Epoch 340, Loss: 0.9467295408248901\n",
      "Epoch 360, Loss: 0.9469228386878967\n",
      "Epoch 380, Loss: 0.9471441507339478\n",
      "Epoch 400, Loss: 0.9473953247070312\n",
      "Epoch 420, Loss: 0.947677731513977\n",
      "Epoch 440, Loss: 0.9479926824569702\n",
      "Epoch 460, Loss: 0.9483416080474854\n",
      "Epoch 480, Loss: 0.9487259387969971\n",
      "weights of  P030 :  [array([[ 0.69775677, -0.41602087],\n",
      "       [ 0.67123556,  0.6536651 ],\n",
      "       [-0.4767047 ,  0.14729634],\n",
      "       [-0.6180919 , -0.23437989],\n",
      "       [ 0.48881802,  0.42305383]], dtype=float32), array([ 0.00497713, -0.00497714], dtype=float32)]\n",
      "Lowest loss of  P030 :  0.9459923\n",
      "Epoch 0, Loss: 1.0886657238006592\n",
      "Epoch 20, Loss: 0.9400584697723389\n",
      "Epoch 40, Loss: 0.9353228807449341\n",
      "Epoch 60, Loss: 0.9361759424209595\n",
      "Epoch 80, Loss: 0.9369444251060486\n",
      "Epoch 100, Loss: 0.9382803440093994\n",
      "Epoch 120, Loss: 0.9399529099464417\n",
      "Epoch 140, Loss: 0.9419852495193481\n",
      "Epoch 160, Loss: 0.9444204568862915\n",
      "Epoch 180, Loss: 0.9472755789756775\n",
      "Epoch 200, Loss: 0.9505634903907776\n",
      "Epoch 220, Loss: 0.954291820526123\n",
      "Epoch 240, Loss: 0.9584625959396362\n",
      "Epoch 260, Loss: 0.963072657585144\n",
      "Epoch 280, Loss: 0.9681134223937988\n",
      "Epoch 300, Loss: 0.9735718965530396\n",
      "Epoch 320, Loss: 0.9794309139251709\n",
      "Epoch 340, Loss: 0.9856691360473633\n",
      "Epoch 360, Loss: 0.9922628402709961\n",
      "Epoch 380, Loss: 0.9991849660873413\n",
      "Epoch 400, Loss: 1.0064070224761963\n",
      "Epoch 420, Loss: 1.0138983726501465\n",
      "Epoch 440, Loss: 1.0216275453567505\n",
      "Epoch 460, Loss: 1.0295624732971191\n",
      "Epoch 480, Loss: 1.0376708507537842\n",
      "weights of  P031 :  [array([[ 0.50036377, -0.42789307],\n",
      "       [-0.02416353,  0.2076147 ],\n",
      "       [ 0.49771547, -0.5128405 ],\n",
      "       [-0.5233297 ,  0.8010991 ],\n",
      "       [-0.37947127, -0.79135317]], dtype=float32), array([ 0.02326996, -0.02326995], dtype=float32)]\n",
      "Lowest loss of  P031 :  0.9347421\n",
      "Epoch 0, Loss: 2.9574973583221436\n",
      "Epoch 20, Loss: 1.153324007987976\n",
      "Epoch 40, Loss: 0.9769822359085083\n",
      "Epoch 60, Loss: 0.9732471108436584\n",
      "Epoch 80, Loss: 0.9676434993743896\n",
      "Epoch 100, Loss: 0.9641656279563904\n",
      "Epoch 120, Loss: 0.9603920578956604\n",
      "Epoch 140, Loss: 0.9565376043319702\n",
      "Epoch 160, Loss: 0.9526520371437073\n",
      "Epoch 180, Loss: 0.9487950801849365\n",
      "Epoch 200, Loss: 0.9450280666351318\n",
      "Epoch 220, Loss: 0.9414099454879761\n",
      "Epoch 240, Loss: 0.9379968643188477\n",
      "Epoch 260, Loss: 0.9348419308662415\n",
      "Epoch 280, Loss: 0.9319958686828613\n",
      "Epoch 300, Loss: 0.929506778717041\n",
      "Epoch 320, Loss: 0.9274204969406128\n",
      "Epoch 340, Loss: 0.9257804155349731\n",
      "Epoch 360, Loss: 0.9246279001235962\n",
      "Epoch 380, Loss: 0.9240016937255859\n",
      "Epoch 400, Loss: 0.9239387512207031\n",
      "Epoch 420, Loss: 0.9244735240936279\n",
      "Epoch 440, Loss: 0.9256385564804077\n",
      "Epoch 460, Loss: 0.9274641275405884\n",
      "Epoch 480, Loss: 0.9299783706665039\n",
      "weights of  P032 :  [array([[ 0.00228443, -1.0440036 ],\n",
      "       [-0.68351185, -0.45732746],\n",
      "       [-0.18377072,  0.1861414 ],\n",
      "       [-0.04598479,  0.67713135],\n",
      "       [ 0.729374  ,  0.02523127]], dtype=float32), array([-0.21059503,  0.210595  ], dtype=float32)]\n",
      "Lowest loss of  P032 :  0.92389417\n",
      "Epoch 0, Loss: 1.5172123908996582\n",
      "Epoch 20, Loss: 1.2299672365188599\n",
      "Epoch 40, Loss: 1.183564305305481\n",
      "Epoch 60, Loss: 1.1369495391845703\n",
      "Epoch 80, Loss: 1.0888055562973022\n",
      "Epoch 100, Loss: 1.0434579849243164\n",
      "Epoch 120, Loss: 1.0028150081634521\n",
      "Epoch 140, Loss: 0.9682302474975586\n",
      "Epoch 160, Loss: 0.9403532147407532\n",
      "Epoch 180, Loss: 0.9192802309989929\n",
      "Epoch 200, Loss: 0.9047092199325562\n",
      "Epoch 220, Loss: 0.8960872888565063\n",
      "Epoch 240, Loss: 0.8927298188209534\n",
      "Epoch 260, Loss: 0.8939113616943359\n",
      "Epoch 280, Loss: 0.8989273309707642\n",
      "Epoch 300, Loss: 0.9071279764175415\n",
      "Epoch 320, Loss: 0.9179359674453735\n",
      "Epoch 340, Loss: 0.9308491945266724\n",
      "Epoch 360, Loss: 0.9454382658004761\n",
      "Epoch 380, Loss: 0.9613398313522339\n",
      "Epoch 400, Loss: 0.9782484769821167\n",
      "Epoch 420, Loss: 0.9959088563919067\n",
      "Epoch 440, Loss: 1.0141072273254395\n",
      "Epoch 460, Loss: 1.0326659679412842\n",
      "Epoch 480, Loss: 1.0514378547668457\n",
      "weights of  P033 :  [array([[-0.08980054, -0.93169785],\n",
      "       [-0.48529637,  0.00765742],\n",
      "       [ 0.65212435, -0.2644484 ],\n",
      "       [-0.4690114 , -0.5061208 ],\n",
      "       [-0.5329302 ,  0.83264285]], dtype=float32), array([-0.1449924 ,  0.14499237], dtype=float32)]\n",
      "Lowest loss of  P033 :  0.89262605\n",
      "Epoch 0, Loss: 7.995872497558594\n",
      "Epoch 20, Loss: 2.789111852645874\n",
      "Epoch 40, Loss: 1.1898618936538696\n",
      "Epoch 60, Loss: 0.9257245063781738\n",
      "Epoch 80, Loss: 0.8845541477203369\n",
      "Epoch 100, Loss: 0.8777885437011719\n",
      "Epoch 120, Loss: 0.8736486434936523\n",
      "Epoch 140, Loss: 0.8694736957550049\n",
      "Epoch 160, Loss: 0.865498423576355\n",
      "Epoch 180, Loss: 0.8617618083953857\n",
      "Epoch 200, Loss: 0.8583537936210632\n",
      "Epoch 220, Loss: 0.855343222618103\n",
      "Epoch 240, Loss: 0.8527921438217163\n",
      "Epoch 260, Loss: 0.8507552146911621\n",
      "Epoch 280, Loss: 0.849278450012207\n",
      "Epoch 300, Loss: 0.8484004735946655\n",
      "Epoch 320, Loss: 0.8481529355049133\n",
      "Epoch 340, Loss: 0.8485612273216248\n",
      "Epoch 360, Loss: 0.8496451377868652\n",
      "Epoch 380, Loss: 0.8514183759689331\n",
      "Epoch 400, Loss: 0.8538913130760193\n",
      "Epoch 420, Loss: 0.8570688962936401\n",
      "Epoch 440, Loss: 0.8609530329704285\n",
      "Epoch 460, Loss: 0.865542471408844\n",
      "Epoch 480, Loss: 0.8708329200744629\n",
      "weights of  P034 :  [array([[ 1.218691  ,  0.045485  ],\n",
      "       [-0.5428669 ,  0.20449242],\n",
      "       [-0.508288  , -0.1123326 ],\n",
      "       [-0.54535675, -0.08522117],\n",
      "       [-0.32682467, -1.0611231 ]], dtype=float32), array([-0.44071522,  0.44071522], dtype=float32)]\n",
      "Lowest loss of  P034 :  0.8481486\n",
      "Epoch 0, Loss: 1.3334083557128906\n",
      "Epoch 20, Loss: 1.2348440885543823\n",
      "Epoch 40, Loss: 1.2090703248977661\n",
      "Epoch 60, Loss: 1.1855692863464355\n",
      "Epoch 80, Loss: 1.161489725112915\n",
      "Epoch 100, Loss: 1.1382976770401\n",
      "Epoch 120, Loss: 1.1168153285980225\n",
      "Epoch 140, Loss: 1.0976030826568604\n",
      "Epoch 160, Loss: 1.0809012651443481\n",
      "Epoch 180, Loss: 1.0667085647583008\n",
      "Epoch 200, Loss: 1.054857611656189\n",
      "Epoch 220, Loss: 1.0450806617736816\n",
      "Epoch 240, Loss: 1.0370618104934692\n",
      "Epoch 260, Loss: 1.0304758548736572\n",
      "Epoch 280, Loss: 1.0250146389007568\n",
      "Epoch 300, Loss: 1.0204023122787476\n",
      "Epoch 320, Loss: 1.0164039134979248\n",
      "Epoch 340, Loss: 1.0128264427185059\n",
      "Epoch 360, Loss: 1.0095183849334717\n",
      "Epoch 380, Loss: 1.0063642263412476\n",
      "Epoch 400, Loss: 1.0032811164855957\n",
      "Epoch 420, Loss: 1.000212550163269\n",
      "Epoch 440, Loss: 0.9971243143081665\n",
      "Epoch 460, Loss: 0.9939988851547241\n",
      "Epoch 480, Loss: 0.9908318519592285\n",
      "weights of  P035 :  [array([[ 0.9811308 , -0.9166213 ],\n",
      "       [-0.3224926 ,  0.16540474],\n",
      "       [-0.15319999, -0.01524696],\n",
      "       [-0.31017485,  0.3179497 ],\n",
      "       [ 0.47127348,  0.9997506 ]], dtype=float32), array([-0.7942123,  0.7942124], dtype=float32)]\n",
      "Lowest loss of  P035 :  0.98778933\n",
      "Epoch 0, Loss: 5.315034866333008\n",
      "Epoch 20, Loss: 0.9811887741088867\n",
      "Epoch 40, Loss: 0.9811812043190002\n",
      "Epoch 60, Loss: 0.9861912727355957\n",
      "Epoch 80, Loss: 0.977042555809021\n",
      "Epoch 100, Loss: 0.9753034114837646\n",
      "Epoch 120, Loss: 0.9745283126831055\n",
      "Epoch 140, Loss: 0.9736580848693848\n",
      "Epoch 160, Loss: 0.9727509617805481\n",
      "Epoch 180, Loss: 0.9718054533004761\n",
      "Epoch 200, Loss: 0.9708207845687866\n",
      "Epoch 220, Loss: 0.9698008298873901\n",
      "Epoch 240, Loss: 0.9687504768371582\n",
      "Epoch 260, Loss: 0.9676746129989624\n",
      "Epoch 280, Loss: 0.9665776491165161\n",
      "Epoch 300, Loss: 0.9654637575149536\n",
      "Epoch 320, Loss: 0.9643366932868958\n",
      "Epoch 340, Loss: 0.9632008075714111\n",
      "Epoch 360, Loss: 0.962059497833252\n",
      "Epoch 380, Loss: 0.9609167575836182\n",
      "Epoch 400, Loss: 0.9597756862640381\n",
      "Epoch 420, Loss: 0.9586400985717773\n",
      "Epoch 440, Loss: 0.9575132727622986\n",
      "Epoch 460, Loss: 0.9563984870910645\n",
      "Epoch 480, Loss: 0.9552991986274719\n",
      "weights of  P036 :  [array([[ 0.93659264,  0.41100186],\n",
      "       [ 0.81288445,  0.35957807],\n",
      "       [ 0.2666715 ,  0.31604695],\n",
      "       [-0.5879413 ,  0.34157315],\n",
      "       [-0.04872798, -0.33499107]], dtype=float32), array([-0.20628755,  0.20628758], dtype=float32)]\n",
      "Lowest loss of  P036 :  0.95427203\n",
      "Epoch 0, Loss: 8.500682830810547\n",
      "Epoch 20, Loss: 3.2665305137634277\n",
      "Epoch 40, Loss: 1.3789931535720825\n",
      "Epoch 60, Loss: 1.11651611328125\n",
      "Epoch 80, Loss: 1.0754135847091675\n",
      "Epoch 100, Loss: 1.0697617530822754\n",
      "Epoch 120, Loss: 1.069053292274475\n",
      "Epoch 140, Loss: 1.0682194232940674\n",
      "Epoch 160, Loss: 1.067363977432251\n",
      "Epoch 180, Loss: 1.0664641857147217\n",
      "Epoch 200, Loss: 1.0655169486999512\n",
      "Epoch 220, Loss: 1.064524531364441\n",
      "Epoch 240, Loss: 1.063489556312561\n",
      "Epoch 260, Loss: 1.062415361404419\n",
      "Epoch 280, Loss: 1.061303734779358\n",
      "Epoch 300, Loss: 1.0601577758789062\n",
      "Epoch 320, Loss: 1.0589793920516968\n",
      "Epoch 340, Loss: 1.057770848274231\n",
      "Epoch 360, Loss: 1.0565340518951416\n",
      "Epoch 380, Loss: 1.0552709102630615\n",
      "Epoch 400, Loss: 1.053983211517334\n",
      "Epoch 420, Loss: 1.0526728630065918\n",
      "Epoch 440, Loss: 1.0513415336608887\n",
      "Epoch 460, Loss: 1.0499910116195679\n",
      "Epoch 480, Loss: 1.0486228466033936\n",
      "weights of  P037 :  [array([[-0.94030887, -0.6806208 ],\n",
      "       [ 0.39013985,  0.97266203],\n",
      "       [-0.5140127 ,  0.26858568],\n",
      "       [ 0.4381887 , -0.3232285 ],\n",
      "       [ 0.4657375 , -0.35934818]], dtype=float32), array([ 0.3406554 , -0.34065545], dtype=float32)]\n",
      "Lowest loss of  P037 :  1.0473081\n",
      "Epoch 0, Loss: 8.038078308105469\n",
      "Epoch 20, Loss: 2.808819055557251\n",
      "Epoch 40, Loss: 1.285386323928833\n",
      "Epoch 60, Loss: 1.032228708267212\n",
      "Epoch 80, Loss: 0.9954671859741211\n",
      "Epoch 100, Loss: 0.9926679134368896\n",
      "Epoch 120, Loss: 0.9925919771194458\n",
      "Epoch 140, Loss: 0.9923851490020752\n",
      "Epoch 160, Loss: 0.9922372102737427\n",
      "Epoch 180, Loss: 0.9920812845230103\n",
      "Epoch 200, Loss: 0.9919220209121704\n",
      "Epoch 220, Loss: 0.9917583465576172\n",
      "Epoch 240, Loss: 0.9915905594825745\n",
      "Epoch 260, Loss: 0.9914200305938721\n",
      "Epoch 280, Loss: 0.9912475347518921\n",
      "Epoch 300, Loss: 0.9910738468170166\n",
      "Epoch 320, Loss: 0.9908998012542725\n",
      "Epoch 340, Loss: 0.9907263517379761\n",
      "Epoch 360, Loss: 0.9905540943145752\n",
      "Epoch 380, Loss: 0.9903839826583862\n",
      "Epoch 400, Loss: 0.9902166128158569\n",
      "Epoch 420, Loss: 0.9900528192520142\n",
      "Epoch 440, Loss: 0.9898931384086609\n",
      "Epoch 460, Loss: 0.9897387027740479\n",
      "Epoch 480, Loss: 0.9895899295806885\n",
      "weights of  P038 :  [array([[-1.0846611 , -0.29233816],\n",
      "       [-0.10765406,  0.27995637],\n",
      "       [ 0.602343  , -0.28994563],\n",
      "       [ 0.2633078 , -0.28730583],\n",
      "       [-0.8780025 , -0.5003345 ]], dtype=float32), array([ 0.27265134, -0.2726514 ], dtype=float32)]\n",
      "Lowest loss of  P038 :  0.9894544\n",
      "Epoch 0, Loss: 1.748579740524292\n",
      "Epoch 20, Loss: 0.9464536905288696\n",
      "Epoch 40, Loss: 0.9396883249282837\n",
      "Epoch 60, Loss: 0.9367547035217285\n",
      "Epoch 80, Loss: 0.9348421692848206\n",
      "Epoch 100, Loss: 0.9333943724632263\n",
      "Epoch 120, Loss: 0.9319161772727966\n",
      "Epoch 140, Loss: 0.9303460121154785\n",
      "Epoch 160, Loss: 0.9287006855010986\n",
      "Epoch 180, Loss: 0.9269973039627075\n",
      "Epoch 200, Loss: 0.925250768661499\n",
      "Epoch 220, Loss: 0.923475444316864\n",
      "Epoch 240, Loss: 0.9216848611831665\n",
      "Epoch 260, Loss: 0.9198918342590332\n",
      "Epoch 280, Loss: 0.9181088209152222\n",
      "Epoch 300, Loss: 0.9163476824760437\n",
      "Epoch 320, Loss: 0.9146201610565186\n",
      "Epoch 340, Loss: 0.9129372835159302\n",
      "Epoch 360, Loss: 0.9113097786903381\n",
      "Epoch 380, Loss: 0.9097484946250916\n",
      "Epoch 400, Loss: 0.9082635045051575\n",
      "Epoch 420, Loss: 0.9068648815155029\n",
      "Epoch 440, Loss: 0.9055625200271606\n",
      "Epoch 460, Loss: 0.9043656587600708\n",
      "Epoch 480, Loss: 0.9032838344573975\n",
      "weights of  P039 :  [array([[-0.33648494,  0.05730189],\n",
      "       [ 0.6624219 ,  0.67834646],\n",
      "       [ 0.5931002 ,  0.19502042],\n",
      "       [ 0.1069141 , -0.18909907],\n",
      "       [-0.7266679 , -0.40101075]], dtype=float32), array([ 0.3114495, -0.3114494], dtype=float32)]\n",
      "Lowest loss of  P039 :  0.9023705\n",
      "Epoch 0, Loss: 5.346505641937256\n",
      "Epoch 20, Loss: 0.9601224064826965\n",
      "Epoch 40, Loss: 0.9046480655670166\n",
      "Epoch 60, Loss: 0.9119158983230591\n",
      "Epoch 80, Loss: 0.9044334888458252\n",
      "Epoch 100, Loss: 0.9032635688781738\n",
      "Epoch 120, Loss: 0.9033572673797607\n",
      "Epoch 140, Loss: 0.9035176634788513\n",
      "Epoch 160, Loss: 0.9037131071090698\n",
      "Epoch 180, Loss: 0.9039368629455566\n",
      "Epoch 200, Loss: 0.904187798500061\n",
      "Epoch 220, Loss: 0.9044675827026367\n",
      "Epoch 240, Loss: 0.9047773480415344\n",
      "Epoch 260, Loss: 0.9051178693771362\n",
      "Epoch 280, Loss: 0.9054901599884033\n",
      "Epoch 300, Loss: 0.9058949947357178\n",
      "Epoch 320, Loss: 0.9063333868980408\n",
      "Epoch 340, Loss: 0.9068061113357544\n",
      "Epoch 360, Loss: 0.907313883304596\n",
      "Epoch 380, Loss: 0.9078577160835266\n",
      "Epoch 400, Loss: 0.9084384441375732\n",
      "Epoch 420, Loss: 0.9090569019317627\n",
      "Epoch 440, Loss: 0.9097141623497009\n",
      "Epoch 460, Loss: 0.9104110598564148\n",
      "Epoch 480, Loss: 0.9111487865447998\n",
      "weights of  P040 :  [array([[ 0.53186154,  0.48754635],\n",
      "       [-0.7513975 , -0.47389993],\n",
      "       [ 0.36785978, -0.64691794],\n",
      "       [ 0.03558989, -0.05746713],\n",
      "       [-0.580968  ,  0.19941828]], dtype=float32), array([ 0.21327853, -0.21327853], dtype=float32)]\n",
      "Lowest loss of  P040 :  0.90303993\n",
      "Epoch 0, Loss: 4.286576747894287\n",
      "Epoch 20, Loss: 1.150201678276062\n",
      "Epoch 40, Loss: 1.0058727264404297\n",
      "Epoch 60, Loss: 0.9618462920188904\n",
      "Epoch 80, Loss: 0.9535014033317566\n",
      "Epoch 100, Loss: 0.9500821828842163\n",
      "Epoch 120, Loss: 0.9462265968322754\n",
      "Epoch 140, Loss: 0.9422168731689453\n",
      "Epoch 160, Loss: 0.9380173683166504\n",
      "Epoch 180, Loss: 0.9336550235748291\n",
      "Epoch 200, Loss: 0.929164707660675\n",
      "Epoch 220, Loss: 0.9245790243148804\n",
      "Epoch 240, Loss: 0.9199280738830566\n",
      "Epoch 260, Loss: 0.915239691734314\n",
      "Epoch 280, Loss: 0.9105402231216431\n",
      "Epoch 300, Loss: 0.905854344367981\n",
      "Epoch 320, Loss: 0.9012055397033691\n",
      "Epoch 340, Loss: 0.8966154456138611\n",
      "Epoch 360, Loss: 0.8921048641204834\n",
      "Epoch 380, Loss: 0.8876929879188538\n",
      "Epoch 400, Loss: 0.8833981156349182\n",
      "Epoch 420, Loss: 0.8792374730110168\n",
      "Epoch 440, Loss: 0.8752269744873047\n",
      "Epoch 460, Loss: 0.871381402015686\n",
      "Epoch 480, Loss: 0.8677147030830383\n",
      "weights of  P041 :  [array([[-0.45241988, -0.03687882],\n",
      "       [ 0.03038919,  0.4483469 ],\n",
      "       [ 0.5114335 ,  0.67576057],\n",
      "       [-0.08422535, -0.22661732],\n",
      "       [ 0.4644778 , -0.49052802]], dtype=float32), array([ 0.07144735, -0.07144734], dtype=float32)]\n",
      "Lowest loss of  P041 :  0.8644085\n",
      "Epoch 0, Loss: 4.649659156799316\n",
      "Epoch 20, Loss: 0.8982890248298645\n",
      "Epoch 40, Loss: 0.8529332876205444\n",
      "Epoch 60, Loss: 0.8327940106391907\n",
      "Epoch 80, Loss: 0.8173763751983643\n",
      "Epoch 100, Loss: 0.8115347027778625\n",
      "Epoch 120, Loss: 0.805809497833252\n",
      "Epoch 140, Loss: 0.8003589510917664\n",
      "Epoch 160, Loss: 0.7952929735183716\n",
      "Epoch 180, Loss: 0.7907666563987732\n",
      "Epoch 200, Loss: 0.7868921160697937\n",
      "Epoch 220, Loss: 0.78376305103302\n",
      "Epoch 240, Loss: 0.7814563512802124\n",
      "Epoch 260, Loss: 0.7800307869911194\n",
      "Epoch 280, Loss: 0.7795298099517822\n",
      "Epoch 300, Loss: 0.7799825072288513\n",
      "Epoch 320, Loss: 0.7814053297042847\n",
      "Epoch 340, Loss: 0.7838035821914673\n",
      "Epoch 360, Loss: 0.7871733903884888\n",
      "Epoch 380, Loss: 0.7915023565292358\n",
      "Epoch 400, Loss: 0.7967714071273804\n",
      "Epoch 420, Loss: 0.8029555082321167\n",
      "Epoch 440, Loss: 0.810025691986084\n",
      "Epoch 460, Loss: 0.8179490566253662\n",
      "Epoch 480, Loss: 0.8266901969909668\n",
      "weights of  P042 :  [array([[ 1.0186384 ,  0.13246584],\n",
      "       [-0.43539676,  0.44764632],\n",
      "       [-0.0239664 ,  0.37340593],\n",
      "       [-0.4041626 ,  0.09317222],\n",
      "       [ 0.8361218 , -0.5624328 ]], dtype=float32), array([-0.21089754,  0.21089756], dtype=float32)]\n",
      "Lowest loss of  P042 :  0.77952963\n",
      "Epoch 0, Loss: 2.925316333770752\n",
      "Epoch 20, Loss: 0.9823048114776611\n",
      "Epoch 40, Loss: 0.7838313579559326\n",
      "Epoch 60, Loss: 0.7771838903427124\n",
      "Epoch 80, Loss: 0.7768317461013794\n",
      "Epoch 100, Loss: 0.7786750793457031\n",
      "Epoch 120, Loss: 0.7808644771575928\n",
      "Epoch 140, Loss: 0.7834961414337158\n",
      "Epoch 160, Loss: 0.7865519523620605\n",
      "Epoch 180, Loss: 0.7900534868240356\n",
      "Epoch 200, Loss: 0.7940270900726318\n",
      "Epoch 220, Loss: 0.7984983921051025\n",
      "Epoch 240, Loss: 0.8034919500350952\n",
      "Epoch 260, Loss: 0.8090306520462036\n",
      "Epoch 280, Loss: 0.8151358962059021\n",
      "Epoch 300, Loss: 0.8218272924423218\n",
      "Epoch 320, Loss: 0.8291225433349609\n",
      "Epoch 340, Loss: 0.8370378017425537\n",
      "Epoch 360, Loss: 0.8455867767333984\n",
      "Epoch 380, Loss: 0.8547821044921875\n",
      "Epoch 400, Loss: 0.8646336793899536\n",
      "Epoch 420, Loss: 0.8751505613327026\n",
      "Epoch 440, Loss: 0.8863393068313599\n",
      "Epoch 460, Loss: 0.8982053995132446\n",
      "Epoch 480, Loss: 0.910752534866333\n",
      "weights of  P043 :  [array([[ 0.6719932 , -0.45371082],\n",
      "       [-0.07777457,  0.52430224],\n",
      "       [-0.26795304,  0.3687787 ],\n",
      "       [-0.46163782, -0.6022784 ],\n",
      "       [-0.18794   , -0.1659682 ]], dtype=float32), array([-0.10967169,  0.10967169], dtype=float32)]\n",
      "Lowest loss of  P043 :  0.77334183\n",
      "Epoch 0, Loss: 1.8046832084655762\n",
      "Epoch 20, Loss: 0.9900442361831665\n",
      "Epoch 40, Loss: 0.9863303303718567\n",
      "Epoch 60, Loss: 0.9856234192848206\n",
      "Epoch 80, Loss: 0.986595630645752\n",
      "Epoch 100, Loss: 0.9885379076004028\n",
      "Epoch 120, Loss: 0.9908881783485413\n",
      "Epoch 140, Loss: 0.9935547709465027\n",
      "Epoch 160, Loss: 0.9965324997901917\n",
      "Epoch 180, Loss: 0.9998246431350708\n",
      "Epoch 200, Loss: 1.0034366846084595\n",
      "Epoch 220, Loss: 1.007374882698059\n",
      "Epoch 240, Loss: 1.0116431713104248\n",
      "Epoch 260, Loss: 1.016245722770691\n",
      "Epoch 280, Loss: 1.021185278892517\n",
      "Epoch 300, Loss: 1.026463508605957\n",
      "Epoch 320, Loss: 1.0320813655853271\n",
      "Epoch 340, Loss: 1.0380384922027588\n",
      "Epoch 360, Loss: 1.0443334579467773\n",
      "Epoch 380, Loss: 1.0509634017944336\n",
      "Epoch 400, Loss: 1.0579252243041992\n",
      "Epoch 420, Loss: 1.0652140378952026\n",
      "Epoch 440, Loss: 1.072824239730835\n",
      "Epoch 460, Loss: 1.08074951171875\n",
      "Epoch 480, Loss: 1.0889822244644165\n",
      "weights of  P044 :  [array([[ 0.4300652 , -0.8279204 ],\n",
      "       [-0.8104315 ,  0.30715826],\n",
      "       [ 0.6885015 ,  0.36394608],\n",
      "       [-0.5773006 , -0.8258675 ],\n",
      "       [-0.5395726 , -0.07849202]], dtype=float32), array([-0.06479133,  0.06479132], dtype=float32)]\n",
      "Lowest loss of  P044 :  0.9816898\n",
      "Epoch 0, Loss: 1.5204671621322632\n",
      "Epoch 20, Loss: 0.9744588136672974\n",
      "Epoch 40, Loss: 0.9429008960723877\n",
      "Epoch 60, Loss: 0.9372247457504272\n",
      "Epoch 80, Loss: 0.9315820932388306\n",
      "Epoch 100, Loss: 0.9254453182220459\n",
      "Epoch 120, Loss: 0.9189676642417908\n",
      "Epoch 140, Loss: 0.9122101664543152\n",
      "Epoch 160, Loss: 0.9052943587303162\n",
      "Epoch 180, Loss: 0.8983169794082642\n",
      "Epoch 200, Loss: 0.8913623094558716\n",
      "Epoch 220, Loss: 0.8845099806785583\n",
      "Epoch 240, Loss: 0.8778315186500549\n",
      "Epoch 260, Loss: 0.871391773223877\n",
      "Epoch 280, Loss: 0.8652498722076416\n",
      "Epoch 300, Loss: 0.8594586849212646\n",
      "Epoch 320, Loss: 0.8540661931037903\n",
      "Epoch 340, Loss: 0.8491146564483643\n",
      "Epoch 360, Loss: 0.8446418046951294\n",
      "Epoch 380, Loss: 0.8406801819801331\n",
      "Epoch 400, Loss: 0.8372582197189331\n",
      "Epoch 420, Loss: 0.8343997597694397\n",
      "Epoch 440, Loss: 0.8321245908737183\n",
      "Epoch 460, Loss: 0.8304485082626343\n",
      "Epoch 480, Loss: 0.8293840289115906\n",
      "weights of  P045 :  [array([[ 0.3608982 ,  0.12881105],\n",
      "       [-0.75080746, -0.41309574],\n",
      "       [-0.18493481, -0.38748622],\n",
      "       [-0.391653  , -0.49020296],\n",
      "       [-0.14707518, -0.15702325]], dtype=float32), array([ 0.36482763, -0.36482766], dtype=float32)]\n",
      "Lowest loss of  P045 :  0.8289471\n",
      "Epoch 0, Loss: 4.228520393371582\n",
      "Epoch 20, Loss: 1.185557246208191\n",
      "Epoch 40, Loss: 1.04352867603302\n",
      "Epoch 60, Loss: 0.9983123540878296\n",
      "Epoch 80, Loss: 0.9935610890388489\n",
      "Epoch 100, Loss: 0.9926592111587524\n",
      "Epoch 120, Loss: 0.991687536239624\n",
      "Epoch 140, Loss: 0.9907103776931763\n",
      "Epoch 160, Loss: 0.989703357219696\n",
      "Epoch 180, Loss: 0.9886566400527954\n",
      "Epoch 200, Loss: 0.9875770807266235\n",
      "Epoch 220, Loss: 0.9864720106124878\n",
      "Epoch 240, Loss: 0.9853491187095642\n",
      "Epoch 260, Loss: 0.9842153191566467\n",
      "Epoch 280, Loss: 0.9830774068832397\n",
      "Epoch 300, Loss: 0.9819416999816895\n",
      "Epoch 320, Loss: 0.9808145761489868\n",
      "Epoch 340, Loss: 0.9797022342681885\n",
      "Epoch 360, Loss: 0.978610634803772\n",
      "Epoch 380, Loss: 0.9775452613830566\n",
      "Epoch 400, Loss: 0.9765118360519409\n",
      "Epoch 420, Loss: 0.9755160808563232\n",
      "Epoch 440, Loss: 0.9745632410049438\n",
      "Epoch 460, Loss: 0.9736587405204773\n",
      "Epoch 480, Loss: 0.9728074669837952\n",
      "weights of  P046 :  [array([[ 0.6114881 ,  0.18441564],\n",
      "       [ 0.5383835 ,  0.3781492 ],\n",
      "       [ 0.5641643 , -0.43228996],\n",
      "       [ 0.37171608,  1.0291734 ],\n",
      "       [-0.08998366,  0.506453  ]], dtype=float32), array([ 0.1659743 , -0.16597429], dtype=float32)]\n",
      "Lowest loss of  P046 :  0.9720527\n",
      "Epoch 0, Loss: 3.967985153198242\n",
      "Epoch 20, Loss: 1.226723313331604\n",
      "Epoch 40, Loss: 1.0351375341415405\n",
      "Epoch 60, Loss: 0.9894238710403442\n",
      "Epoch 80, Loss: 0.988227128982544\n",
      "Epoch 100, Loss: 0.987430214881897\n",
      "Epoch 120, Loss: 0.9870705008506775\n",
      "Epoch 140, Loss: 0.9866740703582764\n",
      "Epoch 160, Loss: 0.9862858057022095\n",
      "Epoch 180, Loss: 0.9858986735343933\n",
      "Epoch 200, Loss: 0.9855149984359741\n",
      "Epoch 220, Loss: 0.9851393699645996\n",
      "Epoch 240, Loss: 0.9847759008407593\n",
      "Epoch 260, Loss: 0.9844290614128113\n",
      "Epoch 280, Loss: 0.9841028451919556\n",
      "Epoch 300, Loss: 0.9838008880615234\n",
      "Epoch 320, Loss: 0.9835267663002014\n",
      "Epoch 340, Loss: 0.9832836985588074\n",
      "Epoch 360, Loss: 0.9830743074417114\n",
      "Epoch 380, Loss: 0.9829018115997314\n",
      "Epoch 400, Loss: 0.9827684164047241\n",
      "Epoch 420, Loss: 0.9826767444610596\n",
      "Epoch 440, Loss: 0.9826287031173706\n",
      "Epoch 460, Loss: 0.98262619972229\n",
      "Epoch 480, Loss: 0.982670783996582\n",
      "weights of  P047 :  [array([[-0.34995314, -0.01680535],\n",
      "       [ 0.20173405,  0.09273025],\n",
      "       [-0.82957965, -0.6591015 ],\n",
      "       [ 0.2969971 ,  0.82719034],\n",
      "       [ 0.78030753, -0.6056348 ]], dtype=float32), array([ 0.33214876, -0.33214873], dtype=float32)]\n",
      "Lowest loss of  P047 :  0.98262155\n",
      "Epoch 0, Loss: 1.0148234367370605\n",
      "Epoch 20, Loss: 1.0041390657424927\n",
      "Epoch 40, Loss: 0.996429443359375\n",
      "Epoch 60, Loss: 0.9935390949249268\n",
      "Epoch 80, Loss: 0.9972794651985168\n",
      "Epoch 100, Loss: 1.0082752704620361\n",
      "Epoch 120, Loss: 1.0263367891311646\n",
      "Epoch 140, Loss: 1.0508553981781006\n",
      "Epoch 160, Loss: 1.0810332298278809\n",
      "Epoch 180, Loss: 1.1160402297973633\n",
      "Epoch 200, Loss: 1.1550908088684082\n",
      "Epoch 220, Loss: 1.1974754333496094\n",
      "Epoch 240, Loss: 1.2425692081451416\n",
      "Epoch 260, Loss: 1.2898294925689697\n",
      "Epoch 280, Loss: 1.3387863636016846\n",
      "Epoch 300, Loss: 1.3890327215194702\n",
      "Epoch 320, Loss: 1.4402151107788086\n",
      "Epoch 340, Loss: 1.4920258522033691\n",
      "Epoch 360, Loss: 1.5441968441009521\n",
      "Epoch 380, Loss: 1.5964937210083008\n",
      "Epoch 400, Loss: 1.6487107276916504\n",
      "Epoch 420, Loss: 1.700667381286621\n",
      "Epoch 440, Loss: 1.7522070407867432\n",
      "Epoch 460, Loss: 1.803192377090454\n",
      "Epoch 480, Loss: 1.8535051345825195\n",
      "weights of  P048 :  [array([[ 0.97220343,  0.5542682 ],\n",
      "       [ 0.34649572,  0.9017161 ],\n",
      "       [ 0.15988344, -0.5712648 ],\n",
      "       [-0.49031895, -0.33448282],\n",
      "       [ 0.5146486 ,  0.6779614 ]], dtype=float32), array([ 0.12103602, -0.12103601], dtype=float32)]\n",
      "Lowest loss of  P048 :  0.9935391\n",
      "Epoch 0, Loss: 1.6016021966934204\n",
      "Epoch 20, Loss: 1.0491327047348022\n",
      "Epoch 40, Loss: 1.018818736076355\n",
      "Epoch 60, Loss: 1.0155322551727295\n",
      "Epoch 80, Loss: 1.0129470825195312\n",
      "Epoch 100, Loss: 1.0105394124984741\n",
      "Epoch 120, Loss: 1.008521318435669\n",
      "Epoch 140, Loss: 1.006996512413025\n",
      "Epoch 160, Loss: 1.0061079263687134\n",
      "Epoch 180, Loss: 1.0059837102890015\n",
      "Epoch 200, Loss: 1.0067332983016968\n",
      "Epoch 220, Loss: 1.0084542036056519\n",
      "Epoch 240, Loss: 1.0112289190292358\n",
      "Epoch 260, Loss: 1.0151268243789673\n",
      "Epoch 280, Loss: 1.020205020904541\n",
      "Epoch 300, Loss: 1.0265085697174072\n",
      "Epoch 320, Loss: 1.0340707302093506\n",
      "Epoch 340, Loss: 1.0429152250289917\n",
      "Epoch 360, Loss: 1.053056001663208\n",
      "Epoch 380, Loss: 1.0644980669021606\n",
      "Epoch 400, Loss: 1.0772385597229004\n",
      "Epoch 420, Loss: 1.0912675857543945\n",
      "Epoch 440, Loss: 1.1065692901611328\n",
      "Epoch 460, Loss: 1.1231220960617065\n",
      "Epoch 480, Loss: 1.140899896621704\n",
      "weights of  P049 :  [array([[-0.3162893 , -0.45289254],\n",
      "       [-0.26935497,  0.3662403 ],\n",
      "       [-0.06377827,  0.7106918 ],\n",
      "       [-0.84957856, -0.9633902 ],\n",
      "       [ 0.48787856, -0.8704847 ]], dtype=float32), array([ 0.00861692, -0.00861692], dtype=float32)]\n",
      "Lowest loss of  P049 :  1.005933\n",
      "Epoch 0, Loss: 2.214773654937744\n",
      "Epoch 20, Loss: 1.0381884574890137\n",
      "Epoch 40, Loss: 1.0343818664550781\n",
      "Epoch 60, Loss: 1.0239026546478271\n",
      "Epoch 80, Loss: 1.0242120027542114\n",
      "Epoch 100, Loss: 1.0245705842971802\n",
      "Epoch 120, Loss: 1.0249978303909302\n",
      "Epoch 140, Loss: 1.0254839658737183\n",
      "Epoch 160, Loss: 1.0260242223739624\n",
      "Epoch 180, Loss: 1.026619791984558\n",
      "Epoch 200, Loss: 1.0272722244262695\n",
      "Epoch 220, Loss: 1.0279836654663086\n",
      "Epoch 240, Loss: 1.028755784034729\n",
      "Epoch 260, Loss: 1.0295909643173218\n",
      "Epoch 280, Loss: 1.0304912328720093\n",
      "Epoch 300, Loss: 1.031458854675293\n",
      "Epoch 320, Loss: 1.0324962139129639\n",
      "Epoch 340, Loss: 1.0336058139801025\n",
      "Epoch 360, Loss: 1.0347900390625\n",
      "Epoch 380, Loss: 1.0360511541366577\n",
      "Epoch 400, Loss: 1.0373916625976562\n",
      "Epoch 420, Loss: 1.0388139486312866\n",
      "Epoch 440, Loss: 1.0403201580047607\n",
      "Epoch 460, Loss: 1.0419129133224487\n",
      "Epoch 480, Loss: 1.0435943603515625\n",
      "weights of  P050 :  [array([[-0.3846331 ,  0.08408073],\n",
      "       [-0.86660117,  0.4475392 ],\n",
      "       [ 0.34347093,  0.89856577],\n",
      "       [ 0.6983366 , -0.31607094],\n",
      "       [ 0.8010849 , -0.48114458]], dtype=float32), array([ 0.07044084, -0.07044084], dtype=float32)]\n",
      "Lowest loss of  P050 :  1.0234492\n",
      "Epoch 0, Loss: 6.680047988891602\n",
      "Epoch 20, Loss: 1.381956934928894\n",
      "Epoch 40, Loss: 1.0015558004379272\n",
      "Epoch 60, Loss: 0.9071344137191772\n",
      "Epoch 80, Loss: 0.9040654897689819\n",
      "Epoch 100, Loss: 0.9038482308387756\n",
      "Epoch 120, Loss: 0.9030312895774841\n",
      "Epoch 140, Loss: 0.9026451110839844\n",
      "Epoch 160, Loss: 0.9022401571273804\n",
      "Epoch 180, Loss: 0.901810884475708\n",
      "Epoch 200, Loss: 0.9013627171516418\n",
      "Epoch 220, Loss: 0.900900661945343\n",
      "Epoch 240, Loss: 0.9004263281822205\n",
      "Epoch 260, Loss: 0.8999422192573547\n",
      "Epoch 280, Loss: 0.8994502425193787\n",
      "Epoch 300, Loss: 0.8989526033401489\n",
      "Epoch 320, Loss: 0.8984515070915222\n",
      "Epoch 340, Loss: 0.8979488611221313\n",
      "Epoch 360, Loss: 0.8974466323852539\n",
      "Epoch 380, Loss: 0.8969471454620361\n",
      "Epoch 400, Loss: 0.896452009677887\n",
      "Epoch 420, Loss: 0.8959633111953735\n",
      "Epoch 440, Loss: 0.8954829573631287\n",
      "Epoch 460, Loss: 0.8950127363204956\n",
      "Epoch 480, Loss: 0.894554853439331\n",
      "weights of  P051 :  [array([[ 0.65260124,  0.3365259 ],\n",
      "       [-0.22730206,  0.06322719],\n",
      "       [ 0.09863321,  0.8873052 ],\n",
      "       [ 0.34761775, -0.5579066 ],\n",
      "       [-0.01697936,  0.47629052]], dtype=float32), array([ 0.19532005, -0.1953201 ], dtype=float32)]\n",
      "Lowest loss of  P051 :  0.8941327\n",
      "Epoch 0, Loss: 2.5220987796783447\n",
      "Epoch 20, Loss: 1.198845624923706\n",
      "Epoch 40, Loss: 1.1484094858169556\n",
      "Epoch 60, Loss: 1.1252292394638062\n",
      "Epoch 80, Loss: 1.1057631969451904\n",
      "Epoch 100, Loss: 1.084873080253601\n",
      "Epoch 120, Loss: 1.0633573532104492\n",
      "Epoch 140, Loss: 1.041599154472351\n",
      "Epoch 160, Loss: 1.020018219947815\n",
      "Epoch 180, Loss: 0.9989851713180542\n",
      "Epoch 200, Loss: 0.9788131713867188\n",
      "Epoch 220, Loss: 0.9597620964050293\n",
      "Epoch 240, Loss: 0.94204181432724\n",
      "Epoch 260, Loss: 0.9258162975311279\n",
      "Epoch 280, Loss: 0.911206841468811\n",
      "Epoch 300, Loss: 0.898296594619751\n",
      "Epoch 320, Loss: 0.8871341347694397\n",
      "Epoch 340, Loss: 0.8777373433113098\n",
      "Epoch 360, Loss: 0.8700984120368958\n",
      "Epoch 380, Loss: 0.8641873002052307\n",
      "Epoch 400, Loss: 0.8599558472633362\n",
      "Epoch 420, Loss: 0.8573421835899353\n",
      "Epoch 440, Loss: 0.85627281665802\n",
      "Epoch 460, Loss: 0.8566673398017883\n",
      "Epoch 480, Loss: 0.858439564704895\n",
      "weights of  P052 :  [array([[ 0.8226761 , -0.029684  ],\n",
      "       [ 0.03464595,  0.08975482],\n",
      "       [ 0.23104185, -0.42898235],\n",
      "       [ 0.26071638,  0.55439365],\n",
      "       [-0.03662417,  0.99719095]], dtype=float32), array([ 0.13551863, -0.13551864], dtype=float32)]\n",
      "Lowest loss of  P052 :  0.8562374\n",
      "Epoch 0, Loss: 1.4007562398910522\n",
      "Epoch 20, Loss: 1.0132429599761963\n",
      "Epoch 40, Loss: 0.98671555519104\n",
      "Epoch 60, Loss: 0.9783583283424377\n",
      "Epoch 80, Loss: 0.9723031520843506\n",
      "Epoch 100, Loss: 0.9659513235092163\n",
      "Epoch 120, Loss: 0.9592175483703613\n",
      "Epoch 140, Loss: 0.9522517323493958\n",
      "Epoch 160, Loss: 0.9451673030853271\n",
      "Epoch 180, Loss: 0.9380741119384766\n",
      "Epoch 200, Loss: 0.9310703277587891\n",
      "Epoch 220, Loss: 0.9242464303970337\n",
      "Epoch 240, Loss: 0.9176843762397766\n",
      "Epoch 260, Loss: 0.9114594459533691\n",
      "Epoch 280, Loss: 0.9056398868560791\n",
      "Epoch 300, Loss: 0.9002873301506042\n",
      "Epoch 320, Loss: 0.8954573273658752\n",
      "Epoch 340, Loss: 0.8911996483802795\n",
      "Epoch 360, Loss: 0.8875576853752136\n",
      "Epoch 380, Loss: 0.8845698237419128\n",
      "Epoch 400, Loss: 0.8822686672210693\n",
      "Epoch 420, Loss: 0.8806820511817932\n",
      "Epoch 440, Loss: 0.879832923412323\n",
      "Epoch 460, Loss: 0.8797396421432495\n",
      "Epoch 480, Loss: 0.8804163336753845\n",
      "weights of  P053 :  [array([[-0.39185488, -0.69613016],\n",
      "       [-0.01441933, -0.1935382 ],\n",
      "       [-0.47777182, -0.29558656],\n",
      "       [ 0.37603754,  0.39896655],\n",
      "       [-0.74233955, -0.40394136]], dtype=float32), array([-0.03057243,  0.0305725 ], dtype=float32)]\n",
      "Lowest loss of  P053 :  0.8796853\n",
      "Epoch 0, Loss: 5.916840553283691\n",
      "Epoch 20, Loss: 1.219266414642334\n",
      "Epoch 40, Loss: 0.9512720704078674\n",
      "Epoch 60, Loss: 0.9349045753479004\n",
      "Epoch 80, Loss: 0.9325443506240845\n",
      "Epoch 100, Loss: 0.9273732900619507\n",
      "Epoch 120, Loss: 0.9241563677787781\n",
      "Epoch 140, Loss: 0.9207720756530762\n",
      "Epoch 160, Loss: 0.9173433780670166\n",
      "Epoch 180, Loss: 0.9138766527175903\n",
      "Epoch 200, Loss: 0.9104146957397461\n",
      "Epoch 220, Loss: 0.9069960713386536\n",
      "Epoch 240, Loss: 0.9036564230918884\n",
      "Epoch 260, Loss: 0.9004281163215637\n",
      "Epoch 280, Loss: 0.8973410129547119\n",
      "Epoch 300, Loss: 0.8944223523139954\n",
      "Epoch 320, Loss: 0.8916960954666138\n",
      "Epoch 340, Loss: 0.8891849517822266\n",
      "Epoch 360, Loss: 0.8869085311889648\n",
      "Epoch 380, Loss: 0.8848844766616821\n",
      "Epoch 400, Loss: 0.88312828540802\n",
      "Epoch 420, Loss: 0.8816534280776978\n",
      "Epoch 440, Loss: 0.8804717063903809\n",
      "Epoch 460, Loss: 0.8795925378799438\n",
      "Epoch 480, Loss: 0.879024088382721\n",
      "weights of  P054 :  [array([[ 0.45502242,  0.03105054],\n",
      "       [-0.62047076,  0.31503296],\n",
      "       [ 0.9169404 ,  0.45019853],\n",
      "       [-0.6292471 , -0.5194481 ],\n",
      "       [ 0.43835554,  0.30319887]], dtype=float32), array([-0.6942469,  0.6942469], dtype=float32)]\n",
      "Lowest loss of  P054 :  0.8787775\n",
      "Epoch 0, Loss: 1.3318824768066406\n",
      "Epoch 20, Loss: 1.0642207860946655\n",
      "Epoch 40, Loss: 1.0528513193130493\n",
      "Epoch 60, Loss: 1.0443155765533447\n",
      "Epoch 80, Loss: 1.0345450639724731\n",
      "Epoch 100, Loss: 1.0252997875213623\n",
      "Epoch 120, Loss: 1.0167194604873657\n",
      "Epoch 140, Loss: 1.0092484951019287\n",
      "Epoch 160, Loss: 1.0032622814178467\n",
      "Epoch 180, Loss: 0.999071478843689\n",
      "Epoch 200, Loss: 0.9969273805618286\n",
      "Epoch 220, Loss: 0.9970309734344482\n",
      "Epoch 240, Loss: 0.9995354413986206\n",
      "Epoch 260, Loss: 1.0045506954193115\n",
      "Epoch 280, Loss: 1.0121486186981201\n",
      "Epoch 300, Loss: 1.0223677158355713\n",
      "Epoch 320, Loss: 1.0352158546447754\n",
      "Epoch 340, Loss: 1.0506768226623535\n",
      "Epoch 360, Loss: 1.0687119960784912\n",
      "Epoch 380, Loss: 1.0892657041549683\n",
      "Epoch 400, Loss: 1.1122677326202393\n",
      "Epoch 420, Loss: 1.1376370191574097\n",
      "Epoch 440, Loss: 1.1652836799621582\n",
      "Epoch 460, Loss: 1.195111632347107\n",
      "Epoch 480, Loss: 1.2270212173461914\n",
      "weights of  P055 :  [array([[ 1.0571251 ,  0.41497073],\n",
      "       [-0.6189624 , -0.2961325 ],\n",
      "       [-0.6360076 ,  0.5004706 ],\n",
      "       [-0.06902169, -0.68428487],\n",
      "       [-0.62147075, -0.6106737 ]], dtype=float32), array([ 0.02744543, -0.02744544], dtype=float32)]\n",
      "Lowest loss of  P055 :  0.99668574\n",
      "Epoch 0, Loss: 2.3451032638549805\n",
      "Epoch 20, Loss: 0.9407491087913513\n",
      "Epoch 40, Loss: 0.8859352469444275\n",
      "Epoch 60, Loss: 0.8781541585922241\n",
      "Epoch 80, Loss: 0.8747400641441345\n",
      "Epoch 100, Loss: 0.8712005019187927\n",
      "Epoch 120, Loss: 0.8676644563674927\n",
      "Epoch 140, Loss: 0.8640309572219849\n",
      "Epoch 160, Loss: 0.8603414297103882\n",
      "Epoch 180, Loss: 0.8566522598266602\n",
      "Epoch 200, Loss: 0.8530170917510986\n",
      "Epoch 220, Loss: 0.8494862914085388\n",
      "Epoch 240, Loss: 0.8461066484451294\n",
      "Epoch 260, Loss: 0.8429218530654907\n",
      "Epoch 280, Loss: 0.839972972869873\n",
      "Epoch 300, Loss: 0.8372981548309326\n",
      "Epoch 320, Loss: 0.8349329829216003\n",
      "Epoch 340, Loss: 0.8329105377197266\n",
      "Epoch 360, Loss: 0.831261396408081\n",
      "Epoch 380, Loss: 0.8300139307975769\n",
      "Epoch 400, Loss: 0.829194188117981\n",
      "Epoch 420, Loss: 0.8288257122039795\n",
      "Epoch 440, Loss: 0.8289299011230469\n",
      "Epoch 460, Loss: 0.8295261859893799\n",
      "Epoch 480, Loss: 0.8306319713592529\n",
      "weights of  P056 :  [array([[-0.03951592,  0.00293599],\n",
      "       [ 0.03276643,  0.5078015 ],\n",
      "       [ 0.27221018, -0.8310117 ],\n",
      "       [ 0.26765424,  0.25525138],\n",
      "       [ 0.18895288,  0.69033   ]], dtype=float32), array([ 0.02810488, -0.02810489], dtype=float32)]\n",
      "Lowest loss of  P056 :  0.8288064\n",
      "Epoch 0, Loss: 1.0090223550796509\n",
      "Epoch 20, Loss: 0.934515118598938\n",
      "Epoch 40, Loss: 0.9219071269035339\n",
      "Epoch 60, Loss: 0.9072251319885254\n",
      "Epoch 80, Loss: 0.8911307454109192\n",
      "Epoch 100, Loss: 0.8747120499610901\n",
      "Epoch 120, Loss: 0.8586072325706482\n",
      "Epoch 140, Loss: 0.8433845043182373\n",
      "Epoch 160, Loss: 0.8294975161552429\n",
      "Epoch 180, Loss: 0.8173008561134338\n",
      "Epoch 200, Loss: 0.8070610761642456\n",
      "Epoch 220, Loss: 0.7989660501480103\n",
      "Epoch 240, Loss: 0.7931351065635681\n",
      "Epoch 260, Loss: 0.7896271347999573\n",
      "Epoch 280, Loss: 0.7884498834609985\n",
      "Epoch 300, Loss: 0.7895689010620117\n",
      "Epoch 320, Loss: 0.7929154634475708\n",
      "Epoch 340, Loss: 0.7983940243721008\n",
      "Epoch 360, Loss: 0.8058896064758301\n",
      "Epoch 380, Loss: 0.8152734041213989\n",
      "Epoch 400, Loss: 0.8264085650444031\n",
      "Epoch 420, Loss: 0.8391538262367249\n",
      "Epoch 440, Loss: 0.8533675670623779\n",
      "Epoch 460, Loss: 0.8689103722572327\n",
      "Epoch 480, Loss: 0.8856469392776489\n",
      "weights of  P057 :  [array([[-0.13207787,  0.13930134],\n",
      "       [ 0.00257293,  0.7129374 ],\n",
      "       [ 0.18862405, -0.01951175],\n",
      "       [-0.24628122, -0.47010005],\n",
      "       [ 0.02165023, -0.5404838 ]], dtype=float32), array([-0.11888815,  0.1188881 ], dtype=float32)]\n",
      "Lowest loss of  P057 :  0.7884499\n",
      "Epoch 0, Loss: 4.971149921417236\n",
      "Epoch 20, Loss: 0.9189759492874146\n",
      "Epoch 40, Loss: 0.8852636814117432\n",
      "Epoch 60, Loss: 0.8793935179710388\n",
      "Epoch 80, Loss: 0.8701598644256592\n",
      "Epoch 100, Loss: 0.8690413236618042\n",
      "Epoch 120, Loss: 0.8684754967689514\n",
      "Epoch 140, Loss: 0.8679488897323608\n",
      "Epoch 160, Loss: 0.8674147725105286\n",
      "Epoch 180, Loss: 0.8668752908706665\n",
      "Epoch 200, Loss: 0.8663354516029358\n",
      "Epoch 220, Loss: 0.8658019304275513\n",
      "Epoch 240, Loss: 0.8652799725532532\n",
      "Epoch 260, Loss: 0.8647748231887817\n",
      "Epoch 280, Loss: 0.8642914295196533\n",
      "Epoch 300, Loss: 0.8638337850570679\n",
      "Epoch 320, Loss: 0.8634060025215149\n",
      "Epoch 340, Loss: 0.8630117774009705\n",
      "Epoch 360, Loss: 0.8626543283462524\n",
      "Epoch 380, Loss: 0.8623368144035339\n",
      "Epoch 400, Loss: 0.862061619758606\n",
      "Epoch 420, Loss: 0.8618314266204834\n",
      "Epoch 440, Loss: 0.8616480827331543\n",
      "Epoch 460, Loss: 0.8615135550498962\n",
      "Epoch 480, Loss: 0.8614293336868286\n",
      "weights of  P058 :  [array([[-0.71231294, -0.5708451 ],\n",
      "       [ 0.13558765,  0.39379358],\n",
      "       [ 0.17962115,  0.00864365],\n",
      "       [-0.30729246, -0.31394666],\n",
      "       [ 0.643662  ,  0.34185433]], dtype=float32), array([ 0.07120966, -0.0712097 ], dtype=float32)]\n",
      "Lowest loss of  P058 :  0.861397\n",
      "Epoch 0, Loss: 3.8399088382720947\n",
      "Epoch 20, Loss: 1.2516746520996094\n",
      "Epoch 40, Loss: 1.0870728492736816\n",
      "Epoch 60, Loss: 1.0388693809509277\n",
      "Epoch 80, Loss: 1.0350764989852905\n",
      "Epoch 100, Loss: 1.0302209854125977\n",
      "Epoch 120, Loss: 1.0256215333938599\n",
      "Epoch 140, Loss: 1.0207278728485107\n",
      "Epoch 160, Loss: 1.0156506299972534\n",
      "Epoch 180, Loss: 1.0104424953460693\n",
      "Epoch 200, Loss: 1.005150318145752\n",
      "Epoch 220, Loss: 0.9998180866241455\n",
      "Epoch 240, Loss: 0.9944860935211182\n",
      "Epoch 260, Loss: 0.9891914129257202\n",
      "Epoch 280, Loss: 0.983966588973999\n",
      "Epoch 300, Loss: 0.9788417220115662\n",
      "Epoch 320, Loss: 0.9738433361053467\n",
      "Epoch 340, Loss: 0.9689948558807373\n",
      "Epoch 360, Loss: 0.9643170833587646\n",
      "Epoch 380, Loss: 0.9598278999328613\n",
      "Epoch 400, Loss: 0.955542802810669\n",
      "Epoch 420, Loss: 0.9514746069908142\n",
      "Epoch 440, Loss: 0.9476338624954224\n",
      "Epoch 460, Loss: 0.944028913974762\n",
      "Epoch 480, Loss: 0.9406658411026001\n",
      "weights of  P059 :  [array([[-0.4355764 , -0.58024263],\n",
      "       [-0.6521044 , -0.24381834],\n",
      "       [-0.22938672,  0.24251215],\n",
      "       [ 1.0116357 ,  0.0949064 ],\n",
      "       [-0.388148  ,  0.38446856]], dtype=float32), array([-0.2681323,  0.2681323], dtype=float32)]\n",
      "Lowest loss of  P059 :  0.93769896\n",
      "Epoch 0, Loss: 4.0494890213012695\n",
      "Epoch 20, Loss: 1.6377058029174805\n",
      "Epoch 40, Loss: 1.4404863119125366\n",
      "Epoch 60, Loss: 1.387499451637268\n",
      "Epoch 80, Loss: 1.3683531284332275\n",
      "Epoch 100, Loss: 1.3473531007766724\n",
      "Epoch 120, Loss: 1.3255563974380493\n",
      "Epoch 140, Loss: 1.3028862476348877\n",
      "Epoch 160, Loss: 1.279726505279541\n",
      "Epoch 180, Loss: 1.256397008895874\n",
      "Epoch 200, Loss: 1.2331911325454712\n",
      "Epoch 220, Loss: 1.2103718519210815\n",
      "Epoch 240, Loss: 1.1881757974624634\n",
      "Epoch 260, Loss: 1.1668140888214111\n",
      "Epoch 280, Loss: 1.1464738845825195\n",
      "Epoch 300, Loss: 1.127319574356079\n",
      "Epoch 320, Loss: 1.109493374824524\n",
      "Epoch 340, Loss: 1.093116283416748\n",
      "Epoch 360, Loss: 1.0782884359359741\n",
      "Epoch 380, Loss: 1.0650912523269653\n",
      "Epoch 400, Loss: 1.0535874366760254\n",
      "Epoch 420, Loss: 1.0438227653503418\n",
      "Epoch 440, Loss: 1.0358272790908813\n",
      "Epoch 460, Loss: 1.0296168327331543\n",
      "Epoch 480, Loss: 1.0251942873001099\n",
      "weights of  P060 :  [array([[ 0.30588686, -1.3775084 ],\n",
      "       [ 0.3481885 , -0.31957415],\n",
      "       [-0.51960236,  0.69163966],\n",
      "       [-0.4640552 ,  0.6257466 ],\n",
      "       [-0.48359242, -0.7829951 ]], dtype=float32), array([-0.13997005,  0.13996997], dtype=float32)]\n",
      "Lowest loss of  P060 :  1.0226413\n",
      "Epoch 0, Loss: 1.1957063674926758\n",
      "Epoch 20, Loss: 1.01346755027771\n",
      "Epoch 40, Loss: 1.0082015991210938\n",
      "Epoch 60, Loss: 1.0013868808746338\n",
      "Epoch 80, Loss: 0.9955445528030396\n",
      "Epoch 100, Loss: 0.9899006485939026\n",
      "Epoch 120, Loss: 0.9847065210342407\n",
      "Epoch 140, Loss: 0.9802426099777222\n",
      "Epoch 160, Loss: 0.9767533540725708\n",
      "Epoch 180, Loss: 0.9744454622268677\n",
      "Epoch 200, Loss: 0.9734919667243958\n",
      "Epoch 220, Loss: 0.9740349054336548\n",
      "Epoch 240, Loss: 0.9761877059936523\n",
      "Epoch 260, Loss: 0.9800373315811157\n",
      "Epoch 280, Loss: 0.9856468439102173\n",
      "Epoch 300, Loss: 0.9930575489997864\n",
      "Epoch 320, Loss: 1.0022920370101929\n",
      "Epoch 340, Loss: 1.0133557319641113\n",
      "Epoch 360, Loss: 1.026239037513733\n",
      "Epoch 380, Loss: 1.0409201383590698\n",
      "Epoch 400, Loss: 1.0573663711547852\n",
      "Epoch 420, Loss: 1.0755358934402466\n",
      "Epoch 440, Loss: 1.0953798294067383\n",
      "Epoch 460, Loss: 1.1168437004089355\n",
      "Epoch 480, Loss: 1.1398680210113525\n",
      "weights of  P061 :  [array([[ 0.24577846,  0.07715218],\n",
      "       [ 0.20646602,  0.81849486],\n",
      "       [ 0.84393317, -0.86089265],\n",
      "       [-0.3381554 , -0.3188264 ],\n",
      "       [-0.5584565 ,  0.6557889 ]], dtype=float32), array([-0.13459566,  0.13459565], dtype=float32)]\n",
      "Lowest loss of  P061 :  0.973475\n",
      "Epoch 0, Loss: 1.3021410703659058\n",
      "Epoch 20, Loss: 1.0864053964614868\n",
      "Epoch 40, Loss: 1.0418970584869385\n",
      "Epoch 60, Loss: 0.9904811382293701\n",
      "Epoch 80, Loss: 0.9419534206390381\n",
      "Epoch 100, Loss: 0.8981537222862244\n",
      "Epoch 120, Loss: 0.8611800670623779\n",
      "Epoch 140, Loss: 0.832098126411438\n",
      "Epoch 160, Loss: 0.811241626739502\n",
      "Epoch 180, Loss: 0.7984508275985718\n",
      "Epoch 200, Loss: 0.7932643890380859\n",
      "Epoch 220, Loss: 0.7950557470321655\n",
      "Epoch 240, Loss: 0.8031321167945862\n",
      "Epoch 260, Loss: 0.8167961835861206\n",
      "Epoch 280, Loss: 0.8353848457336426\n",
      "Epoch 300, Loss: 0.8582891821861267\n",
      "Epoch 320, Loss: 0.8849622011184692\n",
      "Epoch 340, Loss: 0.9149194955825806\n",
      "Epoch 360, Loss: 0.9477375149726868\n",
      "Epoch 380, Loss: 0.9830471873283386\n",
      "Epoch 400, Loss: 1.020528793334961\n",
      "Epoch 420, Loss: 1.059905767440796\n",
      "Epoch 440, Loss: 1.1009387969970703\n",
      "Epoch 460, Loss: 1.143420934677124\n",
      "Epoch 480, Loss: 1.1871732473373413\n",
      "weights of  P062 :  [array([[ 0.39037588, -0.09452615],\n",
      "       [-1.2541168 ,  0.17767052],\n",
      "       [ 0.4881673 , -0.31484574],\n",
      "       [-0.05439417, -0.81743085],\n",
      "       [-0.49039966,  0.14352821]], dtype=float32), array([ 0.0967053 , -0.09670528], dtype=float32)]\n",
      "Lowest loss of  P062 :  0.7930846\n",
      "Epoch 0, Loss: 1.239255666732788\n",
      "Epoch 20, Loss: 1.01703941822052\n",
      "Epoch 40, Loss: 1.0122706890106201\n",
      "Epoch 60, Loss: 1.0049980878829956\n",
      "Epoch 80, Loss: 0.9971600770950317\n",
      "Epoch 100, Loss: 0.9888971447944641\n",
      "Epoch 120, Loss: 0.9800984859466553\n",
      "Epoch 140, Loss: 0.9710075259208679\n",
      "Epoch 160, Loss: 0.9617291688919067\n",
      "Epoch 180, Loss: 0.9523879289627075\n",
      "Epoch 200, Loss: 0.9430839419364929\n",
      "Epoch 220, Loss: 0.9339019060134888\n",
      "Epoch 240, Loss: 0.924915075302124\n",
      "Epoch 260, Loss: 0.9161856174468994\n",
      "Epoch 280, Loss: 0.9077666401863098\n",
      "Epoch 300, Loss: 0.8997031450271606\n",
      "Epoch 320, Loss: 0.8920327425003052\n",
      "Epoch 340, Loss: 0.8847870826721191\n",
      "Epoch 360, Loss: 0.8779914975166321\n",
      "Epoch 380, Loss: 0.8716669678688049\n",
      "Epoch 400, Loss: 0.865829348564148\n",
      "Epoch 420, Loss: 0.8604906797409058\n",
      "Epoch 440, Loss: 0.8556591272354126\n",
      "Epoch 460, Loss: 0.8513391017913818\n",
      "Epoch 480, Loss: 0.847532331943512\n",
      "weights of  P063 :  [array([[-0.3213198 , -0.09246514],\n",
      "       [-0.5575823 , -0.28544328],\n",
      "       [ 0.18515639, -0.1219881 ],\n",
      "       [ 0.09147005,  0.5769478 ],\n",
      "       [ 0.08839431, -0.8582013 ]], dtype=float32), array([-0.25648797,  0.25648794], dtype=float32)]\n",
      "Lowest loss of  P063 :  0.84439015\n",
      "Epoch 0, Loss: 0.9244577884674072\n",
      "Epoch 20, Loss: 0.9217265844345093\n",
      "Epoch 40, Loss: 0.9160314798355103\n",
      "Epoch 60, Loss: 0.9114891290664673\n",
      "Epoch 80, Loss: 0.9080530405044556\n",
      "Epoch 100, Loss: 0.9062302112579346\n",
      "Epoch 120, Loss: 0.9066077470779419\n",
      "Epoch 140, Loss: 0.9096584320068359\n",
      "Epoch 160, Loss: 0.915754497051239\n",
      "Epoch 180, Loss: 0.925179123878479\n",
      "Epoch 200, Loss: 0.938135027885437\n",
      "Epoch 220, Loss: 0.9547526836395264\n",
      "Epoch 240, Loss: 0.9750992059707642\n",
      "Epoch 260, Loss: 0.9991858005523682\n",
      "Epoch 280, Loss: 1.0269770622253418\n",
      "Epoch 300, Loss: 1.0583977699279785\n",
      "Epoch 320, Loss: 1.0933396816253662\n",
      "Epoch 340, Loss: 1.1316685676574707\n",
      "Epoch 360, Loss: 1.1732298135757446\n",
      "Epoch 380, Loss: 1.2178540229797363\n",
      "Epoch 400, Loss: 1.2653605937957764\n",
      "Epoch 420, Loss: 1.3155627250671387\n",
      "Epoch 440, Loss: 1.368269920349121\n",
      "Epoch 460, Loss: 1.4232923984527588\n",
      "Epoch 480, Loss: 1.480440378189087\n",
      "weights of  P064 :  [array([[ 0.9447296 ,  0.5949001 ],\n",
      "       [-0.26779723, -0.3214614 ],\n",
      "       [ 0.02778416,  0.25812283],\n",
      "       [ 0.46586064,  0.678857  ],\n",
      "       [ 0.27411336,  0.10361703]], dtype=float32), array([ 0.07107393, -0.07107393], dtype=float32)]\n",
      "Lowest loss of  P064 :  0.90608555\n",
      "Epoch 0, Loss: 2.143580913543701\n",
      "Epoch 20, Loss: 0.94733065366745\n",
      "Epoch 40, Loss: 0.9403195381164551\n",
      "Epoch 60, Loss: 0.9285466074943542\n",
      "Epoch 80, Loss: 0.9270743727684021\n",
      "Epoch 100, Loss: 0.9253536462783813\n",
      "Epoch 120, Loss: 0.9235081076622009\n",
      "Epoch 140, Loss: 0.9215486645698547\n",
      "Epoch 160, Loss: 0.919488251209259\n",
      "Epoch 180, Loss: 0.9173442721366882\n",
      "Epoch 200, Loss: 0.9151337742805481\n",
      "Epoch 220, Loss: 0.9128726720809937\n",
      "Epoch 240, Loss: 0.9105764031410217\n",
      "Epoch 260, Loss: 0.9082591533660889\n",
      "Epoch 280, Loss: 0.9059348106384277\n",
      "Epoch 300, Loss: 0.9036167860031128\n",
      "Epoch 320, Loss: 0.9013177156448364\n",
      "Epoch 340, Loss: 0.8990503549575806\n",
      "Epoch 360, Loss: 0.8968266248703003\n",
      "Epoch 380, Loss: 0.8946584463119507\n",
      "Epoch 400, Loss: 0.8925573825836182\n",
      "Epoch 420, Loss: 0.8905345797538757\n",
      "Epoch 440, Loss: 0.8886012434959412\n",
      "Epoch 460, Loss: 0.8867679834365845\n",
      "Epoch 480, Loss: 0.8850453495979309\n",
      "weights of  P065 :  [array([[-0.03938419, -0.7486957 ],\n",
      "       [-0.5872026 ,  0.33469462],\n",
      "       [-0.06341793,  0.45460665],\n",
      "       [ 0.43387383,  0.3478423 ],\n",
      "       [ 0.29885986, -0.5710725 ]], dtype=float32), array([-0.01282775,  0.01282776], dtype=float32)]\n",
      "Lowest loss of  P065 :  0.88352066\n",
      "Epoch 0, Loss: 3.2037768363952637\n",
      "Epoch 20, Loss: 1.2218185663223267\n",
      "Epoch 40, Loss: 1.0219581127166748\n",
      "Epoch 60, Loss: 1.0125371217727661\n",
      "Epoch 80, Loss: 1.006720781326294\n",
      "Epoch 100, Loss: 1.0028108358383179\n",
      "Epoch 120, Loss: 0.9985429048538208\n",
      "Epoch 140, Loss: 0.9940400719642639\n",
      "Epoch 160, Loss: 0.9893190264701843\n",
      "Epoch 180, Loss: 0.9844229221343994\n",
      "Epoch 200, Loss: 0.9793922901153564\n",
      "Epoch 220, Loss: 0.9742639064788818\n",
      "Epoch 240, Loss: 0.9690715074539185\n",
      "Epoch 260, Loss: 0.9638461470603943\n",
      "Epoch 280, Loss: 0.958616316318512\n",
      "Epoch 300, Loss: 0.9534085392951965\n",
      "Epoch 320, Loss: 0.9482475519180298\n",
      "Epoch 340, Loss: 0.9431559443473816\n",
      "Epoch 360, Loss: 0.9381546974182129\n",
      "Epoch 380, Loss: 0.9332633018493652\n",
      "Epoch 400, Loss: 0.9284995198249817\n",
      "Epoch 420, Loss: 0.9238795638084412\n",
      "Epoch 440, Loss: 0.9194182753562927\n",
      "Epoch 460, Loss: 0.9151290655136108\n",
      "Epoch 480, Loss: 0.9110239744186401\n",
      "weights of  P066 :  [array([[-0.82935673,  0.37429202],\n",
      "       [ 0.02326479, -0.64920545],\n",
      "       [ 0.44651368,  0.47063538],\n",
      "       [ 0.55897915,  0.37895653],\n",
      "       [ 0.11669976,  0.04163469]], dtype=float32), array([-0.09294618,  0.09294619], dtype=float32)]\n",
      "Lowest loss of  P066 :  0.9073047\n",
      "Epoch 0, Loss: 4.840394020080566\n",
      "Epoch 20, Loss: 0.9465314149856567\n",
      "Epoch 40, Loss: 0.9490761756896973\n",
      "Epoch 60, Loss: 0.9327539205551147\n",
      "Epoch 80, Loss: 0.9233314394950867\n",
      "Epoch 100, Loss: 0.9229023456573486\n",
      "Epoch 120, Loss: 0.9223740100860596\n",
      "Epoch 140, Loss: 0.9220618009567261\n",
      "Epoch 160, Loss: 0.9217545986175537\n",
      "Epoch 180, Loss: 0.9214468002319336\n",
      "Epoch 200, Loss: 0.9211438894271851\n",
      "Epoch 220, Loss: 0.920851469039917\n",
      "Epoch 240, Loss: 0.9205738306045532\n",
      "Epoch 260, Loss: 0.9203144311904907\n",
      "Epoch 280, Loss: 0.9200761318206787\n",
      "Epoch 300, Loss: 0.9198616743087769\n",
      "Epoch 320, Loss: 0.9196736812591553\n",
      "Epoch 340, Loss: 0.9195135235786438\n",
      "Epoch 360, Loss: 0.919383704662323\n",
      "Epoch 380, Loss: 0.9192850589752197\n",
      "Epoch 400, Loss: 0.9192191362380981\n",
      "Epoch 420, Loss: 0.9191868305206299\n",
      "Epoch 440, Loss: 0.919188916683197\n",
      "Epoch 460, Loss: 0.919225811958313\n",
      "Epoch 480, Loss: 0.9192980527877808\n",
      "weights of  P067 :  [array([[ 0.6661583 , -0.68813634],\n",
      "       [-0.4316027 ,  0.4150877 ],\n",
      "       [-0.6202315 , -0.656935  ],\n",
      "       [-0.32675892,  0.19436952],\n",
      "       [ 0.51333934, -0.07858405]], dtype=float32), array([ 0.20143466, -0.20143464], dtype=float32)]\n",
      "Lowest loss of  P067 :  0.9191835\n",
      "Epoch 0, Loss: 1.613648533821106\n",
      "Epoch 20, Loss: 0.9578633904457092\n",
      "Epoch 40, Loss: 0.9367923140525818\n",
      "Epoch 60, Loss: 0.9368165135383606\n",
      "Epoch 80, Loss: 0.9367451667785645\n",
      "Epoch 100, Loss: 0.9367549419403076\n",
      "Epoch 120, Loss: 0.9367794990539551\n",
      "Epoch 140, Loss: 0.9368342161178589\n",
      "Epoch 160, Loss: 0.9369148015975952\n",
      "Epoch 180, Loss: 0.9370307922363281\n",
      "Epoch 200, Loss: 0.9371879696846008\n",
      "Epoch 220, Loss: 0.9373904466629028\n",
      "Epoch 240, Loss: 0.9376438856124878\n",
      "Epoch 260, Loss: 0.9379533529281616\n",
      "Epoch 280, Loss: 0.9383240938186646\n",
      "Epoch 300, Loss: 0.9387613534927368\n",
      "Epoch 320, Loss: 0.9392702579498291\n",
      "Epoch 340, Loss: 0.9398559331893921\n",
      "Epoch 360, Loss: 0.9405235052108765\n",
      "Epoch 380, Loss: 0.9412785172462463\n",
      "Epoch 400, Loss: 0.9421254396438599\n",
      "Epoch 420, Loss: 0.9430698156356812\n",
      "Epoch 440, Loss: 0.9441166520118713\n",
      "Epoch 460, Loss: 0.9452709555625916\n",
      "Epoch 480, Loss: 0.9465375542640686\n",
      "weights of  P068 :  [array([[ 0.3347758 , -0.28307396],\n",
      "       [ 0.6204494 ,  0.5284748 ],\n",
      "       [ 0.13667923, -0.04602658],\n",
      "       [ 0.556215  ,  0.15321733],\n",
      "       [-0.72749054,  0.85740876]], dtype=float32), array([ 0.04893973, -0.04893973], dtype=float32)]\n",
      "Lowest loss of  P068 :  0.93674517\n",
      "Epoch 0, Loss: 1.3331140279769897\n",
      "Epoch 20, Loss: 1.0338257551193237\n",
      "Epoch 40, Loss: 1.019243836402893\n",
      "Epoch 60, Loss: 1.0133888721466064\n",
      "Epoch 80, Loss: 1.0069702863693237\n",
      "Epoch 100, Loss: 1.0000401735305786\n",
      "Epoch 120, Loss: 0.9926899671554565\n",
      "Epoch 140, Loss: 0.9850506782531738\n",
      "Epoch 160, Loss: 0.9772505760192871\n",
      "Epoch 180, Loss: 0.9694028496742249\n",
      "Epoch 200, Loss: 0.9616137146949768\n",
      "Epoch 220, Loss: 0.9539802074432373\n",
      "Epoch 240, Loss: 0.946591854095459\n",
      "Epoch 260, Loss: 0.9395323991775513\n",
      "Epoch 280, Loss: 0.9328785538673401\n",
      "Epoch 300, Loss: 0.9267019033432007\n",
      "Epoch 320, Loss: 0.9210683107376099\n",
      "Epoch 340, Loss: 0.9160381555557251\n",
      "Epoch 360, Loss: 0.9116666316986084\n",
      "Epoch 380, Loss: 0.908004105091095\n",
      "Epoch 400, Loss: 0.9050959348678589\n",
      "Epoch 420, Loss: 0.9029828906059265\n",
      "Epoch 440, Loss: 0.9017009735107422\n",
      "Epoch 460, Loss: 0.9012819528579712\n",
      "Epoch 480, Loss: 0.9017531871795654\n",
      "weights of  P069 :  [array([[-0.18907085, -0.39826643],\n",
      "       [ 0.61526704,  0.30678228],\n",
      "       [ 0.11146516,  0.4459844 ],\n",
      "       [ 0.63984615,  0.6092494 ],\n",
      "       [-0.6864101 , -0.27136818]], dtype=float32), array([-0.10900449,  0.10900442], dtype=float32)]\n",
      "Lowest loss of  P069 :  0.90128195\n",
      "Epoch 0, Loss: 1.1740546226501465\n",
      "Epoch 20, Loss: 1.1341725587844849\n",
      "Epoch 40, Loss: 1.1087218523025513\n",
      "Epoch 60, Loss: 1.0837805271148682\n",
      "Epoch 80, Loss: 1.0637447834014893\n",
      "Epoch 100, Loss: 1.0513358116149902\n",
      "Epoch 120, Loss: 1.0482276678085327\n",
      "Epoch 140, Loss: 1.0551942586898804\n",
      "Epoch 160, Loss: 1.072374939918518\n",
      "Epoch 180, Loss: 1.099461555480957\n",
      "Epoch 200, Loss: 1.1358587741851807\n",
      "Epoch 220, Loss: 1.18080735206604\n",
      "Epoch 240, Loss: 1.2334778308868408\n",
      "Epoch 260, Loss: 1.2930288314819336\n",
      "Epoch 280, Loss: 1.3586490154266357\n",
      "Epoch 300, Loss: 1.429578185081482\n",
      "Epoch 320, Loss: 1.5051186084747314\n",
      "Epoch 340, Loss: 1.5846388339996338\n",
      "Epoch 360, Loss: 1.667572021484375\n",
      "Epoch 380, Loss: 1.7534123659133911\n",
      "Epoch 400, Loss: 1.8417106866836548\n",
      "Epoch 420, Loss: 1.9320683479309082\n",
      "Epoch 440, Loss: 2.024132013320923\n",
      "Epoch 460, Loss: 2.117588758468628\n",
      "Epoch 480, Loss: 2.21216082572937\n",
      "weights of  P070 :  [array([[ 0.31502268,  0.13780317],\n",
      "       [-0.7914635 , -0.6124309 ],\n",
      "       [-0.9853809 , -0.17049347],\n",
      "       [ 1.0424064 ,  0.51443493],\n",
      "       [-0.51906765, -0.605928  ]], dtype=float32), array([ 0.0004715 , -0.00047156], dtype=float32)]\n",
      "Lowest loss of  P070 :  1.048057\n",
      "Epoch 0, Loss: 3.7766292095184326\n",
      "Epoch 20, Loss: 1.2823413610458374\n",
      "Epoch 40, Loss: 1.0780736207962036\n",
      "Epoch 60, Loss: 1.036316156387329\n",
      "Epoch 80, Loss: 1.0346193313598633\n",
      "Epoch 100, Loss: 1.0314874649047852\n",
      "Epoch 120, Loss: 1.0289760828018188\n",
      "Epoch 140, Loss: 1.0264967679977417\n",
      "Epoch 160, Loss: 1.024127721786499\n",
      "Epoch 180, Loss: 1.0219374895095825\n",
      "Epoch 200, Loss: 1.0199871063232422\n",
      "Epoch 220, Loss: 1.0183333158493042\n",
      "Epoch 240, Loss: 1.0170291662216187\n",
      "Epoch 260, Loss: 1.016123652458191\n",
      "Epoch 280, Loss: 1.0156621932983398\n",
      "Epoch 300, Loss: 1.0156866312026978\n",
      "Epoch 320, Loss: 1.0162353515625\n",
      "Epoch 340, Loss: 1.0173428058624268\n",
      "Epoch 360, Loss: 1.0190410614013672\n",
      "Epoch 380, Loss: 1.0213582515716553\n",
      "Epoch 400, Loss: 1.0243196487426758\n",
      "Epoch 420, Loss: 1.0279479026794434\n",
      "Epoch 440, Loss: 1.0322625637054443\n",
      "Epoch 460, Loss: 1.0372800827026367\n",
      "Epoch 480, Loss: 1.0430155992507935\n",
      "weights of  P071 :  [array([[-0.27249226, -0.78248286],\n",
      "       [ 0.38290927,  1.2365323 ],\n",
      "       [ 0.4267795 , -0.7162883 ],\n",
      "       [ 0.5426116 ,  0.513879  ],\n",
      "       [ 0.14941172,  0.69075966]], dtype=float32), array([ 0.2058637 , -0.20586368], dtype=float32)]\n",
      "Lowest loss of  P071 :  1.0156107\n",
      "Epoch 0, Loss: 1.410628318786621\n",
      "Epoch 20, Loss: 0.7820855379104614\n",
      "Epoch 40, Loss: 0.7456747889518738\n",
      "Epoch 60, Loss: 0.7311525940895081\n",
      "Epoch 80, Loss: 0.7172672748565674\n",
      "Epoch 100, Loss: 0.705264687538147\n",
      "Epoch 120, Loss: 0.6958680152893066\n",
      "Epoch 140, Loss: 0.6895117163658142\n",
      "Epoch 160, Loss: 0.6863661408424377\n",
      "Epoch 180, Loss: 0.6864267587661743\n",
      "Epoch 200, Loss: 0.689574122428894\n",
      "Epoch 220, Loss: 0.6956200003623962\n",
      "Epoch 240, Loss: 0.7043392658233643\n",
      "Epoch 260, Loss: 0.7154934406280518\n",
      "Epoch 280, Loss: 0.7288457155227661\n",
      "Epoch 300, Loss: 0.7441703081130981\n",
      "Epoch 320, Loss: 0.7612574100494385\n",
      "Epoch 340, Loss: 0.7799164056777954\n",
      "Epoch 360, Loss: 0.7999751567840576\n",
      "Epoch 380, Loss: 0.821280837059021\n",
      "Epoch 400, Loss: 0.8436974287033081\n",
      "Epoch 420, Loss: 0.8671053647994995\n",
      "Epoch 440, Loss: 0.8913991451263428\n",
      "Epoch 460, Loss: 0.916486382484436\n",
      "Epoch 480, Loss: 0.9422855377197266\n",
      "weights of  P072 :  [array([[ 0.1393906 , -0.8061399 ],\n",
      "       [ 0.09821004,  1.1792622 ],\n",
      "       [-0.22526006, -0.2026848 ],\n",
      "       [ 0.42993996, -0.00748292],\n",
      "       [-0.4523001 , -0.19671428]], dtype=float32), array([-0.07791068,  0.07791066], dtype=float32)]\n",
      "Lowest loss of  P072 :  0.68600124\n",
      "Epoch 0, Loss: 0.9592888355255127\n",
      "Epoch 20, Loss: 0.9313869476318359\n",
      "Epoch 40, Loss: 0.9238727688789368\n",
      "Epoch 60, Loss: 0.9154590964317322\n",
      "Epoch 80, Loss: 0.9067803621292114\n",
      "Epoch 100, Loss: 0.8981877565383911\n",
      "Epoch 120, Loss: 0.8902010321617126\n",
      "Epoch 140, Loss: 0.8831610679626465\n",
      "Epoch 160, Loss: 0.8773618340492249\n",
      "Epoch 180, Loss: 0.8730173707008362\n",
      "Epoch 200, Loss: 0.8702839016914368\n",
      "Epoch 220, Loss: 0.8692644834518433\n",
      "Epoch 240, Loss: 0.8700146079063416\n",
      "Epoch 260, Loss: 0.8725490570068359\n",
      "Epoch 280, Loss: 0.8768477439880371\n",
      "Epoch 300, Loss: 0.8828611969947815\n",
      "Epoch 320, Loss: 0.8905155658721924\n",
      "Epoch 340, Loss: 0.8997182846069336\n",
      "Epoch 360, Loss: 0.910361647605896\n",
      "Epoch 380, Loss: 0.9223273992538452\n",
      "Epoch 400, Loss: 0.9354904294013977\n",
      "Epoch 420, Loss: 0.9497215151786804\n",
      "Epoch 440, Loss: 0.964890718460083\n",
      "Epoch 460, Loss: 0.9808688163757324\n",
      "Epoch 480, Loss: 0.9975303411483765\n",
      "weights of  P073 :  [array([[-0.16442771, -0.16254579],\n",
      "       [ 0.6948518 , -0.16495007],\n",
      "       [-0.01059298,  0.41914415],\n",
      "       [ 0.32157037, -0.10499449],\n",
      "       [-0.7085296 ,  0.74665904]], dtype=float32), array([-0.15716806,  0.15716805], dtype=float32)]\n",
      "Lowest loss of  P073 :  0.86925924\n",
      "Epoch 0, Loss: 6.012973308563232\n",
      "Epoch 20, Loss: 1.2009198665618896\n",
      "Epoch 40, Loss: 0.9553302526473999\n",
      "Epoch 60, Loss: 0.937264621257782\n",
      "Epoch 80, Loss: 0.9375117421150208\n",
      "Epoch 100, Loss: 0.9344167113304138\n",
      "Epoch 120, Loss: 0.9333946704864502\n",
      "Epoch 140, Loss: 0.9323109984397888\n",
      "Epoch 160, Loss: 0.931178867816925\n",
      "Epoch 180, Loss: 0.9299801588058472\n",
      "Epoch 200, Loss: 0.9287241697311401\n",
      "Epoch 220, Loss: 0.9274153709411621\n",
      "Epoch 240, Loss: 0.9260585308074951\n",
      "Epoch 260, Loss: 0.9246582388877869\n",
      "Epoch 280, Loss: 0.9232189655303955\n",
      "Epoch 300, Loss: 0.9217445850372314\n",
      "Epoch 320, Loss: 0.9202390313148499\n",
      "Epoch 340, Loss: 0.9187058806419373\n",
      "Epoch 360, Loss: 0.917148768901825\n",
      "Epoch 380, Loss: 0.9155709147453308\n",
      "Epoch 400, Loss: 0.9139755964279175\n",
      "Epoch 420, Loss: 0.9123659729957581\n",
      "Epoch 440, Loss: 0.9107446670532227\n",
      "Epoch 460, Loss: 0.9091150164604187\n",
      "Epoch 480, Loss: 0.9074795842170715\n",
      "weights of  P074 :  [array([[ 0.33260012,  0.24267311],\n",
      "       [ 0.7938982 ,  0.06606787],\n",
      "       [ 0.08570779,  0.5240991 ],\n",
      "       [-0.19865151,  0.24853604],\n",
      "       [-0.665358  , -0.6294856 ]], dtype=float32), array([-0.28704357,  0.28704354], dtype=float32)]\n",
      "Lowest loss of  P074 :  0.905923\n",
      "Epoch 0, Loss: 2.990187406539917\n",
      "Epoch 20, Loss: 1.0532503128051758\n",
      "Epoch 40, Loss: 0.8522903323173523\n",
      "Epoch 60, Loss: 0.8467991352081299\n",
      "Epoch 80, Loss: 0.8425140976905823\n",
      "Epoch 100, Loss: 0.8405133485794067\n",
      "Epoch 120, Loss: 0.8383579254150391\n",
      "Epoch 140, Loss: 0.8362451791763306\n",
      "Epoch 160, Loss: 0.8341963291168213\n",
      "Epoch 180, Loss: 0.8322509527206421\n",
      "Epoch 200, Loss: 0.8304519653320312\n",
      "Epoch 220, Loss: 0.8288397789001465\n",
      "Epoch 240, Loss: 0.8274515867233276\n",
      "Epoch 260, Loss: 0.8263216018676758\n",
      "Epoch 280, Loss: 0.8254807591438293\n",
      "Epoch 300, Loss: 0.8249574303627014\n",
      "Epoch 320, Loss: 0.8247765302658081\n",
      "Epoch 340, Loss: 0.8249605298042297\n",
      "Epoch 360, Loss: 0.8255290985107422\n",
      "Epoch 380, Loss: 0.8264989852905273\n",
      "Epoch 400, Loss: 0.8278846740722656\n",
      "Epoch 420, Loss: 0.8296981453895569\n",
      "Epoch 440, Loss: 0.8319486379623413\n",
      "Epoch 460, Loss: 0.8346433639526367\n",
      "Epoch 480, Loss: 0.8377873301506042\n",
      "weights of  P075 :  [array([[ 0.8675584 ,  0.14921546],\n",
      "       [ 0.1603308 ,  0.3147675 ],\n",
      "       [-0.16983706, -0.4804846 ],\n",
      "       [-0.67046094,  0.07123955],\n",
      "       [ 0.03250766, -0.23326215]], dtype=float32), array([-0.14642169,  0.14642167], dtype=float32)]\n",
      "Lowest loss of  P075 :  0.82477653\n",
      "Epoch 0, Loss: 0.9507771730422974\n",
      "Epoch 20, Loss: 0.9347854852676392\n",
      "Epoch 40, Loss: 0.9280492067337036\n",
      "Epoch 60, Loss: 0.9242097735404968\n",
      "Epoch 80, Loss: 0.9235858917236328\n",
      "Epoch 100, Loss: 0.926850438117981\n",
      "Epoch 120, Loss: 0.934135913848877\n",
      "Epoch 140, Loss: 0.9453057646751404\n",
      "Epoch 160, Loss: 0.9601011872291565\n",
      "Epoch 180, Loss: 0.978243350982666\n",
      "Epoch 200, Loss: 0.9994875192642212\n",
      "Epoch 220, Loss: 1.0236445665359497\n",
      "Epoch 240, Loss: 1.0505801439285278\n",
      "Epoch 260, Loss: 1.080203890800476\n",
      "Epoch 280, Loss: 1.1124542951583862\n",
      "Epoch 300, Loss: 1.1472837924957275\n",
      "Epoch 320, Loss: 1.1846463680267334\n",
      "Epoch 340, Loss: 1.2244869470596313\n",
      "Epoch 360, Loss: 1.2667384147644043\n",
      "Epoch 380, Loss: 1.3113157749176025\n",
      "Epoch 400, Loss: 1.3581175804138184\n",
      "Epoch 420, Loss: 1.407027006149292\n",
      "Epoch 440, Loss: 1.4579133987426758\n",
      "Epoch 460, Loss: 1.5106340646743774\n",
      "Epoch 480, Loss: 1.565039873123169\n",
      "weights of  P076 :  [array([[-0.03681835, -0.732682  ],\n",
      "       [ 0.6456266 ,  0.24430472],\n",
      "       [-0.07857049,  0.39369854],\n",
      "       [-0.6333136 , -0.26123738],\n",
      "       [ 0.58748037,  0.7126368 ]], dtype=float32), array([ 0.06501231, -0.06501229], dtype=float32)]\n",
      "Lowest loss of  P076 :  0.92338043\n",
      "Epoch 0, Loss: 2.856708526611328\n",
      "Epoch 20, Loss: 1.2641785144805908\n",
      "Epoch 40, Loss: 1.1314340829849243\n",
      "Epoch 60, Loss: 1.1337133646011353\n",
      "Epoch 80, Loss: 1.1313731670379639\n",
      "Epoch 100, Loss: 1.1312499046325684\n",
      "Epoch 120, Loss: 1.1311980485916138\n",
      "Epoch 140, Loss: 1.131152629852295\n",
      "Epoch 160, Loss: 1.1311357021331787\n",
      "Epoch 180, Loss: 1.1311535835266113\n",
      "Epoch 200, Loss: 1.131211519241333\n",
      "Epoch 220, Loss: 1.1313155889511108\n",
      "Epoch 240, Loss: 1.1314711570739746\n",
      "Epoch 260, Loss: 1.1316845417022705\n",
      "Epoch 280, Loss: 1.131961464881897\n",
      "Epoch 300, Loss: 1.132307767868042\n",
      "Epoch 320, Loss: 1.132729172706604\n",
      "Epoch 340, Loss: 1.1332311630249023\n",
      "Epoch 360, Loss: 1.1338191032409668\n",
      "Epoch 380, Loss: 1.1344985961914062\n",
      "Epoch 400, Loss: 1.1352750062942505\n",
      "Epoch 420, Loss: 1.1361531019210815\n",
      "Epoch 440, Loss: 1.1371382474899292\n",
      "Epoch 460, Loss: 1.1382352113723755\n",
      "Epoch 480, Loss: 1.1394487619400024\n",
      "weights of  P077 :  [array([[-0.90821725, -0.60328287],\n",
      "       [ 0.69181675, -0.04279285],\n",
      "       [-0.75427777, -0.35917237],\n",
      "       [ 0.7829156 , -0.21735384],\n",
      "       [-0.9207811 ,  0.8769614 ]], dtype=float32), array([ 0.1013662 , -0.10136624], dtype=float32)]\n",
      "Lowest loss of  P077 :  1.1311343\n",
      "Epoch 0, Loss: 1.8021622896194458\n",
      "Epoch 20, Loss: 1.0418760776519775\n",
      "Epoch 40, Loss: 1.0268608331680298\n",
      "Epoch 60, Loss: 1.0245141983032227\n",
      "Epoch 80, Loss: 1.0221810340881348\n",
      "Epoch 100, Loss: 1.0199636220932007\n",
      "Epoch 120, Loss: 1.0175902843475342\n",
      "Epoch 140, Loss: 1.0150797367095947\n",
      "Epoch 160, Loss: 1.01248037815094\n",
      "Epoch 180, Loss: 1.0098183155059814\n",
      "Epoch 200, Loss: 1.0071135759353638\n",
      "Epoch 220, Loss: 1.0043864250183105\n",
      "Epoch 240, Loss: 1.0016546249389648\n",
      "Epoch 260, Loss: 0.9989327192306519\n",
      "Epoch 280, Loss: 0.9962331056594849\n",
      "Epoch 300, Loss: 0.9935659170150757\n",
      "Epoch 320, Loss: 0.9909398555755615\n",
      "Epoch 340, Loss: 0.9883614778518677\n",
      "Epoch 360, Loss: 0.9858360290527344\n",
      "Epoch 380, Loss: 0.9833676815032959\n",
      "Epoch 400, Loss: 0.9809591770172119\n",
      "Epoch 420, Loss: 0.9786121845245361\n",
      "Epoch 440, Loss: 0.9763277769088745\n",
      "Epoch 460, Loss: 0.9741062521934509\n",
      "Epoch 480, Loss: 0.971947193145752\n",
      "weights of  P078 :  [array([[-0.28699753,  0.08150987],\n",
      "       [ 0.52569085,  0.7262039 ],\n",
      "       [-0.39655703,  0.5863852 ],\n",
      "       [-0.0614952 , -0.6054838 ],\n",
      "       [ 0.9626197 ,  0.25334492]], dtype=float32), array([-0.22619706,  0.22619705], dtype=float32)]\n",
      "Lowest loss of  P078 :  0.9699534\n",
      "Epoch 0, Loss: 2.7204201221466064\n",
      "Epoch 20, Loss: 1.0292432308197021\n",
      "Epoch 40, Loss: 0.8880214095115662\n",
      "Epoch 60, Loss: 0.890306293964386\n",
      "Epoch 80, Loss: 0.8866260647773743\n",
      "Epoch 100, Loss: 0.8856047987937927\n",
      "Epoch 120, Loss: 0.8846532106399536\n",
      "Epoch 140, Loss: 0.8836545944213867\n",
      "Epoch 160, Loss: 0.8826255798339844\n",
      "Epoch 180, Loss: 0.8815796375274658\n",
      "Epoch 200, Loss: 0.8805297017097473\n",
      "Epoch 220, Loss: 0.8794881105422974\n",
      "Epoch 240, Loss: 0.8784664273262024\n",
      "Epoch 260, Loss: 0.8774760365486145\n",
      "Epoch 280, Loss: 0.8765276074409485\n",
      "Epoch 300, Loss: 0.8756314516067505\n",
      "Epoch 320, Loss: 0.8747976422309875\n",
      "Epoch 340, Loss: 0.8740358352661133\n",
      "Epoch 360, Loss: 0.8733555674552917\n",
      "Epoch 380, Loss: 0.8727657794952393\n",
      "Epoch 400, Loss: 0.872275173664093\n",
      "Epoch 420, Loss: 0.8718923330307007\n",
      "Epoch 440, Loss: 0.8716254234313965\n",
      "Epoch 460, Loss: 0.8714823722839355\n",
      "Epoch 480, Loss: 0.8714709281921387\n",
      "weights of  P079 :  [array([[ 0.55108786,  0.5210898 ],\n",
      "       [-0.5160624 ,  0.12170401],\n",
      "       [ 0.3110477 ,  0.37572575],\n",
      "       [-0.30863556, -0.58723795],\n",
      "       [ 0.57099205,  0.3121225 ]], dtype=float32), array([-0.37773722,  0.37773725], dtype=float32)]\n",
      "Lowest loss of  P079 :  0.8714592\n",
      "Epoch 0, Loss: 6.802469253540039\n",
      "Epoch 20, Loss: 1.6984184980392456\n",
      "Epoch 40, Loss: 1.2349926233291626\n",
      "Epoch 60, Loss: 1.1477515697479248\n",
      "Epoch 80, Loss: 1.1453520059585571\n",
      "Epoch 100, Loss: 1.1436697244644165\n",
      "Epoch 120, Loss: 1.1420104503631592\n",
      "Epoch 140, Loss: 1.1405977010726929\n",
      "Epoch 160, Loss: 1.139070987701416\n",
      "Epoch 180, Loss: 1.137467622756958\n",
      "Epoch 200, Loss: 1.1357876062393188\n",
      "Epoch 220, Loss: 1.1340374946594238\n",
      "Epoch 240, Loss: 1.1322243213653564\n",
      "Epoch 260, Loss: 1.130354881286621\n",
      "Epoch 280, Loss: 1.1284352540969849\n",
      "Epoch 300, Loss: 1.126470685005188\n",
      "Epoch 320, Loss: 1.1244672536849976\n",
      "Epoch 340, Loss: 1.1224300861358643\n",
      "Epoch 360, Loss: 1.1203640699386597\n",
      "Epoch 380, Loss: 1.1182746887207031\n",
      "Epoch 400, Loss: 1.1161670684814453\n",
      "Epoch 420, Loss: 1.11404550075531\n",
      "Epoch 440, Loss: 1.1119149923324585\n",
      "Epoch 460, Loss: 1.1097795963287354\n",
      "Epoch 480, Loss: 1.1076442003250122\n",
      "weights of  P080 :  [array([[ 0.51270235,  0.82329607],\n",
      "       [ 0.37789407, -0.48632243],\n",
      "       [ 0.7325704 ,  0.13154554],\n",
      "       [-1.1595646 , -0.4664711 ],\n",
      "       [-0.61917186, -0.3509936 ]], dtype=float32), array([ 0.20661525, -0.20661518], dtype=float32)]\n",
      "Lowest loss of  P080 :  1.1056195\n",
      "Epoch 0, Loss: 2.4747464656829834\n",
      "Epoch 20, Loss: 1.1362935304641724\n",
      "Epoch 40, Loss: 1.0994888544082642\n",
      "Epoch 60, Loss: 1.0920571088790894\n",
      "Epoch 80, Loss: 1.0916917324066162\n",
      "Epoch 100, Loss: 1.0912387371063232\n",
      "Epoch 120, Loss: 1.0909318923950195\n",
      "Epoch 140, Loss: 1.0906572341918945\n",
      "Epoch 160, Loss: 1.0904031991958618\n",
      "Epoch 180, Loss: 1.0901767015457153\n",
      "Epoch 200, Loss: 1.0899864435195923\n",
      "Epoch 220, Loss: 1.0898404121398926\n",
      "Epoch 240, Loss: 1.0897468328475952\n",
      "Epoch 260, Loss: 1.0897139310836792\n",
      "Epoch 280, Loss: 1.0897483825683594\n",
      "Epoch 300, Loss: 1.0898579359054565\n",
      "Epoch 320, Loss: 1.0900492668151855\n",
      "Epoch 340, Loss: 1.0903292894363403\n",
      "Epoch 360, Loss: 1.0907042026519775\n",
      "Epoch 380, Loss: 1.0911803245544434\n",
      "Epoch 400, Loss: 1.0917632579803467\n",
      "Epoch 420, Loss: 1.0924588441848755\n",
      "Epoch 440, Loss: 1.0932724475860596\n",
      "Epoch 460, Loss: 1.0942087173461914\n",
      "Epoch 480, Loss: 1.0952726602554321\n",
      "weights of  P081 :  [array([[-0.47147658, -1.0235751 ],\n",
      "       [-0.66904   ,  0.1883711 ],\n",
      "       [ 0.6387654 ,  0.27720064],\n",
      "       [-0.4740248 , -0.7111941 ],\n",
      "       [-0.8262114 , -0.6177785 ]], dtype=float32), array([-0.03772019,  0.03772014], dtype=float32)]\n",
      "Lowest loss of  P081 :  1.0897138\n",
      "Epoch 0, Loss: 8.95499038696289\n",
      "Epoch 20, Loss: 3.666566848754883\n",
      "Epoch 40, Loss: 1.1783167123794556\n",
      "Epoch 60, Loss: 0.9463778734207153\n",
      "Epoch 80, Loss: 0.9110405445098877\n",
      "Epoch 100, Loss: 0.9041665196418762\n",
      "Epoch 120, Loss: 0.9036642909049988\n",
      "Epoch 140, Loss: 0.9036868214607239\n",
      "Epoch 160, Loss: 0.9037493467330933\n",
      "Epoch 180, Loss: 0.9038461446762085\n",
      "Epoch 200, Loss: 0.903975248336792\n",
      "Epoch 220, Loss: 0.9041450023651123\n",
      "Epoch 240, Loss: 0.9043583869934082\n",
      "Epoch 260, Loss: 0.9046188592910767\n",
      "Epoch 280, Loss: 0.9049302935600281\n",
      "Epoch 300, Loss: 0.905296266078949\n",
      "Epoch 320, Loss: 0.9057202339172363\n",
      "Epoch 340, Loss: 0.9062056541442871\n",
      "Epoch 360, Loss: 0.9067556858062744\n",
      "Epoch 380, Loss: 0.9073731899261475\n",
      "Epoch 400, Loss: 0.9080612659454346\n",
      "Epoch 420, Loss: 0.9088221788406372\n",
      "Epoch 440, Loss: 0.9096589088439941\n",
      "Epoch 460, Loss: 0.9105733633041382\n",
      "Epoch 480, Loss: 0.9115679264068604\n",
      "weights of  P082 :  [array([[-0.64124024, -0.31275553],\n",
      "       [ 0.47036892,  0.6260131 ],\n",
      "       [ 0.5421185 , -0.53583103],\n",
      "       [ 0.32259014,  0.5137643 ],\n",
      "       [-0.42928305, -0.29616737]], dtype=float32), array([ 0.33207855, -0.33207855], dtype=float32)]\n",
      "Lowest loss of  P082 :  0.9036214\n",
      "Epoch 0, Loss: 3.6874516010284424\n",
      "Epoch 20, Loss: 1.245957374572754\n",
      "Epoch 40, Loss: 1.03829026222229\n",
      "Epoch 60, Loss: 1.0013923645019531\n",
      "Epoch 80, Loss: 1.0020833015441895\n",
      "Epoch 100, Loss: 1.0016818046569824\n",
      "Epoch 120, Loss: 1.0019193887710571\n",
      "Epoch 140, Loss: 1.0022003650665283\n",
      "Epoch 160, Loss: 1.0025548934936523\n",
      "Epoch 180, Loss: 1.002994179725647\n",
      "Epoch 200, Loss: 1.0035254955291748\n",
      "Epoch 220, Loss: 1.0041570663452148\n",
      "Epoch 240, Loss: 1.0048972368240356\n",
      "Epoch 260, Loss: 1.0057555437088013\n",
      "Epoch 280, Loss: 1.0067402124404907\n",
      "Epoch 300, Loss: 1.007859468460083\n",
      "Epoch 320, Loss: 1.009122371673584\n",
      "Epoch 340, Loss: 1.010536789894104\n",
      "Epoch 360, Loss: 1.0121110677719116\n",
      "Epoch 380, Loss: 1.013852834701538\n",
      "Epoch 400, Loss: 1.0157701969146729\n",
      "Epoch 420, Loss: 1.017870306968689\n",
      "Epoch 440, Loss: 1.0201609134674072\n",
      "Epoch 460, Loss: 1.022648572921753\n",
      "Epoch 480, Loss: 1.0253403186798096\n",
      "weights of  P083 :  [array([[-0.53387016, -0.81831574],\n",
      "       [-0.7735673 , -0.11256348],\n",
      "       [ 0.36588648,  0.39469716],\n",
      "       [-0.09588932, -1.0325378 ],\n",
      "       [-0.6229591 ,  0.30193433]], dtype=float32), array([-0.13225417,  0.13225418], dtype=float32)]\n",
      "Lowest loss of  P083 :  1.0013396\n",
      "Epoch 0, Loss: 2.868278980255127\n",
      "Epoch 20, Loss: 1.1695903539657593\n",
      "Epoch 40, Loss: 1.0266170501708984\n",
      "Epoch 60, Loss: 1.0197670459747314\n",
      "Epoch 80, Loss: 1.0058335065841675\n",
      "Epoch 100, Loss: 0.9942291975021362\n",
      "Epoch 120, Loss: 0.9827635288238525\n",
      "Epoch 140, Loss: 0.9716205596923828\n",
      "Epoch 160, Loss: 0.9610549211502075\n",
      "Epoch 180, Loss: 0.9512557983398438\n",
      "Epoch 200, Loss: 0.9423540830612183\n",
      "Epoch 220, Loss: 0.9344293475151062\n",
      "Epoch 240, Loss: 0.9275182485580444\n",
      "Epoch 260, Loss: 0.921621561050415\n",
      "Epoch 280, Loss: 0.9167124629020691\n",
      "Epoch 300, Loss: 0.9127417802810669\n",
      "Epoch 320, Loss: 0.9096455574035645\n",
      "Epoch 340, Loss: 0.9073488712310791\n",
      "Epoch 360, Loss: 0.9057714939117432\n",
      "Epoch 380, Loss: 0.9048303365707397\n",
      "Epoch 400, Loss: 0.9044430255889893\n",
      "Epoch 420, Loss: 0.9045294523239136\n",
      "Epoch 440, Loss: 0.9050136804580688\n",
      "Epoch 460, Loss: 0.9058244228363037\n",
      "Epoch 480, Loss: 0.9068962335586548\n",
      "weights of  P084 :  [array([[ 0.5461143 ,  0.16161227],\n",
      "       [-0.8941643 , -0.21143095],\n",
      "       [ 0.87281984, -0.5801358 ],\n",
      "       [-0.4484521 ,  0.088641  ],\n",
      "       [ 0.29874253,  0.4828291 ]], dtype=float32), array([-0.01142645,  0.01142645], dtype=float32)]\n",
      "Lowest loss of  P084 :  0.90442276\n",
      "Epoch 0, Loss: 1.0434598922729492\n",
      "Epoch 20, Loss: 0.7868571281433105\n",
      "Epoch 40, Loss: 0.7861606478691101\n",
      "Epoch 60, Loss: 0.7950042486190796\n",
      "Epoch 80, Loss: 0.8064574003219604\n",
      "Epoch 100, Loss: 0.8209315538406372\n",
      "Epoch 120, Loss: 0.8384287357330322\n",
      "Epoch 140, Loss: 0.8590002059936523\n",
      "Epoch 160, Loss: 0.8826704025268555\n",
      "Epoch 180, Loss: 0.9094167947769165\n",
      "Epoch 200, Loss: 0.9391874670982361\n",
      "Epoch 220, Loss: 0.9719036817550659\n",
      "Epoch 240, Loss: 1.007468342781067\n",
      "Epoch 260, Loss: 1.0457714796066284\n",
      "Epoch 280, Loss: 1.0866961479187012\n",
      "Epoch 300, Loss: 1.1301219463348389\n",
      "Epoch 320, Loss: 1.1759274005889893\n",
      "Epoch 340, Loss: 1.2239934206008911\n",
      "Epoch 360, Loss: 1.2742042541503906\n",
      "Epoch 380, Loss: 1.326448678970337\n",
      "Epoch 400, Loss: 1.3806207180023193\n",
      "Epoch 420, Loss: 1.4366201162338257\n",
      "Epoch 440, Loss: 1.494351863861084\n",
      "Epoch 460, Loss: 1.5537272691726685\n",
      "Epoch 480, Loss: 1.6146639585494995\n",
      "weights of  P085 :  [array([[ 0.22675912, -0.7797187 ],\n",
      "       [-0.7663999 ,  0.8252248 ],\n",
      "       [-0.7022689 , -0.6010278 ],\n",
      "       [ 0.35610932, -0.35720873],\n",
      "       [-0.313012  , -0.31714618]], dtype=float32), array([-0.03050632,  0.03050632], dtype=float32)]\n",
      "Lowest loss of  P085 :  0.77825403\n",
      "Epoch 0, Loss: 4.258800506591797\n",
      "Epoch 20, Loss: 1.2314436435699463\n",
      "Epoch 40, Loss: 1.055775761604309\n",
      "Epoch 60, Loss: 1.0052863359451294\n",
      "Epoch 80, Loss: 1.0022575855255127\n",
      "Epoch 100, Loss: 1.000617265701294\n",
      "Epoch 120, Loss: 0.9994313716888428\n",
      "Epoch 140, Loss: 0.9981940984725952\n",
      "Epoch 160, Loss: 0.9969469308853149\n",
      "Epoch 180, Loss: 0.9956979751586914\n",
      "Epoch 200, Loss: 0.9944645166397095\n",
      "Epoch 220, Loss: 0.9932636022567749\n",
      "Epoch 240, Loss: 0.9921115636825562\n",
      "Epoch 260, Loss: 0.9910238981246948\n",
      "Epoch 280, Loss: 0.990014910697937\n",
      "Epoch 300, Loss: 0.9890986680984497\n",
      "Epoch 320, Loss: 0.9882882833480835\n",
      "Epoch 340, Loss: 0.9875961542129517\n",
      "Epoch 360, Loss: 0.9870343208312988\n",
      "Epoch 380, Loss: 0.986613929271698\n",
      "Epoch 400, Loss: 0.9863457679748535\n",
      "Epoch 420, Loss: 0.986240029335022\n",
      "Epoch 440, Loss: 0.9863061904907227\n",
      "Epoch 460, Loss: 0.9865530729293823\n",
      "Epoch 480, Loss: 0.9869897365570068\n",
      "weights of  P086 :  [array([[-0.22351083, -0.7755089 ],\n",
      "       [ 0.48762012,  0.53426653],\n",
      "       [ 0.30162054,  0.6184898 ],\n",
      "       [ 0.54771775,  0.6857062 ],\n",
      "       [ 0.62394816,  0.5724907 ]], dtype=float32), array([-0.00297482,  0.00297485], dtype=float32)]\n",
      "Lowest loss of  P086 :  0.9862387\n",
      "Epoch 0, Loss: 2.308304786682129\n",
      "Epoch 20, Loss: 0.9360479116439819\n",
      "Epoch 40, Loss: 0.8966279625892639\n",
      "Epoch 60, Loss: 0.8899984955787659\n",
      "Epoch 80, Loss: 0.889525830745697\n",
      "Epoch 100, Loss: 0.8889380097389221\n",
      "Epoch 120, Loss: 0.8885050415992737\n",
      "Epoch 140, Loss: 0.8880866169929504\n",
      "Epoch 160, Loss: 0.8876588344573975\n",
      "Epoch 180, Loss: 0.8872216939926147\n",
      "Epoch 200, Loss: 0.8867784142494202\n",
      "Epoch 220, Loss: 0.8863332271575928\n",
      "Epoch 240, Loss: 0.8858899474143982\n",
      "Epoch 260, Loss: 0.8854527473449707\n",
      "Epoch 280, Loss: 0.8850258588790894\n",
      "Epoch 300, Loss: 0.8846131563186646\n",
      "Epoch 320, Loss: 0.8842183947563171\n",
      "Epoch 340, Loss: 0.8838454484939575\n",
      "Epoch 360, Loss: 0.8834981918334961\n",
      "Epoch 380, Loss: 0.8831801414489746\n",
      "Epoch 400, Loss: 0.8828951120376587\n",
      "Epoch 420, Loss: 0.8826466798782349\n",
      "Epoch 440, Loss: 0.8824383020401001\n",
      "Epoch 460, Loss: 0.8822738528251648\n",
      "Epoch 480, Loss: 0.8821565508842468\n",
      "weights of  P087 :  [array([[ 0.7447096 , -0.78999066],\n",
      "       [-0.35413384,  0.39437348],\n",
      "       [-0.61936784, -0.12789789],\n",
      "       [-0.06172271, -0.00777302],\n",
      "       [ 0.3379537 ,  0.21614142]], dtype=float32), array([ 0.1948342, -0.1948342], dtype=float32)]\n",
      "Lowest loss of  P087 :  0.8820922\n",
      "Epoch 0, Loss: 1.3124996423721313\n",
      "Epoch 20, Loss: 0.9102718830108643\n",
      "Epoch 40, Loss: 0.8830233812332153\n",
      "Epoch 60, Loss: 0.8755496740341187\n",
      "Epoch 80, Loss: 0.8705460429191589\n",
      "Epoch 100, Loss: 0.865565299987793\n",
      "Epoch 120, Loss: 0.8603764176368713\n",
      "Epoch 140, Loss: 0.8551385998725891\n",
      "Epoch 160, Loss: 0.8499593734741211\n",
      "Epoch 180, Loss: 0.8449403047561646\n",
      "Epoch 200, Loss: 0.840171217918396\n",
      "Epoch 220, Loss: 0.8357306122779846\n",
      "Epoch 240, Loss: 0.8316864371299744\n",
      "Epoch 260, Loss: 0.8280967473983765\n",
      "Epoch 280, Loss: 0.8250100016593933\n",
      "Epoch 300, Loss: 0.822466254234314\n",
      "Epoch 320, Loss: 0.8204976320266724\n",
      "Epoch 340, Loss: 0.8191285133361816\n",
      "Epoch 360, Loss: 0.8183773756027222\n",
      "Epoch 380, Loss: 0.8182568550109863\n",
      "Epoch 400, Loss: 0.8187747597694397\n",
      "Epoch 420, Loss: 0.8199352025985718\n",
      "Epoch 440, Loss: 0.8217387199401855\n",
      "Epoch 460, Loss: 0.8241838216781616\n",
      "Epoch 480, Loss: 0.827266275882721\n",
      "weights of  P088 :  [array([[ 0.7859333 ,  0.04712585],\n",
      "       [ 0.32669014,  0.22980583],\n",
      "       [ 0.24779952, -0.08797197],\n",
      "       [-0.6280298 , -0.04588013],\n",
      "       [ 0.13498688,  0.41313794]], dtype=float32), array([ 0.09532677, -0.09532676], dtype=float32)]\n",
      "Lowest loss of  P088 :  0.8182263\n",
      "Epoch 0, Loss: 2.5409812927246094\n",
      "Epoch 20, Loss: 1.108534336090088\n",
      "Epoch 40, Loss: 1.040339469909668\n",
      "Epoch 60, Loss: 1.037899136543274\n",
      "Epoch 80, Loss: 1.0367286205291748\n",
      "Epoch 100, Loss: 1.0362733602523804\n",
      "Epoch 120, Loss: 1.036045789718628\n",
      "Epoch 140, Loss: 1.035838007926941\n",
      "Epoch 160, Loss: 1.0356409549713135\n",
      "Epoch 180, Loss: 1.0354598760604858\n",
      "Epoch 200, Loss: 1.0353007316589355\n",
      "Epoch 220, Loss: 1.0351693630218506\n",
      "Epoch 240, Loss: 1.0350711345672607\n",
      "Epoch 260, Loss: 1.035012125968933\n",
      "Epoch 280, Loss: 1.0349973440170288\n",
      "Epoch 300, Loss: 1.0350323915481567\n",
      "Epoch 320, Loss: 1.0351223945617676\n",
      "Epoch 340, Loss: 1.0352723598480225\n",
      "Epoch 360, Loss: 1.035487413406372\n",
      "Epoch 380, Loss: 1.0357725620269775\n",
      "Epoch 400, Loss: 1.0361323356628418\n",
      "Epoch 420, Loss: 1.036571741104126\n",
      "Epoch 440, Loss: 1.0370951890945435\n",
      "Epoch 460, Loss: 1.0377073287963867\n",
      "Epoch 480, Loss: 1.0384125709533691\n",
      "weights of  P089 :  [array([[ 0.0660753 , -0.7663929 ],\n",
      "       [-0.31597513, -0.7444505 ],\n",
      "       [-0.11041404,  0.54858655],\n",
      "       [ 0.81824213,  0.9110348 ],\n",
      "       [ 0.10124936,  0.66763574]], dtype=float32), array([ 0.09509072, -0.09509072], dtype=float32)]\n",
      "Lowest loss of  P089 :  1.0349965\n",
      "Epoch 0, Loss: 1.7420088052749634\n",
      "Epoch 20, Loss: 0.9409375190734863\n",
      "Epoch 40, Loss: 0.9355943202972412\n",
      "Epoch 60, Loss: 0.9307419061660767\n",
      "Epoch 80, Loss: 0.9270597100257874\n",
      "Epoch 100, Loss: 0.9237401485443115\n",
      "Epoch 120, Loss: 0.9202593564987183\n",
      "Epoch 140, Loss: 0.916567325592041\n",
      "Epoch 160, Loss: 0.9126930236816406\n",
      "Epoch 180, Loss: 0.9086705446243286\n",
      "Epoch 200, Loss: 0.904532790184021\n",
      "Epoch 220, Loss: 0.9003100991249084\n",
      "Epoch 240, Loss: 0.8960296511650085\n",
      "Epoch 260, Loss: 0.8917169570922852\n",
      "Epoch 280, Loss: 0.8873953819274902\n",
      "Epoch 300, Loss: 0.8830869197845459\n",
      "Epoch 320, Loss: 0.8788120746612549\n",
      "Epoch 340, Loss: 0.8745898008346558\n",
      "Epoch 360, Loss: 0.8704383373260498\n",
      "Epoch 380, Loss: 0.8663738965988159\n",
      "Epoch 400, Loss: 0.8624124526977539\n",
      "Epoch 420, Loss: 0.8585681915283203\n",
      "Epoch 440, Loss: 0.8548545241355896\n",
      "Epoch 460, Loss: 0.8512840270996094\n",
      "Epoch 480, Loss: 0.8478682637214661\n",
      "weights of  P090 :  [array([[ 0.42014876, -0.19803983],\n",
      "       [ 0.67280966,  0.5984111 ],\n",
      "       [ 0.20119062,  0.38638481],\n",
      "       [-0.4495135 ,  0.01708016],\n",
      "       [ 0.17001481, -0.20614457]], dtype=float32), array([ 0.3064594 , -0.30645943], dtype=float32)]\n",
      "Lowest loss of  P090 :  0.8447763\n",
      "Epoch 0, Loss: 5.9041008949279785\n",
      "Epoch 20, Loss: 1.1414012908935547\n",
      "Epoch 40, Loss: 1.0670149326324463\n",
      "Epoch 60, Loss: 1.065325140953064\n",
      "Epoch 80, Loss: 1.0645737648010254\n",
      "Epoch 100, Loss: 1.0621029138565063\n",
      "Epoch 120, Loss: 1.0617425441741943\n",
      "Epoch 140, Loss: 1.0613645315170288\n",
      "Epoch 160, Loss: 1.0609761476516724\n",
      "Epoch 180, Loss: 1.0605708360671997\n",
      "Epoch 200, Loss: 1.060157060623169\n",
      "Epoch 220, Loss: 1.0597373247146606\n",
      "Epoch 240, Loss: 1.059314250946045\n",
      "Epoch 260, Loss: 1.05889093875885\n",
      "Epoch 280, Loss: 1.0584707260131836\n",
      "Epoch 300, Loss: 1.058056116104126\n",
      "Epoch 320, Loss: 1.0576504468917847\n",
      "Epoch 340, Loss: 1.0572564601898193\n",
      "Epoch 360, Loss: 1.0568768978118896\n",
      "Epoch 380, Loss: 1.0565142631530762\n",
      "Epoch 400, Loss: 1.0561717748641968\n",
      "Epoch 420, Loss: 1.055851936340332\n",
      "Epoch 440, Loss: 1.0555572509765625\n",
      "Epoch 460, Loss: 1.0552902221679688\n",
      "Epoch 480, Loss: 1.0550537109375\n",
      "weights of  P091 :  [array([[ 0.70883036, -0.9344803 ],\n",
      "       [-0.5546537 ,  0.3943041 ],\n",
      "       [-0.20240094,  0.7730831 ],\n",
      "       [ 0.95660496,  0.19475475],\n",
      "       [-0.59179646,  0.18206823]], dtype=float32), array([-0.31549582,  0.3154958 ], dtype=float32)]\n",
      "Lowest loss of  P091 :  1.0548596\n",
      "Epoch 0, Loss: 8.412064552307129\n",
      "Epoch 20, Loss: 3.586074113845825\n",
      "Epoch 40, Loss: 1.0409334897994995\n",
      "Epoch 60, Loss: 0.8337855935096741\n",
      "Epoch 80, Loss: 0.7886519432067871\n",
      "Epoch 100, Loss: 0.7803890705108643\n",
      "Epoch 120, Loss: 0.7790289521217346\n",
      "Epoch 140, Loss: 0.7774593234062195\n",
      "Epoch 160, Loss: 0.7759246826171875\n",
      "Epoch 180, Loss: 0.7743605375289917\n",
      "Epoch 200, Loss: 0.7727816104888916\n",
      "Epoch 220, Loss: 0.7712019681930542\n",
      "Epoch 240, Loss: 0.7696352601051331\n",
      "Epoch 260, Loss: 0.7680945992469788\n",
      "Epoch 280, Loss: 0.766591489315033\n",
      "Epoch 300, Loss: 0.76513671875\n",
      "Epoch 320, Loss: 0.7637403011322021\n",
      "Epoch 340, Loss: 0.7624116539955139\n",
      "Epoch 360, Loss: 0.7611589431762695\n",
      "Epoch 380, Loss: 0.7599900364875793\n",
      "Epoch 400, Loss: 0.7589121460914612\n",
      "Epoch 420, Loss: 0.7579318881034851\n",
      "Epoch 440, Loss: 0.7570550441741943\n",
      "Epoch 460, Loss: 0.7562873363494873\n",
      "Epoch 480, Loss: 0.7556337118148804\n",
      "weights of  P092 :  [array([[-0.20683134,  0.06475818],\n",
      "       [-0.27086306,  0.70126563],\n",
      "       [-0.04540034, -0.1861852 ],\n",
      "       [ 0.26189023, -0.51223487],\n",
      "       [ 0.1218132 , -0.46068987]], dtype=float32), array([ 0.60008746, -0.60008746], dtype=float32)]\n",
      "Lowest loss of  P092 :  0.7551224\n",
      "Epoch 0, Loss: 2.9271795749664307\n",
      "Epoch 20, Loss: 1.1075841188430786\n",
      "Epoch 40, Loss: 0.9315327405929565\n",
      "Epoch 60, Loss: 0.9286198616027832\n",
      "Epoch 80, Loss: 0.9231085777282715\n",
      "Epoch 100, Loss: 0.9203793406486511\n",
      "Epoch 120, Loss: 0.9177002906799316\n",
      "Epoch 140, Loss: 0.9152770042419434\n",
      "Epoch 160, Loss: 0.9132022857666016\n",
      "Epoch 180, Loss: 0.9115692377090454\n",
      "Epoch 200, Loss: 0.9104630351066589\n",
      "Epoch 220, Loss: 0.9099591970443726\n",
      "Epoch 240, Loss: 0.9101231098175049\n",
      "Epoch 260, Loss: 0.9110107421875\n",
      "Epoch 280, Loss: 0.9126695394515991\n",
      "Epoch 300, Loss: 0.9151382446289062\n",
      "Epoch 320, Loss: 0.9184482097625732\n",
      "Epoch 340, Loss: 0.9226235151290894\n",
      "Epoch 360, Loss: 0.9276817440986633\n",
      "Epoch 380, Loss: 0.9336342811584473\n",
      "Epoch 400, Loss: 0.9404873847961426\n",
      "Epoch 420, Loss: 0.9482419490814209\n",
      "Epoch 440, Loss: 0.9568948745727539\n",
      "Epoch 460, Loss: 0.966438889503479\n",
      "Epoch 480, Loss: 0.9768638610839844\n",
      "weights of  P093 :  [array([[-0.48401242, -0.39849445],\n",
      "       [-1.2170383 , -0.03632358],\n",
      "       [ 0.7599503 ,  0.6381352 ],\n",
      "       [ 0.45096743, -0.10738228],\n",
      "       [ 0.14948677, -0.5382493 ]], dtype=float32), array([ 0.08002938, -0.08002935], dtype=float32)]\n",
      "Lowest loss of  P093 :  0.90993536\n",
      "Epoch 0, Loss: 9.405046463012695\n",
      "Epoch 20, Loss: 4.2950029373168945\n",
      "Epoch 40, Loss: 1.10765540599823\n",
      "Epoch 60, Loss: 0.9839093685150146\n",
      "Epoch 80, Loss: 0.960906982421875\n",
      "Epoch 100, Loss: 0.9521956443786621\n",
      "Epoch 120, Loss: 0.9480941295623779\n",
      "Epoch 140, Loss: 0.9451720714569092\n",
      "Epoch 160, Loss: 0.9422098398208618\n",
      "Epoch 180, Loss: 0.9391931891441345\n",
      "Epoch 200, Loss: 0.9361906051635742\n",
      "Epoch 220, Loss: 0.9332444667816162\n",
      "Epoch 240, Loss: 0.9303858280181885\n",
      "Epoch 260, Loss: 0.9276473522186279\n",
      "Epoch 280, Loss: 0.9250607490539551\n",
      "Epoch 300, Loss: 0.9226556420326233\n",
      "Epoch 320, Loss: 0.9204602837562561\n",
      "Epoch 340, Loss: 0.9185011386871338\n",
      "Epoch 360, Loss: 0.916803240776062\n",
      "Epoch 380, Loss: 0.9153900146484375\n",
      "Epoch 400, Loss: 0.9142834544181824\n",
      "Epoch 420, Loss: 0.9135043621063232\n",
      "Epoch 440, Loss: 0.9130721092224121\n",
      "Epoch 460, Loss: 0.9130045175552368\n",
      "Epoch 480, Loss: 0.9133181571960449\n",
      "weights of  P094 :  [array([[ 0.5384781 , -0.5274161 ],\n",
      "       [-0.76390606,  0.05849688],\n",
      "       [ 0.40735346,  0.04643204],\n",
      "       [ 0.57725185,  1.1237128 ],\n",
      "       [ 0.49602723, -0.14400494]], dtype=float32), array([ 0.2835841 , -0.28358403], dtype=float32)]\n",
      "Lowest loss of  P094 :  0.91298544\n",
      "Epoch 0, Loss: 4.310976505279541\n",
      "Epoch 20, Loss: 1.134589433670044\n",
      "Epoch 40, Loss: 1.0048823356628418\n",
      "Epoch 60, Loss: 0.9608030319213867\n",
      "Epoch 80, Loss: 0.9518938660621643\n",
      "Epoch 100, Loss: 0.9481592774391174\n",
      "Epoch 120, Loss: 0.9439690113067627\n",
      "Epoch 140, Loss: 0.9396142959594727\n",
      "Epoch 160, Loss: 0.9350656867027283\n",
      "Epoch 180, Loss: 0.9303559064865112\n",
      "Epoch 200, Loss: 0.9255253672599792\n",
      "Epoch 220, Loss: 0.9206115007400513\n",
      "Epoch 240, Loss: 0.9156497716903687\n",
      "Epoch 260, Loss: 0.9106726050376892\n",
      "Epoch 280, Loss: 0.9057106971740723\n",
      "Epoch 300, Loss: 0.9007934331893921\n",
      "Epoch 320, Loss: 0.8959476947784424\n",
      "Epoch 340, Loss: 0.8911992907524109\n",
      "Epoch 360, Loss: 0.8865728378295898\n",
      "Epoch 380, Loss: 0.88209068775177\n",
      "Epoch 400, Loss: 0.8777745962142944\n",
      "Epoch 420, Loss: 0.8736448287963867\n",
      "Epoch 440, Loss: 0.8697198033332825\n",
      "Epoch 460, Loss: 0.8660172820091248\n",
      "Epoch 480, Loss: 0.8625534772872925\n",
      "weights of  P095 :  [array([[ 0.53865105,  0.52332664],\n",
      "       [-0.08764616, -0.25327963],\n",
      "       [ 0.17467207,  0.4229304 ],\n",
      "       [-0.5522138 , -0.11017399],\n",
      "       [ 0.68193   , -0.05164346]], dtype=float32), array([ 0.00744755, -0.00744757], dtype=float32)]\n",
      "Lowest loss of  P095 :  0.8594974\n",
      "Epoch 0, Loss: 1.6542693376541138\n",
      "Epoch 20, Loss: 1.116160273551941\n",
      "Epoch 40, Loss: 1.0842043161392212\n",
      "Epoch 60, Loss: 1.0805771350860596\n",
      "Epoch 80, Loss: 1.0778751373291016\n",
      "Epoch 100, Loss: 1.075047492980957\n",
      "Epoch 120, Loss: 1.0721242427825928\n",
      "Epoch 140, Loss: 1.0690621137619019\n",
      "Epoch 160, Loss: 1.0658674240112305\n",
      "Epoch 180, Loss: 1.0625755786895752\n",
      "Epoch 200, Loss: 1.059220552444458\n",
      "Epoch 220, Loss: 1.0558257102966309\n",
      "Epoch 240, Loss: 1.052415370941162\n",
      "Epoch 260, Loss: 1.049010157585144\n",
      "Epoch 280, Loss: 1.0456290245056152\n",
      "Epoch 300, Loss: 1.0422885417938232\n",
      "Epoch 320, Loss: 1.0390037298202515\n",
      "Epoch 340, Loss: 1.0357871055603027\n",
      "Epoch 360, Loss: 1.0326504707336426\n",
      "Epoch 380, Loss: 1.029603362083435\n",
      "Epoch 400, Loss: 1.0266542434692383\n",
      "Epoch 420, Loss: 1.0238101482391357\n",
      "Epoch 440, Loss: 1.0210767984390259\n",
      "Epoch 460, Loss: 1.018458604812622\n",
      "Epoch 480, Loss: 1.0159592628479004\n",
      "weights of  P096 :  [array([[ 0.36617303, -0.25488746],\n",
      "       [-0.52543736,  0.64093006],\n",
      "       [-0.33745855, -0.8428241 ],\n",
      "       [ 0.3185478 ,  0.79265094],\n",
      "       [ 0.52886415, -0.81324786]], dtype=float32), array([ 0.43356535, -0.43356538], dtype=float32)]\n",
      "Lowest loss of  P096 :  1.0136969\n",
      "Epoch 0, Loss: 0.7931599020957947\n",
      "Epoch 20, Loss: 0.7509029507637024\n",
      "Epoch 40, Loss: 0.7479360699653625\n",
      "Epoch 60, Loss: 0.7462563514709473\n",
      "Epoch 80, Loss: 0.7445854544639587\n",
      "Epoch 100, Loss: 0.742816686630249\n",
      "Epoch 120, Loss: 0.7409658432006836\n",
      "Epoch 140, Loss: 0.7390651702880859\n",
      "Epoch 160, Loss: 0.7371505498886108\n",
      "Epoch 180, Loss: 0.7352515459060669\n",
      "Epoch 200, Loss: 0.7333945035934448\n",
      "Epoch 220, Loss: 0.7316023111343384\n",
      "Epoch 240, Loss: 0.729894757270813\n",
      "Epoch 260, Loss: 0.7282885909080505\n",
      "Epoch 280, Loss: 0.7267981171607971\n",
      "Epoch 300, Loss: 0.725435197353363\n",
      "Epoch 320, Loss: 0.7242088913917542\n",
      "Epoch 340, Loss: 0.7231267690658569\n",
      "Epoch 360, Loss: 0.7221938371658325\n",
      "Epoch 380, Loss: 0.7214135527610779\n",
      "Epoch 400, Loss: 0.7207877039909363\n",
      "Epoch 420, Loss: 0.7203162312507629\n",
      "Epoch 440, Loss: 0.7199981212615967\n",
      "Epoch 460, Loss: 0.7198306918144226\n",
      "Epoch 480, Loss: 0.7198105454444885\n",
      "weights of  P097 :  [array([[ 0.03174133,  0.13394798],\n",
      "       [-0.07269672,  0.08143435],\n",
      "       [ 0.3439035 , -0.16789043],\n",
      "       [ 0.01529446,  0.1670671 ],\n",
      "       [-0.32720506,  0.3408665 ]], dtype=float32), array([-1.3152663,  1.3152659], dtype=float32)]\n",
      "Lowest loss of  P097 :  0.7198011\n",
      "Epoch 0, Loss: 4.195130348205566\n",
      "Epoch 20, Loss: 1.0954267978668213\n",
      "Epoch 40, Loss: 0.9500017166137695\n",
      "Epoch 60, Loss: 0.9053958654403687\n",
      "Epoch 80, Loss: 0.9014198780059814\n",
      "Epoch 100, Loss: 0.9015601873397827\n",
      "Epoch 120, Loss: 0.9017748832702637\n",
      "Epoch 140, Loss: 0.9020893573760986\n",
      "Epoch 160, Loss: 0.9024641513824463\n",
      "Epoch 180, Loss: 0.9028870463371277\n",
      "Epoch 200, Loss: 0.9033592939376831\n",
      "Epoch 220, Loss: 0.9038838148117065\n",
      "Epoch 240, Loss: 0.9044628143310547\n",
      "Epoch 260, Loss: 0.9050987958908081\n",
      "Epoch 280, Loss: 0.9057940244674683\n",
      "Epoch 300, Loss: 0.9065508246421814\n",
      "Epoch 320, Loss: 0.9073714017868042\n",
      "Epoch 340, Loss: 0.9082577228546143\n",
      "Epoch 360, Loss: 0.9092118144035339\n",
      "Epoch 380, Loss: 0.9102356433868408\n",
      "Epoch 400, Loss: 0.9113308191299438\n",
      "Epoch 420, Loss: 0.9124990105628967\n",
      "Epoch 440, Loss: 0.9137415885925293\n",
      "Epoch 460, Loss: 0.9150600433349609\n",
      "Epoch 480, Loss: 0.9164557456970215\n",
      "weights of  P098 :  [array([[ 0.20898007, -0.51183933],\n",
      "       [-0.02052036,  1.0678632 ],\n",
      "       [ 0.05254582,  0.23693834],\n",
      "       [-0.03687692, -0.48855358],\n",
      "       [-0.4948891 , -0.70707786]], dtype=float32), array([ 0.14429142, -0.14429139], dtype=float32)]\n",
      "Lowest loss of  P098 :  0.90103495\n",
      "Epoch 0, Loss: 5.756803035736084\n",
      "Epoch 20, Loss: 1.0825783014297485\n",
      "Epoch 40, Loss: 1.0553475618362427\n",
      "Epoch 60, Loss: 1.0598047971725464\n",
      "Epoch 80, Loss: 1.0565886497497559\n",
      "Epoch 100, Loss: 1.0542168617248535\n",
      "Epoch 120, Loss: 1.0538690090179443\n",
      "Epoch 140, Loss: 1.0534642934799194\n",
      "Epoch 160, Loss: 1.0530576705932617\n",
      "Epoch 180, Loss: 1.0526306629180908\n",
      "Epoch 200, Loss: 1.0521914958953857\n",
      "Epoch 220, Loss: 1.0517430305480957\n",
      "Epoch 240, Loss: 1.051288366317749\n",
      "Epoch 260, Loss: 1.0508301258087158\n",
      "Epoch 280, Loss: 1.0503714084625244\n",
      "Epoch 300, Loss: 1.0499143600463867\n",
      "Epoch 320, Loss: 1.0494621992111206\n",
      "Epoch 340, Loss: 1.049017071723938\n",
      "Epoch 360, Loss: 1.0485817193984985\n",
      "Epoch 380, Loss: 1.0481584072113037\n",
      "Epoch 400, Loss: 1.0477497577667236\n",
      "Epoch 420, Loss: 1.0473577976226807\n",
      "Epoch 440, Loss: 1.0469849109649658\n",
      "Epoch 460, Loss: 1.0466331243515015\n",
      "Epoch 480, Loss: 1.0463050603866577\n",
      "weights of  P099 :  [array([[-0.64142865,  0.21823904],\n",
      "       [-0.73751366, -0.4233483 ],\n",
      "       [ 0.3654133 , -0.05547376],\n",
      "       [ 0.8511281 ,  0.44267693],\n",
      "       [-0.7302868 , -0.89413303]], dtype=float32), array([-0.10955182,  0.10955184], dtype=float32)]\n",
      "Lowest loss of  P099 :  1.0460169\n",
      "Epoch 0, Loss: 0.9821746349334717\n",
      "Epoch 20, Loss: 0.9082958102226257\n",
      "Epoch 40, Loss: 0.898753821849823\n",
      "Epoch 60, Loss: 0.8879930377006531\n",
      "Epoch 80, Loss: 0.8762200474739075\n",
      "Epoch 100, Loss: 0.8641549944877625\n",
      "Epoch 120, Loss: 0.8522215485572815\n",
      "Epoch 140, Loss: 0.8407862782478333\n",
      "Epoch 160, Loss: 0.8301250338554382\n",
      "Epoch 180, Loss: 0.8204379677772522\n",
      "Epoch 200, Loss: 0.8118593692779541\n",
      "Epoch 220, Loss: 0.8044683933258057\n",
      "Epoch 240, Loss: 0.798299252986908\n",
      "Epoch 260, Loss: 0.7933496832847595\n",
      "Epoch 280, Loss: 0.7895885705947876\n",
      "Epoch 300, Loss: 0.7869641780853271\n",
      "Epoch 320, Loss: 0.7854093909263611\n",
      "Epoch 340, Loss: 0.7848477959632874\n",
      "Epoch 360, Loss: 0.7851975560188293\n",
      "Epoch 380, Loss: 0.7863757014274597\n",
      "Epoch 400, Loss: 0.7882999777793884\n",
      "Epoch 420, Loss: 0.7908914089202881\n",
      "Epoch 440, Loss: 0.7940747737884521\n",
      "Epoch 460, Loss: 0.7977805137634277\n",
      "Epoch 480, Loss: 0.8019437789916992\n",
      "weights of  P100 :  [array([[-0.17794566, -0.38027844],\n",
      "       [-0.5418195 , -0.2830078 ],\n",
      "       [ 0.09771034, -0.08182319],\n",
      "       [ 0.03767408, -0.3317914 ],\n",
      "       [-0.31778768,  0.46732023]], dtype=float32), array([-0.25865352,  0.25865337], dtype=float32)]\n",
      "Lowest loss of  P100 :  0.78484297\n",
      "Epoch 0, Loss: 8.155656814575195\n",
      "Epoch 20, Loss: 2.9128358364105225\n",
      "Epoch 40, Loss: 1.1936496496200562\n",
      "Epoch 60, Loss: 0.9354340434074402\n",
      "Epoch 80, Loss: 0.8951216340065002\n",
      "Epoch 100, Loss: 0.8910180330276489\n",
      "Epoch 120, Loss: 0.8907983303070068\n",
      "Epoch 140, Loss: 0.8903953433036804\n",
      "Epoch 160, Loss: 0.8900537490844727\n",
      "Epoch 180, Loss: 0.8896982669830322\n",
      "Epoch 200, Loss: 0.8893264532089233\n",
      "Epoch 220, Loss: 0.8889420628547668\n",
      "Epoch 240, Loss: 0.8885468244552612\n",
      "Epoch 260, Loss: 0.8881427645683289\n",
      "Epoch 280, Loss: 0.8877315521240234\n",
      "Epoch 300, Loss: 0.8873145580291748\n",
      "Epoch 320, Loss: 0.8868938684463501\n",
      "Epoch 340, Loss: 0.8864708542823792\n",
      "Epoch 360, Loss: 0.8860471844673157\n",
      "Epoch 380, Loss: 0.8856244087219238\n",
      "Epoch 400, Loss: 0.8852038979530334\n",
      "Epoch 420, Loss: 0.8847872614860535\n",
      "Epoch 440, Loss: 0.884376049041748\n",
      "Epoch 460, Loss: 0.8839716911315918\n",
      "Epoch 480, Loss: 0.8835754990577698\n",
      "weights of  P101 :  [array([[-0.5088614 ,  0.41331002],\n",
      "       [ 0.20367432, -0.5967193 ],\n",
      "       [ 0.41417164,  0.3902301 ],\n",
      "       [ 0.19585907,  0.62321997],\n",
      "       [ 0.5545402 ,  0.11744174]], dtype=float32), array([-0.26433998,  0.26434   ], dtype=float32)]\n",
      "Lowest loss of  P101 :  0.8832081\n",
      "Epoch 0, Loss: 1.1047987937927246\n",
      "Epoch 20, Loss: 0.9891420602798462\n",
      "Epoch 40, Loss: 0.9777892827987671\n",
      "Epoch 60, Loss: 0.9732515811920166\n",
      "Epoch 80, Loss: 0.9683496356010437\n",
      "Epoch 100, Loss: 0.9633123874664307\n",
      "Epoch 120, Loss: 0.9583606719970703\n",
      "Epoch 140, Loss: 0.9537062048912048\n",
      "Epoch 160, Loss: 0.9495289325714111\n",
      "Epoch 180, Loss: 0.9459891319274902\n",
      "Epoch 200, Loss: 0.9432300329208374\n",
      "Epoch 220, Loss: 0.9413790106773376\n",
      "Epoch 240, Loss: 0.9405472278594971\n",
      "Epoch 260, Loss: 0.9408314228057861\n",
      "Epoch 280, Loss: 0.9423140287399292\n",
      "Epoch 300, Loss: 0.9450633525848389\n",
      "Epoch 320, Loss: 0.9491348266601562\n",
      "Epoch 340, Loss: 0.9545712471008301\n",
      "Epoch 360, Loss: 0.9614036083221436\n",
      "Epoch 380, Loss: 0.969651997089386\n",
      "Epoch 400, Loss: 0.9793258905410767\n",
      "Epoch 420, Loss: 0.9904251098632812\n",
      "Epoch 440, Loss: 1.0029410123825073\n",
      "Epoch 460, Loss: 1.01685631275177\n",
      "Epoch 480, Loss: 1.0321471691131592\n",
      "weights of  P102 :  [array([[ 0.30221772, -0.5713629 ],\n",
      "       [-0.9786813 , -0.51381755],\n",
      "       [-0.16100399, -0.67143613],\n",
      "       [ 0.0514289 ,  0.50139666],\n",
      "       [ 0.2489511 ,  0.4700765 ]], dtype=float32), array([-0.21409665,  0.21409662], dtype=float32)]\n",
      "Lowest loss of  P102 :  0.9405103\n",
      "Epoch 0, Loss: 7.71398401260376\n",
      "Epoch 20, Loss: 2.4398937225341797\n",
      "Epoch 40, Loss: 1.272817611694336\n",
      "Epoch 60, Loss: 1.0575677156448364\n",
      "Epoch 80, Loss: 1.0281442403793335\n",
      "Epoch 100, Loss: 1.0252916812896729\n",
      "Epoch 120, Loss: 1.0226964950561523\n",
      "Epoch 140, Loss: 1.019814372062683\n",
      "Epoch 160, Loss: 1.0168867111206055\n",
      "Epoch 180, Loss: 1.013845443725586\n",
      "Epoch 200, Loss: 1.0107041597366333\n",
      "Epoch 220, Loss: 1.0074849128723145\n",
      "Epoch 240, Loss: 1.0042084455490112\n",
      "Epoch 260, Loss: 1.0008933544158936\n",
      "Epoch 280, Loss: 0.9975566267967224\n",
      "Epoch 300, Loss: 0.9942141771316528\n",
      "Epoch 320, Loss: 0.9908813238143921\n",
      "Epoch 340, Loss: 0.9875719547271729\n",
      "Epoch 360, Loss: 0.984299898147583\n",
      "Epoch 380, Loss: 0.9810774326324463\n",
      "Epoch 400, Loss: 0.9779168367385864\n",
      "Epoch 420, Loss: 0.9748293161392212\n",
      "Epoch 440, Loss: 0.9718254804611206\n",
      "Epoch 460, Loss: 0.9689152836799622\n",
      "Epoch 480, Loss: 0.9661080837249756\n",
      "weights of  P103 :  [array([[-0.32515088,  0.29122725],\n",
      "       [-0.59853053,  0.5355511 ],\n",
      "       [-0.09831604,  0.35277846],\n",
      "       [-0.00247887, -0.8805498 ],\n",
      "       [ 1.006512  , -0.13169914]], dtype=float32), array([-0.23194705,  0.23194711], dtype=float32)]\n",
      "Lowest loss of  P103 :  0.9635445\n",
      "Epoch 0, Loss: 2.120527982711792\n",
      "Epoch 20, Loss: 1.0437248945236206\n",
      "Epoch 40, Loss: 1.0499067306518555\n",
      "Epoch 60, Loss: 1.0385816097259521\n",
      "Epoch 80, Loss: 1.0369864702224731\n",
      "Epoch 100, Loss: 1.0356366634368896\n",
      "Epoch 120, Loss: 1.034159541130066\n",
      "Epoch 140, Loss: 1.032571792602539\n",
      "Epoch 160, Loss: 1.0308879613876343\n",
      "Epoch 180, Loss: 1.0291205644607544\n",
      "Epoch 200, Loss: 1.027280569076538\n",
      "Epoch 220, Loss: 1.0253790616989136\n",
      "Epoch 240, Loss: 1.0234256982803345\n",
      "Epoch 260, Loss: 1.0214297771453857\n",
      "Epoch 280, Loss: 1.019399881362915\n",
      "Epoch 300, Loss: 1.0173447132110596\n",
      "Epoch 320, Loss: 1.0152719020843506\n",
      "Epoch 340, Loss: 1.013189435005188\n",
      "Epoch 360, Loss: 1.0111048221588135\n",
      "Epoch 380, Loss: 1.0090253353118896\n",
      "Epoch 400, Loss: 1.006958246231079\n",
      "Epoch 420, Loss: 1.0049102306365967\n",
      "Epoch 440, Loss: 1.0028884410858154\n",
      "Epoch 460, Loss: 1.000899076461792\n",
      "Epoch 480, Loss: 0.9989488124847412\n",
      "weights of  P104 :  [array([[-0.74057895, -0.23915938],\n",
      "       [ 0.7263585 , -0.88172406],\n",
      "       [-0.25885645,  0.618112  ],\n",
      "       [ 0.05522443,  0.5129249 ],\n",
      "       [-0.50886005, -0.4627077 ]], dtype=float32), array([-0.17530663,  0.17530656], dtype=float32)]\n",
      "Lowest loss of  P104 :  0.99713814\n",
      "Epoch 0, Loss: 3.080345392227173\n",
      "Epoch 20, Loss: 1.0648021697998047\n",
      "Epoch 40, Loss: 0.8564224243164062\n",
      "Epoch 60, Loss: 0.8463455438613892\n",
      "Epoch 80, Loss: 0.8443623781204224\n",
      "Epoch 100, Loss: 0.8437232375144958\n",
      "Epoch 120, Loss: 0.8430628776550293\n",
      "Epoch 140, Loss: 0.8424471616744995\n",
      "Epoch 160, Loss: 0.8418287038803101\n",
      "Epoch 180, Loss: 0.8412083983421326\n",
      "Epoch 200, Loss: 0.8405938744544983\n",
      "Epoch 220, Loss: 0.8399930596351624\n",
      "Epoch 240, Loss: 0.8394135236740112\n",
      "Epoch 260, Loss: 0.8388618230819702\n",
      "Epoch 280, Loss: 0.838344931602478\n",
      "Epoch 300, Loss: 0.837868869304657\n",
      "Epoch 320, Loss: 0.8374394178390503\n",
      "Epoch 340, Loss: 0.8370618224143982\n",
      "Epoch 360, Loss: 0.8367412090301514\n",
      "Epoch 380, Loss: 0.836482048034668\n",
      "Epoch 400, Loss: 0.8362888097763062\n",
      "Epoch 420, Loss: 0.8361653089523315\n",
      "Epoch 440, Loss: 0.8361148834228516\n",
      "Epoch 460, Loss: 0.8361409306526184\n",
      "Epoch 480, Loss: 0.8362464904785156\n",
      "weights of  P105 :  [array([[-0.62839067, -0.04767992],\n",
      "       [-0.07251105,  0.5946166 ],\n",
      "       [-0.28628835, -0.11881299],\n",
      "       [ 0.4272978 , -0.51322603],\n",
      "       [-0.2927701 , -0.46117657]], dtype=float32), array([ 0.04244833, -0.04244833], dtype=float32)]\n",
      "Lowest loss of  P105 :  0.83611387\n",
      "Epoch 0, Loss: 2.800917625427246\n",
      "Epoch 20, Loss: 1.1404356956481934\n",
      "Epoch 40, Loss: 1.0042781829833984\n",
      "Epoch 60, Loss: 1.0054436922073364\n",
      "Epoch 80, Loss: 1.0010162591934204\n",
      "Epoch 100, Loss: 0.9989645481109619\n",
      "Epoch 120, Loss: 0.9967755079269409\n",
      "Epoch 140, Loss: 0.9944285154342651\n",
      "Epoch 160, Loss: 0.9919537305831909\n",
      "Epoch 180, Loss: 0.989370584487915\n",
      "Epoch 200, Loss: 0.9866966009140015\n",
      "Epoch 220, Loss: 0.9839489459991455\n",
      "Epoch 240, Loss: 0.9811429381370544\n",
      "Epoch 260, Loss: 0.9782934188842773\n",
      "Epoch 280, Loss: 0.9754137992858887\n",
      "Epoch 300, Loss: 0.9725174903869629\n",
      "Epoch 320, Loss: 0.969616711139679\n",
      "Epoch 340, Loss: 0.9667229056358337\n",
      "Epoch 360, Loss: 0.963847279548645\n",
      "Epoch 380, Loss: 0.9610003232955933\n",
      "Epoch 400, Loss: 0.9581918716430664\n",
      "Epoch 420, Loss: 0.9554314613342285\n",
      "Epoch 440, Loss: 0.9527279138565063\n",
      "Epoch 460, Loss: 0.9500899314880371\n",
      "Epoch 480, Loss: 0.947525143623352\n",
      "weights of  P106 :  [array([[-0.3646472 ,  0.46593156],\n",
      "       [-0.9869616 , -0.5815445 ],\n",
      "       [-0.15103015, -0.13219556],\n",
      "       [ 0.5189968 , -0.10189246],\n",
      "       [ 0.65330434,  0.35982957]], dtype=float32), array([-0.18642774,  0.18642774], dtype=float32)]\n",
      "Lowest loss of  P106 :  0.9451635\n",
      "Epoch 0, Loss: 1.307408332824707\n",
      "Epoch 20, Loss: 0.9807922840118408\n",
      "Epoch 40, Loss: 0.9646376371383667\n",
      "Epoch 60, Loss: 0.9604461193084717\n",
      "Epoch 80, Loss: 0.9571520090103149\n",
      "Epoch 100, Loss: 0.9535310864448547\n",
      "Epoch 120, Loss: 0.9496249556541443\n",
      "Epoch 140, Loss: 0.9455560445785522\n",
      "Epoch 160, Loss: 0.941354513168335\n",
      "Epoch 180, Loss: 0.9370774626731873\n",
      "Epoch 200, Loss: 0.9327660202980042\n",
      "Epoch 220, Loss: 0.9284604787826538\n",
      "Epoch 240, Loss: 0.9241946339607239\n",
      "Epoch 260, Loss: 0.9199988842010498\n",
      "Epoch 280, Loss: 0.9158993363380432\n",
      "Epoch 300, Loss: 0.9119187593460083\n",
      "Epoch 320, Loss: 0.9080767631530762\n",
      "Epoch 340, Loss: 0.9043898582458496\n",
      "Epoch 360, Loss: 0.9008716344833374\n",
      "Epoch 380, Loss: 0.8975332975387573\n",
      "Epoch 400, Loss: 0.8943840265274048\n",
      "Epoch 420, Loss: 0.8914300203323364\n",
      "Epoch 440, Loss: 0.8886765241622925\n",
      "Epoch 460, Loss: 0.8861268162727356\n",
      "Epoch 480, Loss: 0.8837823867797852\n",
      "weights of  P107 :  [array([[ 0.30638108,  0.35866317],\n",
      "       [ 0.71665657,  0.25450698],\n",
      "       [ 0.6111094 ,  0.40209883],\n",
      "       [-0.26921743, -0.4113955 ],\n",
      "       [-0.5962202 ,  0.1873208 ]], dtype=float32), array([ 0.5476671 , -0.54766715], dtype=float32)]\n",
      "Lowest loss of  P107 :  0.8817459\n",
      "Epoch 0, Loss: 0.8887358903884888\n",
      "Epoch 20, Loss: 0.8839743733406067\n",
      "Epoch 40, Loss: 0.8748881220817566\n",
      "Epoch 60, Loss: 0.8703570365905762\n",
      "Epoch 80, Loss: 0.8708298206329346\n",
      "Epoch 100, Loss: 0.8774350881576538\n",
      "Epoch 120, Loss: 0.8906584978103638\n",
      "Epoch 140, Loss: 0.910582959651947\n",
      "Epoch 160, Loss: 0.9369800090789795\n",
      "Epoch 180, Loss: 0.969434380531311\n",
      "Epoch 200, Loss: 1.0074249505996704\n",
      "Epoch 220, Loss: 1.0503904819488525\n",
      "Epoch 240, Loss: 1.09776771068573\n",
      "Epoch 260, Loss: 1.1490179300308228\n",
      "Epoch 280, Loss: 1.203640103340149\n",
      "Epoch 300, Loss: 1.2611788511276245\n",
      "Epoch 320, Loss: 1.3212237358093262\n",
      "Epoch 340, Loss: 1.3834097385406494\n",
      "Epoch 360, Loss: 1.4474141597747803\n",
      "Epoch 380, Loss: 1.5129504203796387\n",
      "Epoch 400, Loss: 1.5797669887542725\n",
      "Epoch 420, Loss: 1.6476407051086426\n",
      "Epoch 440, Loss: 1.7163753509521484\n",
      "Epoch 460, Loss: 1.7857961654663086\n",
      "Epoch 480, Loss: 1.8557461500167847\n",
      "weights of  P108 :  [array([[ 0.22263657, -0.43109816],\n",
      "       [-0.790179  ,  0.0560847 ],\n",
      "       [ 0.15223509,  0.74128586],\n",
      "       [-0.40645507, -0.39483202],\n",
      "       [ 0.8133321 , -0.3038833 ]], dtype=float32), array([ 0.01218261, -0.01218263], dtype=float32)]\n",
      "Lowest loss of  P108 :  0.8698621\n",
      "Epoch 0, Loss: 5.861978530883789\n",
      "Epoch 20, Loss: 1.0810705423355103\n",
      "Epoch 40, Loss: 0.7947085499763489\n",
      "Epoch 60, Loss: 0.766207754611969\n",
      "Epoch 80, Loss: 0.7673973441123962\n",
      "Epoch 100, Loss: 0.7651396989822388\n",
      "Epoch 120, Loss: 0.7645471692085266\n",
      "Epoch 140, Loss: 0.7640127539634705\n",
      "Epoch 160, Loss: 0.7634683847427368\n",
      "Epoch 180, Loss: 0.7628934979438782\n",
      "Epoch 200, Loss: 0.7622906565666199\n",
      "Epoch 220, Loss: 0.7616624236106873\n",
      "Epoch 240, Loss: 0.7610111236572266\n",
      "Epoch 260, Loss: 0.760339081287384\n",
      "Epoch 280, Loss: 0.7596486210823059\n",
      "Epoch 300, Loss: 0.7589415311813354\n",
      "Epoch 320, Loss: 0.7582199573516846\n",
      "Epoch 340, Loss: 0.7574855089187622\n",
      "Epoch 360, Loss: 0.7567403316497803\n",
      "Epoch 380, Loss: 0.7559857368469238\n",
      "Epoch 400, Loss: 0.7552235722541809\n",
      "Epoch 420, Loss: 0.7544556856155396\n",
      "Epoch 440, Loss: 0.7536833882331848\n",
      "Epoch 460, Loss: 0.7529083490371704\n",
      "Epoch 480, Loss: 0.7521321177482605\n",
      "weights of  P109 :  [array([[-0.37680987,  0.12193692],\n",
      "       [ 0.20589128, -0.25481862],\n",
      "       [ 0.12579285, -0.32985017],\n",
      "       [-0.28622556,  0.24951112],\n",
      "       [ 0.11325438, -0.03090103]], dtype=float32), array([-0.25558665,  0.25558668], dtype=float32)]\n",
      "Lowest loss of  P109 :  0.75139505\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Custom loss function\n",
    "class CustomLossLL1(tf.keras.losses.Loss):\n",
    "    def __init__(self, lambda_t, model):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        ws = self.get_weights_from_model()\n",
    "        reg_term = self.regularization_term(ws)\n",
    "        return ce_loss + self.lambda_t * reg_term\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        # return tf.concat([tf.reshape(w, [-1]) for w in model_weights], axis=0)\n",
    "        return model_weights[0]\n",
    "\n",
    "    def regularization_term(self, ws):\n",
    "        reg_term = tf.pow(tf.norm(ws, ord='euclidean'),2)\n",
    "        return reg_term\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True) # Perform a forward pass and compute the predictions\n",
    "        loss = custom_loss(y, y_pred) # Compute the custom loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_weight_LL(X_train, y_train, lambd, num_tier=10, learning_rate = 0.01):\n",
    "    n_classes = np.unique(y_train).size\n",
    "    _, sequential_indices = np.unique(y_train, return_inverse=True)\n",
    "    y_one_hot = tf.keras.utils.to_categorical(sequential_indices, num_classes=n_classes)\n",
    "\n",
    "    lambda_t = lambd  # Regularization parameter\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_classes, input_shape=(X_train.shape[1],), activation='softmax')  # Adjust input_shape to match the number of features in X\n",
    "\n",
    "        # Dense(5, input_shape=(X_train.shape[1],), activation='relu'),  # Hidden layer with 5 neurons\n",
    "        # Dense(n_classes, activation='softmax')  # Output layer with 'n_classes' neurons \n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate)\n",
    "    custom_loss = CustomLossLL1(lambda_t, model)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = num_tier\n",
    "    lowest_loss = float('inf')\n",
    "    best_weights = None\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X_train, y_one_hot, custom_loss)\n",
    "        loss_value = loss.numpy()\n",
    "        if loss_value < lowest_loss:\n",
    "            lowest_loss = loss_value\n",
    "            best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    return best_weights[0], lowest_loss\n",
    "\n",
    "def build_clf_params(data, target_subjects ,condition, load_weight):\n",
    "\n",
    "    for sub in data.keys():\n",
    "        if (sub  != target_subjects) and  (sub != target_data_0): #Don't apply weight to target subject\n",
    "            # Where the tranining data is stored\n",
    "            if condition == \"noEA\":\n",
    "                X = data[sub]['Raw_csp']\n",
    "                y = data[sub]['Raw_csp_label']\n",
    "                store_ws = 'ws_Raw'\n",
    "                with open(noEA_wLTL_weight, 'rb') as file:\n",
    "                    loaded_weight = pickle.load(file)\n",
    "\n",
    "            else:\n",
    "                X = data[sub]['EA_csp']\n",
    "                y = data[sub]['EA_csp_label']\n",
    "                store_ws = 'ws_EA'\n",
    "                with open(EA_wLTL_weight, 'rb') as file:\n",
    "                    loaded_weight = pickle.load(file)\n",
    "\n",
    "            if load_weight == False:\n",
    "                weights, loss = train_weight_LL(X_train=X, y_train=y, lambd= 0.1, num_tier=500, learning_rate= 0.01)\n",
    "                print(\"weights of \", str(sub), \": \", weights)\n",
    "                print(\"Lowest loss of \", str(sub), \": \", loss)\n",
    "                data[sub][store_ws] = weights\n",
    "\n",
    "            else:\n",
    "                for sub in data.keys():\n",
    "                    if (sub  != target_subjects) and  (sub != target_data_0): #Don't apply weight to target subject\n",
    "                        data[sub][store_ws] = loaded_weight[sub][store_ws][0]\n",
    "\n",
    "build_clf_params(CSP2D_Epoch, target_subjects= target_data ,condition = condition_wLTL, load_weight = load_wLTL_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_19900\\1899423046.py:12: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  sigma_P = np.cov(P, rowvar=False)\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2705: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2705: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_19900\\1899423046.py:13: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  sigma_Q = np.cov(Q, rowvar=False)\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2139: RuntimeWarning: invalid value encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n"
     ]
    }
   ],
   "source": [
    "# First define the kl divergence\n",
    "def KL_div(P, Q):\n",
    "    # First convert to np array\n",
    "    P = np.array(P)\n",
    "    Q = np.array(Q)\n",
    "    \n",
    "    # Then compute their means, datain shape of samples x feat\n",
    "    mu_P = np.mean(P, axis=0)\n",
    "    mu_Q = np.mean(Q, axis=0)    \n",
    "\n",
    "    # Compute their covariance\n",
    "    sigma_P = np.cov(P, rowvar=False)\n",
    "    sigma_Q = np.cov(Q, rowvar=False)  \n",
    "\n",
    "    diff = mu_Q - mu_P\n",
    "\n",
    "    inv_sigma_Q = np.linalg.inv(sigma_Q)\n",
    "    term1 = np.dot(np.dot(diff.T, inv_sigma_Q), diff)\n",
    "    \n",
    "    # Calculate the trace term trace(Sigma_Q^{-1} * Sigma_P)\n",
    "    term2 = np.trace(np.dot(inv_sigma_Q, sigma_P))\n",
    "    \n",
    "    # Calculate the determinant term ln(det(Sigma_P) / det(Sigma_Q))\n",
    "    det_sigma0 = np.linalg.det(sigma_P)\n",
    "    det_sigma1 = np.linalg.det(sigma_Q)\n",
    "\n",
    "    \n",
    "    epsilon = 1e-10\n",
    "    term3 = np.log((det_sigma0+epsilon) / (det_sigma1+epsilon))\n",
    "    \n",
    "    # Dimensionality of the data\n",
    "    K = mu_P.shape[0]\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_div = 0.5 * (term1 + term2 - term3 - K)\n",
    "    \n",
    "    return kl_div\n",
    "\n",
    "# Compute kl divergence of target subject to each source subject\n",
    "def compute_all_kl_div(data, target_subjects , condition):\n",
    "    '''\n",
    "    Parameter:\n",
    "    data, is the whole data containing target and source data\n",
    "    '''\n",
    "    kl_div_score = []\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        target_data = 'Raw_csp'\n",
    "        label_name = 'Raw_csp_label'\n",
    "\n",
    "    else:\n",
    "        target_data = 'EA_csp'\n",
    "        label_name = 'EA_csp_label'\n",
    "        \n",
    "    # cal P from target data\n",
    "    label_tgt =  data[target_subjects][label_name]\n",
    "    P_left =  data[target_subjects][target_data][np.where(label_tgt == 0)]\n",
    "    P_right = data[target_subjects][target_data][np.where(label_tgt == 1)]\n",
    "    P_non = data[target_subjects][target_data][np.where(label_tgt == 2)]\n",
    "    P_feet = data[target_subjects][target_data][np.where(label_tgt == 3)]\n",
    "\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "\n",
    "    #cal Q from each source subject\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            label_src =  data[sub][label_name]\n",
    "            Q_left =  data[sub][target_data][np.where(label_src == 0)]\n",
    "            Q_right = data[sub][target_data][np.where(label_src == 1)]\n",
    "            Q_non = data[sub][target_data][np.where(label_src == 2)]\n",
    "            Q_feet = data[sub][target_data][np.where(label_src == 3)]\n",
    "\n",
    "            kl_left = KL_div(P_left, Q_left)\n",
    "            kl_right = KL_div(P_right, Q_right)\n",
    "            kl_non = KL_div(P_non, Q_non)\n",
    "            kl_feet = KL_div(P_feet, Q_feet)\n",
    "\n",
    "            kl_div_temp = [kl_left, kl_right, kl_non, kl_feet]\n",
    "\n",
    "            kl_div_score.append(kl_div_temp)\n",
    "\n",
    "    data[target_subjects]['kl_div'] = kl_div_score\n",
    "\n",
    "\n",
    "compute_all_kl_div(CSP2D_Epoch, target_subjects=target_data_0 ,condition = condition_wLTL) #target_sub for cal KL is calibrate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15054150878046194, 0.06295430435872192, 0.004077444782470156, 0.0709544524327804, 0.0952656825727091, 0.02168760965272801, 0.10156596317481874, 0.012435480752361424, 0.011350676984812085, 0.006695338812030759, 0.14738393976917732, 0.02218891378408069, 0.04015323659760349, 0.0338675518425011, 0.016730136522939844, 0.0015171961405512904, 0.016727880589464863, 0.12272040517203778, 0.025044553906975135, 0.1318671871876019, 0.0066309649521699935, 0.006333776534486731, 0.017371003506733325, 0.008939042009090093, 0.005545906875625695, 0.03830707805350393, 0.02497602637051444, 0.025011082114658273, 0.018104064269950215, 0.011161553721424772, 0.07781137341418752, 0.0064699440277837805, 0.014103104486715317, 0.11424079494553936, 0.006147930562565121, 0.02807881353294982, 0.010397345727317828, 0.0009021814373565424, 0.006127552697032221, 0.011907912563294177, 0.009306899865296241, 0.002997654598411705, 0.0004765527957801623, 0.0022257868492488848, 0.01803693058854877, 0.03760477756486508, 0.0010670480969610923, 0.01981278208187914, 0.02604706963299722, 0.04558191300676443, 0.003120944635483466, 0.004068795231785477, 0.004805864168911611, 0.025692617846521913, 0.06259347339066078, 0.04267361396288165, 0.009181189044988741, 0.011299903288637366, 0.007730159470371454, 0.04544436154351323, 0.006618279966152453, 0.0033092111249826786, 0.005326136218907818, 0.011638891893601419, 0.03099516273962502, 0.05215446616561882, 0.003428764656781033, 0.004261847233255819, 0.0092584248388509, 0.010360987530637205, 0.001801331990788086, 0.034440030109243444, 0.012101753701032088, 0.10050842286741553, 0.004600413671971319, 0.0035782563840203174, 0.0014375465484488313, 0.0313966772840066, 0.0326322262856351, 0.005429178261773214, 0.00013569989871457645, 0.2475245091992794, 0.027172554931499497, 0.009019765012818261, 0.0035115219150037343, 0.009073852914663404, 0.005942720823154314, 0.04114412730262437, 0.012926256533205635, 0.07045404873870345, 0.020239310453454246, 0.009395610616970734, 0.0037419566801174587, 0.0005068022788726422, 0.007532498053004947, 0.030672373240631174, 0.02530789554846356, 0.07624317147033978, 0.011773586322821931, 0.009506805754456539, 0.0009052073222372563, 0.0024948842889124874, 0.0033069983745316832, 0.006406899150988347, 0.02038475462491857, 0.28270670780318724, 0.009630314626745394, 0.032050713457690276, 0.022643915458270433, 0.021023109556503505, 0.02310328683628666, 0.011806546397296093, 0.020496012742550848, 0.01144516173949878, 0.009863431423098565, 0.027305027594267246, 0.002738662427304742, 0.43526964845026184]\n",
      "[0.04397796876478224, 0.08175465271755566, 0.006396988368009544, 0.05587403381475197, 0.044035234222169435, 0.03378080347450738, 0.04704468948440627, 0.020019147150101738, 0.012954109701322575, 0.022082839744054988, 0.018878636085131958, 0.025486989379206834, 0.012981529373063738, 0.06859806835689823, 0.01439583555830298, 0.0022903794130300316, 0.03419603068903185, 0.05406541887519143, 0.001956577171235512, 0.030852665626475536, 0.007923944932609858, 0.00750293179537149, 0.028237357945681902, 0.015377402712439843, 0.0029755756458711507, 0.020747805409595563, 0.006519732080557465, 0.0012781711705857966, 0.01930192855730581, 0.003777011620895286, 0.0009429083885379672, 0.004929539341091862, 0.0068530923132247915, 0.012344130614501608, 0.004669010413878262, 0.012149228724055479, 0.009135238440077919, 0.00156285529394378, 0.005360308935542764, 0.006572136758776309, 0.03429843949176683, 0.002508187810669172, 0.00025654602505162177, 0.003542776714269432, 0.034427541863323925, 0.02346493577449276, 0.013814404137573505, 0.016893585413434498, 0.035217238042351065, 0.016218449841636385, 0.005510019562192018, 0.0018711735794935255, 0.007758880809981641, 0.01655590903199186, 0.0023460424166115595, 0.027031090777544845, 0.005502180521616346, 0.008180491841219046, 0.06539671303247935, 0.06826522683084078, 0.004064346538453722, 0.01433296742638077, 0.005428778980469175, 0.016412291281352284, 0.01246912115967961, 0.009573299305152914, 0.0012892342098800988, 0.013649349256496565, 0.002716643634797475, 0.0013973275183221938, 0.0007036112035748193, 0.022346641762794845, 0.015021803648001028, 0.045324115178471416, 0.007036295455488749, 0.0008014847926437727, 0.01811140243263013, 0.013816882263236742, 0.009578718764528578, 0.005223587310678573, 0.0029685766770303056, 0.013136944260207898, 0.011579774244645832, 0.008619089001397053, 0.007290914512775383, 0.03148022948123405, 0.005624426822223055, 0.011546320329064509, 0.009280614823437436, 0.007429088367235249, 0.011426713388139867, 0.015740122244234215, 0.0027068639526686474, 0.001926644972270915, 6.906917567047235e-05, 0.021524396873354727, 0.009044244251445153, 0.015880506642290022, 0.016526923209322714, 0.0141285102964942, 0.0014254607892524385, 0.00712709251709694, 0.006322154124642165, 0.040466717611277854, 0.0035249919138667612, 0.03599078694469804, 0.0028799024135194723, 0.0186317678685047, 0.008472240874463799, 0.008591201218916718, 0.023381733677579876, 0.005790302179647826, 0.14778548800912383, 0.011639598397864541, 0.01689372532013511, 0.007884816711017581, 0.0050923891679280275, 0.1263016141513349]\n",
      "[[3.91550878e-02 2.08005196e-02]\n",
      " [1.63740973e-02 3.86679810e-02]\n",
      " [1.06052284e-03 3.02562137e-03]\n",
      " [1.84548955e-02 2.64270718e-02]\n",
      " [2.47781240e-02 2.08276048e-02]\n",
      " [5.64083798e-03 1.59775061e-02]\n",
      " [2.64167952e-02 2.22510046e-02]\n",
      " [3.23440588e-03 9.46857427e-03]\n",
      " [2.95225389e-03 6.12698178e-03]\n",
      " [1.74142389e-03 1.04446511e-02]\n",
      " [3.83338200e-02 8.92914001e-03]\n",
      " [5.77122466e-03 1.20547319e-02]\n",
      " [1.04436545e-02 6.13995062e-03]\n",
      " [8.80877957e-03 3.24452335e-02]\n",
      " [4.35142420e-03 6.80888337e-03]\n",
      " [3.94615071e-04 1.08329428e-03]\n",
      " [4.35083745e-03 1.61738986e-02]\n",
      " [3.19189590e-02 2.55716405e-02]\n",
      " [6.51396227e-03 9.25413861e-04]\n",
      " [3.42979909e-02 1.45925675e-02]\n",
      " [1.72468057e-03 3.74783504e-03]\n",
      " [1.64738336e-03 3.54870597e-03]\n",
      " [4.51811048e-03 1.33555900e-02]\n",
      " [2.32499978e-03 7.27314098e-03]\n",
      " [1.44246243e-03 1.40737559e-03]\n",
      " [9.96347797e-03 9.81321207e-03]\n",
      " [6.49613860e-03 3.08367619e-03]\n",
      " [6.50525642e-03 6.04544168e-04]\n",
      " [4.70877589e-03 9.12934715e-03]\n",
      " [2.90306388e-03 1.78643549e-03]\n",
      " [2.02383461e-02 4.45972950e-04]\n",
      " [1.68279984e-03 2.33155334e-03]\n",
      " [3.66814641e-03 3.24134755e-03]\n",
      " [2.97134551e-02 5.83847636e-03]\n",
      " [1.59904576e-03 2.20832943e-03]\n",
      " [7.30315791e-03 5.74629247e-03]\n",
      " [2.70429723e-03 4.32074768e-03]\n",
      " [2.34652846e-04 7.39192899e-04]\n",
      " [1.59374558e-03 2.53529698e-03]\n",
      " [3.09718805e-03 3.10846234e-03]\n",
      " [2.42067776e-03 1.62223355e-02]\n",
      " [7.79674855e-04 1.18631240e-03]\n",
      " [1.23948981e-04 1.21340089e-04]\n",
      " [5.78915943e-04 1.67564803e-03]\n",
      " [4.69131476e-03 1.62833978e-02]\n",
      " [9.78081315e-03 1.10983493e-02]\n",
      " [2.77533833e-04 6.53388032e-03]\n",
      " [5.15320478e-03 7.99025887e-03]\n",
      " [6.77471156e-03 1.66569051e-02]\n",
      " [1.18556259e-02 7.67093601e-03]\n",
      " [8.11741973e-04 2.60610650e-03]\n",
      " [1.05827314e-03 8.85020019e-04]\n",
      " [1.24998105e-03 3.66976368e-03]\n",
      " [6.68252043e-03 7.83054608e-03]\n",
      " [1.62802470e-02 1.10962154e-03]\n",
      " [1.10991920e-02 1.27850547e-02]\n",
      " [2.38798101e-03 2.60239882e-03]\n",
      " [2.93904790e-03 3.86917554e-03]\n",
      " [2.01057552e-03 3.09310696e-02]\n",
      " [1.18198494e-02 3.22878075e-02]\n",
      " [1.72138127e-03 1.92233799e-03]\n",
      " [8.60709138e-04 6.77914826e-03]\n",
      " [1.38530119e-03 2.56768166e-03]\n",
      " [3.02721713e-03 7.76261834e-03]\n",
      " [8.06168564e-03 5.89759388e-03]\n",
      " [1.35651138e-02 4.52793991e-03]\n",
      " [8.91804409e-04 6.09776719e-04]\n",
      " [1.10848499e-03 6.45581333e-03]\n",
      " [2.40806965e-03 1.28490699e-03]\n",
      " [2.69484065e-03 6.60902249e-04]\n",
      " [4.68517374e-04 3.32791147e-04]\n",
      " [8.95767828e-03 1.05694231e-02]\n",
      " [3.14760516e-03 7.10495119e-03]\n",
      " [2.61417343e-02 2.14372145e-02]\n",
      " [1.19654441e-03 3.32799822e-03]\n",
      " [9.30686454e-04 3.79082996e-04]\n",
      " [3.73898613e-04 8.56625698e-03]\n",
      " [8.16611755e-03 6.53505241e-03]\n",
      " [8.48747762e-03 4.53050319e-03]\n",
      " [1.41210191e-03 2.47063094e-03]\n",
      " [3.52948598e-05 1.40406524e-03]\n",
      " [6.43798775e-02 6.21345812e-03]\n",
      " [7.06744461e-03 5.47695422e-03]\n",
      " [2.34599543e-03 4.07662143e-03]\n",
      " [9.13329155e-04 3.44842690e-03]\n",
      " [2.36006342e-03 1.48893901e-02]\n",
      " [1.54567174e-03 2.66021837e-03]\n",
      " [1.07013802e-02 5.46113132e-03]\n",
      " [3.36205419e-03 4.38950721e-03]\n",
      " [1.83247430e-02 3.51377980e-03]\n",
      " [5.26414266e-03 5.40456012e-03]\n",
      " [2.44375098e-03 7.44469858e-03]\n",
      " [9.73264076e-04 1.28028143e-03]\n",
      " [1.31816719e-04 9.11256652e-04]\n",
      " [1.95916479e-03 3.26680560e-05]\n",
      " [7.97772972e-03 1.01805211e-02]\n",
      " [6.58245610e-03 4.27770962e-03]\n",
      " [1.98304647e-02 7.51109702e-03]\n",
      " [3.06225047e-03 7.81683648e-03]\n",
      " [2.47267227e-03 6.68244496e-03]\n",
      " [2.35439863e-04 6.74208609e-04]\n",
      " [6.48906831e-04 3.37094304e-03]\n",
      " [8.60133613e-04 2.99022658e-03]\n",
      " [1.66640218e-03 1.91397824e-02]\n",
      " [5.30197196e-03 1.66723624e-03]\n",
      " [7.35305901e-02 1.70227751e-02]\n",
      " [2.50479631e-03 1.36212445e-03]\n",
      " [8.33622906e-03 8.81237729e-03]\n",
      " [5.88956830e-03 4.00716581e-03]\n",
      " [5.46800486e-03 4.06343118e-03]\n",
      " [6.00904849e-03 1.10589967e-02]\n",
      " [3.07082322e-03 2.73867342e-03]\n",
      " [5.33090963e-03 6.98989751e-02]\n",
      " [2.97682889e-03 5.50524960e-03]\n",
      " [2.56542881e-03 7.99032504e-03]\n",
      " [7.10190008e-03 3.72932833e-03]\n",
      " [7.12312297e-04 2.40857738e-03]\n",
      " [1.13211442e-01 5.97376204e-02]]\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_weights(data, target_subjects):\n",
    "    kl = data[target_subjects]['kl_div']\n",
    "    KL_inv_left = []\n",
    "    KL_inv_right = []\n",
    "    KL_inv_non = []\n",
    "    KL_inv_feet = []\n",
    "\n",
    "    alpha_s = []\n",
    "    eps = 0.0001\n",
    "    \n",
    "    #equation (9)\n",
    "    for val in kl:\n",
    "        if val != 0: \n",
    "            KL_inv_left.append(1/((val[0] + eps)**4))\n",
    "            KL_inv_right.append(1/((val[1] + eps)**4))\n",
    "            KL_inv_non.append(1/((val[2] + eps)**4))\n",
    "            KL_inv_feet.append(1/((val[3] + eps)**4))\n",
    "\n",
    "    print(KL_inv_left)\n",
    "    print(KL_inv_right)\n",
    "    \n",
    "    for i in range(0,len(KL_inv_left)):\n",
    "        temp = [KL_inv_left[i]/sum(KL_inv_left), KL_inv_right[i]/sum(KL_inv_right), KL_inv_non[i]/sum(KL_inv_non), KL_inv_feet[i]/sum(KL_inv_feet)]\n",
    "        alpha_s.append(temp)\n",
    "\n",
    "    alpha_s = np.array(alpha_s)\n",
    "    print(np.array(alpha_s[:, ~np.isnan(alpha_s).any(axis=0)]))\n",
    "    data[target_subjects]['alpha_s'] = alpha_s[:, ~np.isnan(alpha_s).any(axis=0)]\n",
    "\n",
    "compute_similarity_weights(CSP2D_Epoch, target_subjects=target_data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0214109  -0.09508462]\n",
      " [-0.08722753  0.05763406]\n",
      " [ 0.08301245  0.13565849]\n",
      " [ 0.02977478  0.03381524]\n",
      " [ 0.0301102  -0.02841677]]\n",
      "[[ 1.10986598 -0.41997202 -1.70316629 -0.44666291  0.23981097]\n",
      " [-0.41997202  1.28137057  0.06479524 -0.07786899 -0.49551233]\n",
      " [-1.70316629  0.06479524  2.94347917  0.81764978 -0.15920778]\n",
      " [-0.44666291 -0.07786899  0.81764978  0.24395005 -0.01220101]\n",
      " [ 0.23981097 -0.49551233 -0.15920778 -0.01220101  0.20821075]]\n"
     ]
    }
   ],
   "source": [
    "def compute_ETL_and_mu_ws(data, target_subjects, condition):\n",
    "\n",
    "    mu_ws = 0\n",
    "    temp_ws = 0\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        ws_name = 'ws_Raw'\n",
    "    else:\n",
    "        ws_name = 'ws_EA'\n",
    "\n",
    "    alpha_s = np.array(data[target_subjects]['alpha_s'])\n",
    "\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "    index_count = 0\n",
    "\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            ws = data[sub][ws_name][0]\n",
    "            mu_ws += ws * alpha_s[index_count]\n",
    "            index_count += 1\n",
    "\n",
    "    print(np.array(mu_ws))\n",
    "\n",
    "    index_count = 0\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            ws = data[sub][ws_name][0]\n",
    "            ws_min_mu = np.dot(((ws * alpha_s[index_count]) - mu_ws), np.transpose((ws * alpha_s[index_count]) - mu_ws))\n",
    "            temp_ws += ws_min_mu #equation (11)\n",
    "            index_count += 1\n",
    "\n",
    "    print(np.array(temp_ws))\n",
    "\n",
    "    den = temp_ws\n",
    "    nom = np.trace(temp_ws) #Return the sum along diagonals of the array.\n",
    "    Sigma_TL = den/nom\n",
    "\n",
    "    data[target_subjects]['Sigma_TL'] = Sigma_TL\n",
    "    data[target_subjects]['mu_ws'] = mu_ws\n",
    "\n",
    "compute_ETL_and_mu_ws(CSP2D_Epoch, target_subjects = target_data_0, condition=condition_wLTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(5, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(CSP2D_Epoch[target_data_0]['Sigma_TL']).shape)\n",
    "print(np.array(CSP2D_Epoch[target_data_0]['mu_ws']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 7.879234313964844\n",
      "Epoch 20, Loss: 6.846125602722168\n",
      "Epoch 40, Loss: 6.75704288482666\n",
      "Epoch 60, Loss: 6.7467522621154785\n",
      "Epoch 80, Loss: 6.736814022064209\n",
      "Epoch 100, Loss: 6.729356288909912\n",
      "Epoch 120, Loss: 6.72113561630249\n",
      "Epoch 140, Loss: 6.7120208740234375\n",
      "Epoch 160, Loss: 6.702234745025635\n",
      "Epoch 180, Loss: 6.691806793212891\n",
      "Epoch 200, Loss: 6.680763244628906\n",
      "Epoch 220, Loss: 6.669142246246338\n",
      "Epoch 240, Loss: 6.656959533691406\n",
      "Epoch 260, Loss: 6.6442461013793945\n",
      "Epoch 280, Loss: 6.6310224533081055\n",
      "Epoch 300, Loss: 6.617306709289551\n",
      "Epoch 320, Loss: 6.60312032699585\n",
      "Epoch 340, Loss: 6.588478088378906\n",
      "Epoch 360, Loss: 6.5733962059021\n",
      "Epoch 380, Loss: 6.557884216308594\n",
      "Epoch 400, Loss: 6.541955471038818\n",
      "Epoch 420, Loss: 6.525628089904785\n",
      "Epoch 440, Loss: 6.508910179138184\n",
      "Epoch 460, Loss: 6.491808891296387\n",
      "Epoch 480, Loss: 6.474348545074463\n",
      "Epoch 500, Loss: 6.456530570983887\n",
      "Epoch 520, Loss: 6.4383697509765625\n",
      "Epoch 540, Loss: 6.419868469238281\n",
      "Epoch 560, Loss: 6.401043891906738\n",
      "Epoch 580, Loss: 6.381902694702148\n",
      "Epoch 600, Loss: 6.362453460693359\n",
      "Epoch 620, Loss: 6.342703819274902\n",
      "Epoch 640, Loss: 6.322665214538574\n",
      "Epoch 660, Loss: 6.302346229553223\n",
      "Epoch 680, Loss: 6.281749725341797\n",
      "Epoch 700, Loss: 6.26089334487915\n",
      "Epoch 720, Loss: 6.239772319793701\n",
      "Epoch 740, Loss: 6.218405723571777\n",
      "Epoch 760, Loss: 6.196800231933594\n",
      "Epoch 780, Loss: 6.17495584487915\n",
      "Epoch 800, Loss: 6.1528825759887695\n",
      "Epoch 820, Loss: 6.130593776702881\n",
      "Epoch 840, Loss: 6.108088493347168\n",
      "Epoch 860, Loss: 6.085378170013428\n",
      "Epoch 880, Loss: 6.062466621398926\n",
      "Epoch 900, Loss: 6.039362907409668\n",
      "Epoch 920, Loss: 6.016071796417236\n",
      "Epoch 940, Loss: 5.992600440979004\n",
      "Epoch 960, Loss: 5.968955039978027\n",
      "Epoch 980, Loss: 5.9451446533203125\n",
      "Epoch 1000, Loss: 5.921172142028809\n",
      "Epoch 1020, Loss: 5.897045135498047\n",
      "Epoch 1040, Loss: 5.872767448425293\n",
      "Epoch 1060, Loss: 5.8483476638793945\n",
      "Epoch 1080, Loss: 5.823787689208984\n",
      "Epoch 1100, Loss: 5.799098491668701\n",
      "Epoch 1120, Loss: 5.774279594421387\n",
      "Epoch 1140, Loss: 5.749338626861572\n",
      "Epoch 1160, Loss: 5.72428035736084\n",
      "Epoch 1180, Loss: 5.699110507965088\n",
      "Epoch 1200, Loss: 5.673832893371582\n",
      "Epoch 1220, Loss: 5.648449897766113\n",
      "Epoch 1240, Loss: 5.6229705810546875\n",
      "Epoch 1260, Loss: 5.59739351272583\n",
      "Epoch 1280, Loss: 5.571730136871338\n",
      "Epoch 1300, Loss: 5.54598331451416\n",
      "Epoch 1320, Loss: 5.520155906677246\n",
      "Epoch 1340, Loss: 5.494251728057861\n",
      "Epoch 1360, Loss: 5.468270778656006\n",
      "Epoch 1380, Loss: 5.442216396331787\n",
      "Epoch 1400, Loss: 5.416094779968262\n",
      "Epoch 1420, Loss: 5.389909267425537\n",
      "Epoch 1440, Loss: 5.363664150238037\n",
      "Epoch 1460, Loss: 5.337355613708496\n",
      "Epoch 1480, Loss: 5.310986518859863\n",
      "Epoch 1500, Loss: 5.284566879272461\n",
      "Epoch 1520, Loss: 5.258098602294922\n",
      "Epoch 1540, Loss: 5.231578826904297\n",
      "Epoch 1560, Loss: 5.205014228820801\n",
      "Epoch 1580, Loss: 5.178399085998535\n",
      "Epoch 1600, Loss: 5.151738166809082\n",
      "Epoch 1620, Loss: 5.125033378601074\n",
      "Epoch 1640, Loss: 5.098283290863037\n",
      "Epoch 1660, Loss: 5.071490287780762\n",
      "Epoch 1680, Loss: 5.044652462005615\n",
      "Epoch 1700, Loss: 5.017775058746338\n",
      "Epoch 1720, Loss: 4.990853309631348\n",
      "Epoch 1740, Loss: 4.963885307312012\n",
      "Epoch 1760, Loss: 4.9368767738342285\n",
      "Epoch 1780, Loss: 4.909821510314941\n",
      "Epoch 1800, Loss: 4.882721900939941\n",
      "Epoch 1820, Loss: 4.855572700500488\n",
      "Epoch 1840, Loss: 4.828377723693848\n",
      "Epoch 1860, Loss: 4.801130294799805\n",
      "Epoch 1880, Loss: 4.773830413818359\n",
      "Epoch 1900, Loss: 4.746477127075195\n",
      "Epoch 1920, Loss: 4.7190656661987305\n",
      "Epoch 1940, Loss: 4.691596031188965\n",
      "Epoch 1960, Loss: 4.664061546325684\n",
      "Epoch 1980, Loss: 4.63646125793457\n",
      "weights of  AJpang :  [array([[ 0.3536177 ,  0.58429056],\n",
      "       [ 0.08335624, -0.3256015 ],\n",
      "       [-0.08088038,  0.77065665],\n",
      "       [ 0.17483677,  0.48406118],\n",
      "       [ 0.22197077, -0.75240827]], dtype=float32), array([-0.26943842,  0.269438  ], dtype=float32)]\n",
      "loss of  AJpang :  4.610177\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.50      0.53        10\n",
      "           1       0.55      0.60      0.57        10\n",
      "\n",
      "    accuracy                           0.55        20\n",
      "   macro avg       0.55      0.55      0.55        20\n",
      "weighted avg       0.55      0.55      0.55        20\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[5 5]\n",
      " [4 6]]\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.40      0.41        20\n",
      "           1       0.43      0.45      0.44        20\n",
      "\n",
      "    accuracy                           0.42        40\n",
      "   macro avg       0.42      0.43      0.42        40\n",
      "weighted avg       0.42      0.42      0.42        40\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[ 8 12]\n",
      " [11  9]]\n"
     ]
    }
   ],
   "source": [
    "# Custom loss function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "class CustomLossLL2(tf.keras.losses.Loss):\n",
    "    \n",
    "    def __init__(self, lambda_t, model, mu, sigma_TL):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "        self.mu = tf.convert_to_tensor(mu, dtype=tf.float32)\n",
    "        self.sigma_TL = tf.convert_to_tensor(sigma_TL, dtype=tf.float32)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        wt = self.get_weights_from_model()\n",
    "        reg_term = self.regularization_term(wt)\n",
    "\n",
    "        return ce_loss + (self.lambda_t * tf.linalg.matmul(reg_term, wt))\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        return model_weights[0]\n",
    "\n",
    "    def regularization_term(self, wt):\n",
    "        diff = wt - self.mu\n",
    "        reg_term = 0.5 * tf.linalg.matmul(tf.linalg.matmul(tf.linalg.inv(self.sigma_TL), diff), tf.transpose(diff))\n",
    "        reg_term += 0.5 * tf.math.log(tf.linalg.det(self.sigma_TL))\n",
    "        return reg_term\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True) # Perform a forward pass and compute the predictions\n",
    "        loss = custom_loss(y, y_pred) # Compute the custom loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def train_weight_LL2(X_train, y_train, lambd, mu, sigma_TL, num_tier=10, learning_rate = 0.01):\n",
    "    n_classes = np.unique(y_train).size\n",
    "    _, sequential_indices = np.unique(y_train, return_inverse=True)\n",
    "    y_one_hot = tf.keras.utils.to_categorical(sequential_indices, num_classes=n_classes)\n",
    "\n",
    "    lambda_t = lambd  # Regularization parameter\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_classes, input_shape=(X_train.shape[1],), activation='softmax')  # Adjust input_shape to match the number of features in X\n",
    "\n",
    "        # Dense(5, input_shape=(X_train.shape[1],), activation='relu'),  # Hidden layer with 5 neurons\n",
    "        # Dense(n_classes, activation='softmax')  # Output layer with 'n_classes' neurons \n",
    "\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate)\n",
    "    custom_loss = CustomLossLL2(lambda_t, model, mu, sigma_TL)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = num_tier\n",
    "    lowest_loss = float('inf')\n",
    "    best_weights = None\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X_train, y_one_hot, custom_loss)\n",
    "        loss_value = loss.numpy()\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        if (abs(loss_value) < lowest_loss):\n",
    "            lowest_loss = abs(loss_value)\n",
    "            best_model = model\n",
    "            best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "    return best_model, best_weights[0], lowest_loss\n",
    "\n",
    "def GetConfusionMatrix(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_prob = model.predict(X_train)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    _, y_train = np.unique(y_train, return_inverse=True)\n",
    "\n",
    "\n",
    "    print(\"Classification TRAIN DATA \\n=======================\")\n",
    "    print(classification_report(y_true= y_train, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true= y_train, y_pred=y_pred))\n",
    "\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    _, y_test = np.unique(y_test, return_inverse=True)\n",
    "    \n",
    "    print(\"Classification TEST DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "\n",
    "def tgt_test_wLTL(data, target_subjects ,condition):\n",
    "        tgt_data = target_subjects + \"_test\"\n",
    "\n",
    "        if condition == \"noEA\":\n",
    "            X = data[target_subjects]['Raw_csp']\n",
    "            y = data[target_subjects]['Raw_csp_label']\n",
    "            X_test = data[tgt_data]['Raw_csp']\n",
    "            y_test = data[tgt_data]['Raw_csp_label']\n",
    "            store_ws = 'wt_Raw'\n",
    "\n",
    "        else:\n",
    "            X = data[target_subjects]['EA_csp']\n",
    "            y = data[target_subjects]['EA_csp_label']\n",
    "            X_test = data[tgt_data]['EA_csp']\n",
    "            y_test = data[tgt_data]['EA_csp_label']\n",
    "            store_ws = 'wt_EA'\n",
    "\n",
    "        mu = data[target_subjects]['mu_ws']\n",
    "        sigma_TL = data[target_subjects]['Sigma_TL']\n",
    "\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "        \n",
    "        model, weights, loss = train_weight_LL2(X_train=X_train, y_train=y_train, mu =mu, sigma_TL=sigma_TL, lambd= 0.1, num_tier=2000, learning_rate= 0.005)\n",
    "        print(\"weights of \", str(target_subjects), \": \", weights)\n",
    "        print(\"loss of \", str(target_subjects), \": \", loss)\n",
    "        data[target_subjects][store_ws] = weights\n",
    "\n",
    "        GetConfusionMatrix(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "tgt_test_wLTL(CSP2D_Epoch, target_subjects= target_data_0 ,condition = condition_wLTL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noEA + LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.58      0.61      0.60      3113\n",
      "       Right       0.59      0.55      0.57      3072\n",
      "\n",
      "    accuracy                           0.58      6185\n",
      "   macro avg       0.58      0.58      0.58      6185\n",
      "weighted avg       0.58      0.58      0.58      6185\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[1907 1206]\n",
      " [1370 1702]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.50      1.00      0.67        20\n",
      "       Right       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.50        40\n",
      "   macro avg       0.25      0.50      0.33        40\n",
      "weighted avg       0.25      0.50      0.33        40\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[20  0]\n",
      " [20  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "AllBCIClass.classifyCSP_LDA(combined_epochs, target_subjects= target_data, calibrate_data= target_data_0, condition = \"noEA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noEA + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Best parameters: {'C': 1, 'kernel': 'rbf'}\n",
      "Best cross-validation score: 0.587\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.58      0.70      0.64      3113\n",
      "       Right       0.62      0.50      0.55      3072\n",
      "\n",
      "    accuracy                           0.60      6185\n",
      "   macro avg       0.60      0.60      0.59      6185\n",
      "weighted avg       0.60      0.60      0.59      6185\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[2166  947]\n",
      " [1543 1529]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.50      1.00      0.67        20\n",
      "       Right       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.50        40\n",
      "   macro avg       0.25      0.50      0.33        40\n",
      "weighted avg       0.25      0.50      0.33        40\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[20  0]\n",
      " [20  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "if train_svm == True:\n",
    "    AllBCIClass.classifyCSP_SVM(combined_epochs, target_subjects= target_data, calibrate_data= target_data_0,condition = \"noEA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EA + LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.63      0.64      0.64      3113\n",
      "       Right       0.63      0.62      0.63      3072\n",
      "\n",
      "    accuracy                           0.63      6185\n",
      "   macro avg       0.63      0.63      0.63      6185\n",
      "weighted avg       0.63      0.63      0.63      6185\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[1996 1117]\n",
      " [1160 1912]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.67      0.60      0.63        20\n",
      "       Right       0.64      0.70      0.67        20\n",
      "\n",
      "    accuracy                           0.65        40\n",
      "   macro avg       0.65      0.65      0.65        40\n",
      "weighted avg       0.65      0.65      0.65        40\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[12  8]\n",
      " [ 6 14]]\n"
     ]
    }
   ],
   "source": [
    "AllBCIClass.classifyCSP_LDA(combined_epochs, target_subjects= target_data, calibrate_data= target_data_0, condition = \"EA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EA + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Best parameters: {'C': 1, 'kernel': 'linear'}\n",
      "Best cross-validation score: 0.633\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.63      0.65      0.64      3113\n",
      "       Right       0.63      0.61      0.62      3072\n",
      "\n",
      "    accuracy                           0.63      6185\n",
      "   macro avg       0.63      0.63      0.63      6185\n",
      "weighted avg       0.63      0.63      0.63      6185\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[2027 1086]\n",
      " [1186 1886]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.68      0.65      0.67        20\n",
      "       Right       0.67      0.70      0.68        20\n",
      "\n",
      "    accuracy                           0.68        40\n",
      "   macro avg       0.68      0.68      0.67        40\n",
      "weighted avg       0.68      0.68      0.67        40\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[13  7]\n",
      " [ 6 14]]\n"
     ]
    }
   ],
   "source": [
    "if train_svm == True:\n",
    "    AllBCIClass.classifyCSP_SVM(combined_epochs, target_subjects= target_data, calibrate_data= target_data_0,condition = \"EA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save wLTL weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del CSP2D_Epoch[target_data]\n",
    "# del CSP2D_Epoch[target_data_0]\n",
    "\n",
    "# # Save the dictionary\n",
    "# with open('noEA_wLTL_LR.pkl', 'wb') as file:\n",
    "#     pickle.dump(CSP2D_Epoch, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
