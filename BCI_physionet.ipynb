{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physionet Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://archive.physionet.org/pn4/eegmmidb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the experimental runs were:\n",
    "(list of number according by folder)\n",
    "\n",
    "- 1. Baseline, eyes open\n",
    "- 2. Baseline, eyes closed\n",
    "- 3. Task 1 (open and close left or right fist)\n",
    "- 4. Task 2 (imagine opening and closing left or right fist)\n",
    "- 5. Task 3 (open and close both fists or both feet)\n",
    "- 6. Task 4 (imagine opening and closing both fists or both feet)\n",
    "- 7. Task 1\n",
    "- 8. Task 2\n",
    "- 9. Task 3\n",
    "- 10. Task 4\n",
    "- 11. Task 1\n",
    "- 12. Task 2\n",
    "- 13. Task 3\n",
    "- 14. Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from mne.datasets import eegbci\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from mne.channels import make_standard_montage\n",
    "from scipy import signal\n",
    "from scipy.linalg import sqrtm, inv \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.utils import shuffle\n",
    "from mne.decoding import CSP\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit,StratifiedKFold ,cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pyriemann.utils.distance import distance_riemann\n",
    "from scipy.linalg import logm, expm\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "target_class = [\"Left\", \"Right\", \"Non\"] \n",
    "target_data_0 = \"P007\"\n",
    "calibrate_size = 30\n",
    "alignmentMethod = \"EA\"\n",
    "\n",
    "con_for_cnn = \"EA\"\n",
    "train_svm = False\n",
    "\n",
    "condition_wLTL = \"EA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing subject number:  1\n",
      "processing subject number:  2\n",
      "processing subject number:  3\n",
      "processing subject number:  4\n",
      "processing subject number:  5\n",
      "processing subject number:  6\n",
      "processing subject number:  7\n",
      "processing subject number:  8\n",
      "processing subject number:  9\n",
      "processing subject number:  10\n",
      "processing subject number:  11\n",
      "processing subject number:  12\n",
      "processing subject number:  13\n",
      "processing subject number:  14\n",
      "processing subject number:  15\n",
      "processing subject number:  16\n",
      "processing subject number:  17\n",
      "processing subject number:  18\n",
      "processing subject number:  19\n",
      "processing subject number:  20\n",
      "processing subject number:  21\n",
      "processing subject number:  22\n",
      "processing subject number:  23\n",
      "processing subject number:  24\n",
      "processing subject number:  25\n",
      "processing subject number:  26\n",
      "processing subject number:  27\n",
      "processing subject number:  28\n",
      "processing subject number:  29\n",
      "processing subject number:  30\n"
     ]
    }
   ],
   "source": [
    "selected_ch = ['Fz..','C3..', 'Cz..','C4..','Pz..']\n",
    "\n",
    "num_subject = 30\n",
    "\n",
    "raw_RorL1sub = [0]*num_subject  # RorL is Right or Left fist movement/imagery\n",
    "raw_Both1sub = [0]*num_subject  # Both is both feet and both fits movement/imagery\n",
    "\n",
    "start_subject = 0\n",
    "\n",
    "RAW_data_RorL = {}\n",
    "RAW_data_Both = {}\n",
    "\n",
    "for j in range(1+start_subject,num_subject+1+start_subject):\n",
    "\n",
    "    print(\"processing subject number: \", j)\n",
    "\n",
    "    if j < 10:\n",
    "        subject  = '00' + str(j)\n",
    "    elif j < 100:\n",
    "        subject = '0' + str(j)\n",
    "    else:\n",
    "        subject = str(j)\n",
    "\n",
    "    raw_RorL1 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R04.edf\",preload = True, verbose=False)\n",
    "    raw_RorL2 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R08.edf\",preload = True, verbose=False)\n",
    "    raw_RorL3 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R12.edf\",preload = True, verbose=False)\n",
    "\n",
    "    raw_RorL1sub[j-1+start_subject] = mne.concatenate_raws([raw_RorL1.pick(selected_ch), raw_RorL2.pick(selected_ch), raw_RorL3.pick(selected_ch)])\n",
    "    raw_RorL1sub[j-1+start_subject] = raw_RorL1sub[j-1+start_subject].resample(128) \n",
    "\n",
    "    eegbci.standardize(raw_RorL1sub[j-1+start_subject])  # set channel names\n",
    "    montage = make_standard_montage(\"standard_1005\")    \n",
    "    raw_RorL1sub[j-1+start_subject].set_montage(montage)\n",
    "\n",
    "    RAW_data_RorL[\"P\" + str(subject)] = {\"Raw_data\": raw_RorL1sub[j-1+start_subject].copy()}\n",
    "\n",
    "    raw_Both1 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R06.edf\",preload = True, verbose=False)\n",
    "    raw_Both2 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R10.edf\",preload = True, verbose=False)\n",
    "    raw_Both3 = mne.io.read_raw_edf(\"D:\\physionet_dataset\\S\" + str(subject) +\"\\S\" + str(subject) +\"R14.edf\",preload = True, verbose=False)\n",
    "\n",
    "    raw_Both1sub[j-1+start_subject] = mne.concatenate_raws([raw_Both1.pick(selected_ch), raw_Both2.pick(selected_ch), raw_Both3.pick(selected_ch)])\n",
    "    raw_Both1sub[j-1+start_subject] = raw_Both1sub[j-1+start_subject].resample(128)\n",
    "\n",
    "    eegbci.standardize(raw_Both1sub[j-1+start_subject])  # set channel names\n",
    "    montage = make_standard_montage(\"standard_1005\")    \n",
    "    raw_Both1sub[j-1+start_subject].set_montage(montage)\n",
    "\n",
    "    RAW_data_Both[\"P\" + str(subject)] = {\"Raw_data\": raw_Both1sub[j-1+start_subject].copy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "153 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "152 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:59: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\1053047862.py:63: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n"
     ]
    }
   ],
   "source": [
    "def butter_bandpass(lowcut,highcut,fs,order):\n",
    "    nyq = 0.5*fs\n",
    "    low = lowcut/nyq\n",
    "    high = highcut/nyq\n",
    "    b,a = signal.butter(order,[low,high],'bandpass')\n",
    "    return b,a\n",
    "\n",
    "def butter_bandpass_filter(data,lowcut = 6,highcut = 30, order = 4):\n",
    "    b,a = butter_bandpass(lowcut,highcut,128,order)\n",
    "    y = signal.filtfilt(b,a,data,axis=2)\n",
    "    return y\n",
    "\n",
    "def Get_epoch(RAW_data_RorL, RAW_data_Both, tmin=-2.0, tmax=4.0, crop=(0,2),baseline = (-0.5,0.0), trial_removal_th = 100):\n",
    "    EEG_epoch = {}\n",
    "    for key_subs in RAW_data_RorL:\n",
    "\n",
    "        event_id_mapping_RorL = {\n",
    "            old_event_id: new_event_id\n",
    "            for old_event_id, new_event_id in zip([1, 2, 3], [2, 0, 1])  # Example mapping\n",
    "        }\n",
    "\n",
    "        events_RorL, event_id1 = mne.events_from_annotations(RAW_data_RorL[key_subs]['Raw_data'], verbose=False)\n",
    "        events_RorL[:, 2] = [event_id_mapping_RorL.get(event_id1, event_id1) for event_id1 in events_RorL[:, 2]]\n",
    "\n",
    "        event_id1 = {'Rest': 2, 'Left': 0, 'Right': 1 } \n",
    "\n",
    "        RorL_epochs = mne.Epochs(RAW_data_RorL[key_subs]['Raw_data'], events_RorL, \n",
    "            tmin= tmin,     # init timestamp of epoch (0 means trigger timestamp same as event start)\n",
    "            tmax= tmax,    # final timestamp (10 means set epoch duration 10 second)\n",
    "            event_id= event_id1,\n",
    "            preload = True,\n",
    "            event_repeated='drop',\n",
    "            baseline=baseline,\n",
    "            verbose = False\n",
    "        )\n",
    "\n",
    "        ########################################################################################################\n",
    "\n",
    "        event_id_mapping_Both = {\n",
    "            old_event_id: new_event_id\n",
    "            for old_event_id, new_event_id in zip([1, 2, 3], [2, 4, 3])  # Example mapping\n",
    "        }\n",
    "\n",
    "        events_Both, event_id2 = mne.events_from_annotations(RAW_data_Both[key_subs]['Raw_data'], verbose=False)\n",
    "        events_Both[:, 2] = [event_id_mapping_Both.get(event_id2, event_id2) for event_id2 in events_Both[:, 2]]\n",
    "\n",
    "        event_id2 = {'Rest': 2,'both_feet': 3}  # Don't use both fits\n",
    "\n",
    "        Both_epochs = mne.Epochs(RAW_data_Both[key_subs]['Raw_data'], events_Both, \n",
    "            tmin= tmin,     # init timestamp of epoch (0 means trigger timestamp same as event start)\n",
    "            tmax= tmax,    # final timestamp (10 means set epoch duration 10 second)\n",
    "            event_id= event_id2,\n",
    "            preload = True,\n",
    "            event_repeated='drop',\n",
    "            baseline=baseline,\n",
    "            verbose = False\n",
    "        )\n",
    "\n",
    "        combine_epoch = mne.concatenate_epochs([RorL_epochs, Both_epochs])\n",
    "\n",
    "        EEG_epoch[key_subs] =  {\"Raw_Epoch\": combine_epoch.copy().crop(tmin= crop[0], tmax= crop[1])}\n",
    "\n",
    "        train_data = EEG_epoch[key_subs]['Raw_Epoch'].copy().get_data() * 10e5\n",
    "        labels = EEG_epoch[key_subs][\"Raw_Epoch\"].copy().events[:,-1]\n",
    "\n",
    "        outlier_trial = []\n",
    "        for ii in range(0,train_data.shape[0]):\n",
    "            if train_data[ii].max() > trial_removal_th or train_data[ii].min() < -trial_removal_th:\n",
    "                outlier_trial.append(ii)\n",
    "                print(key_subs,train_data[ii].min(), ii)\n",
    "                print(key_subs,train_data[ii].max(), ii)\n",
    "\n",
    "        EEG_epoch[key_subs]['Raw_Epoch'] = np.delete(train_data, outlier_trial, axis = 0)\n",
    "        EEG_epoch[key_subs]['label'] = np.delete(labels, outlier_trial)\n",
    "\n",
    "        filtered_data = butter_bandpass_filter(EEG_epoch[key_subs]['Raw_Epoch'], lowcut= 6, highcut= 32)\n",
    "        # EEG_epoch[key_subs]['Raw_Epoch'] = filtered_data\n",
    "\n",
    "        random_delete = random.sample(list(np.where(EEG_epoch[key_subs]['label']== 2)[0]), 60)\n",
    "\n",
    "        EEG_epoch[key_subs]['Raw_Epoch'] = np.delete(filtered_data, random_delete, axis = 0)\n",
    "        EEG_epoch[key_subs]['label'] = np.delete(EEG_epoch[key_subs]['label'], random_delete)\n",
    "\n",
    "        if \"Left\" not in target_class:\n",
    "                EEG_epoch[key_subs]['Raw_Epoch'] = np.delete(EEG_epoch[key_subs]['Raw_Epoch'], np.where(EEG_epoch[key_subs]['label']== 0), axis = 0)\n",
    "                EEG_epoch[key_subs]['label'] = np.delete(EEG_epoch[key_subs]['label'], np.where(EEG_epoch[key_subs]['label']== 0))\n",
    "\n",
    "        if \"Right\" not in target_class:\n",
    "            EEG_epoch[key_subs]['Raw_Epoch'] = np.delete(EEG_epoch[key_subs]['Raw_Epoch'], np.where(EEG_epoch[key_subs]['label']== 1), axis = 0)\n",
    "            EEG_epoch[key_subs]['label'] = np.delete(EEG_epoch[key_subs]['label'], np.where(EEG_epoch[key_subs]['label']== 1))\n",
    "\n",
    "        if \"Non\" not in target_class:\n",
    "            EEG_epoch[key_subs]['Raw_Epoch'] = np.delete(EEG_epoch[key_subs]['Raw_Epoch'], np.where(EEG_epoch[key_subs]['label']== 2), axis = 0)\n",
    "            EEG_epoch[key_subs]['label'] = np.delete(EEG_epoch[key_subs]['label'], np.where(EEG_epoch[key_subs]['label']== 2))\n",
    "\n",
    "        if \"Feet\" not in target_class:\n",
    "            EEG_epoch[key_subs]['Raw_Epoch'] = np.delete(EEG_epoch[key_subs]['Raw_Epoch'], np.where(EEG_epoch[key_subs]['label']== 3), axis = 0)\n",
    "            EEG_epoch[key_subs]['label'] = np.delete(EEG_epoch[key_subs]['label'], np.where(EEG_epoch[key_subs]['label']== 3))\n",
    "        \n",
    "    return EEG_epoch\n",
    "\n",
    "EEG_Epochs = Get_epoch(RAW_data_RorL, RAW_data_Both, tmin=-2.0, tmax=4.0, crop=(0,4),baseline = (-0.5,0.0), trial_removal_th = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "del RAW_data_Both\n",
    "del RAW_data_RorL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 1, 0, 1, 2, 0, 0, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1,\n",
       "       0, 1, 2, 0, 2, 1, 1, 0, 0, 1, 1, 2, 0, 0, 1, 0, 1, 2, 0, 0, 1, 2,\n",
       "       1, 0, 0, 2, 1, 1, 0, 2, 1, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EEG_Epochs['P001']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "LDA only Cross-validation scores: 0.7511111111111111\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80        16\n",
      "           1       0.93      0.87      0.90        15\n",
      "           2       0.70      0.82      0.76        17\n",
      "\n",
      "    accuracy                           0.81        48\n",
      "   macro avg       0.83      0.81      0.82        48\n",
      "weighted avg       0.82      0.81      0.81        48\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[12  0  4]\n",
      " [ 0 13  2]\n",
      " [ 2  1 14]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.43      0.55         7\n",
      "           1       0.78      1.00      0.88         7\n",
      "           2       0.75      0.86      0.80         7\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.76      0.76      0.74        21\n",
      "weighted avg       0.76      0.76      0.74        21\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[3 2 2]\n",
      " [0 7 0]\n",
      " [1 0 6]]\n"
     ]
    }
   ],
   "source": [
    "def GetConfusionMatrix(models, X_train, X_test, y_train, y_test):\n",
    "    y_pred = models.predict(X_train)\n",
    "    print(\"Classification TRAIN DATA \\n=======================\")\n",
    "    print(classification_report(y_true= y_train, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true= y_train, y_pred=y_pred))\n",
    "\n",
    "    y_pred = models.predict(X_test)\n",
    "    print(\"Classification TEST DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "    \n",
    "\n",
    "label_target = EEG_Epochs[target_data_0]['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(EEG_Epochs[target_data_0]['Raw_Epoch'], label_target, test_size=0.3, random_state = 42, stratify=label_target)\n",
    "\n",
    "csp = CSP(n_components = 5, reg=None, log=None, rank= 'info')\n",
    "csp.fit(x_train, y_train)   \n",
    "\n",
    "x_train = csp.transform(x_train)\n",
    "x_test = csp.transform(x_test)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "score = cross_val_score(lda, x_train, y_train, cv= 5)\n",
    "print(\"LDA only Cross-validation scores:\", np.mean(score))\n",
    "lda.fit(x_train, y_train)\n",
    "\n",
    "GetConfusionMatrix(lda, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BCIAllFunction import BCIFuntions\n",
    "\n",
    "calibrate_size = calibrate_size / EEG_Epochs[target_data_0]['Raw_Epoch'].shape[0]\n",
    "AllBCIClass = BCIFuntions(numclass = 4, frequency = 128, ch_pick = ['Fz','C3', 'Cz','C4','Pz'])\n",
    "\n",
    "if alignmentMethod == \"LA\":\n",
    "    AllBCIClass.ComputeLA(EEG_Epochs, target_subject= target_data_0, calibrate_size=calibrate_size)\n",
    "    if calibrate_size != 0:\n",
    "        target_data = target_data_0 + \"_test\"\n",
    "    count = 0\n",
    "\n",
    "    for index in range(len(EEG_Epochs[target_data]['KMediod_label'])):\n",
    "        if EEG_Epochs[target_data]['label'][index] == EEG_Epochs[target_data]['KMediod_label'][index]:\n",
    "            count += 1\n",
    "    print(count/len(EEG_Epochs[target_data]['KMediod_label']) * 100)\n",
    "    \n",
    "else:\n",
    "    AllBCIClass.GetRawSet_ComputeEA(EEG_Epochs, target_subject= target_data_0, calibrate_size=calibrate_size)\n",
    "    if calibrate_size != 0:\n",
    "        target_data = target_data_0 + \"_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Raw_Epoch', 'label', 'Raw_left', 'Raw_right', 'Raw_non', 'Raw_feet', 'EA_left', 'EA_right', 'EA_feet', 'EA_non', 'EA_Epoch'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EEG_Epochs[target_data_0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "LDA only Cross-validation scores: 0.41949676422293053\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.35      0.38       670\n",
      "           1       0.43      0.40      0.41       655\n",
      "           2       0.44      0.54      0.49       706\n",
      "\n",
      "    accuracy                           0.43      2031\n",
      "   macro avg       0.43      0.43      0.43      2031\n",
      "weighted avg       0.43      0.43      0.43      2031\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[236 185 249]\n",
      " [171 259 225]\n",
      " [170 156 380]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.77      0.61        13\n",
      "           1       0.65      0.92      0.76        12\n",
      "           2       0.50      0.07      0.12        14\n",
      "\n",
      "    accuracy                           0.56        39\n",
      "   macro avg       0.55      0.59      0.50        39\n",
      "weighted avg       0.55      0.56      0.48        39\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[10  2  1]\n",
      " [ 1 11  0]\n",
      " [ 9  4  1]]\n"
     ]
    }
   ],
   "source": [
    "AllBCIClass.classifyCSP_LDA(EEG_Epochs, target_subjects= target_data, condition = \"noEA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "LDA only Cross-validation scores: 0.510069545059403\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.48      0.48       670\n",
      "           1       0.49      0.46      0.48       655\n",
      "           2       0.56      0.60      0.58       706\n",
      "\n",
      "    accuracy                           0.52      2031\n",
      "   macro avg       0.52      0.52      0.52      2031\n",
      "weighted avg       0.52      0.52      0.52      2031\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[322 175 173]\n",
      " [195 304 156]\n",
      " [141 139 426]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.76        13\n",
      "           1       1.00      0.75      0.86        12\n",
      "           2       0.89      0.57      0.70        14\n",
      "\n",
      "    accuracy                           0.77        39\n",
      "   macro avg       0.84      0.77      0.77        39\n",
      "weighted avg       0.83      0.77      0.77        39\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[13  0  0]\n",
      " [ 2  9  1]\n",
      " [ 6  0  8]]\n"
     ]
    }
   ],
   "source": [
    "AllBCIClass.classifyCSP_LDA(EEG_Epochs, target_subjects= target_data, condition = \"EA\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_svm == True:\n",
    "    AllBCIClass.classifyCSP_SVM(EEG_Epochs, target_subjects= target_data, condition = \"noEA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_svm == True:\n",
    "    AllBCIClass.classifyCSP_SVM(EEG_Epochs, target_subjects= target_data, condition = \"EA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "\n",
    "# def discriminative_frequency_band_selection(data, true_label, label_1, label_2, x_test, y_test):\n",
    "#     Bh, Bl = 32, 6  # Initial upper and lower frequency limits\n",
    "#     A1, A2 = 0, 0  # Initial accuracy values\n",
    "\n",
    "#     # Step 1: Finding Bh\n",
    "#     while A1 >= A2:\n",
    "#         Bh -= 2\n",
    "#         A2 = A1        \n",
    "#         if(Bl >= Bh):\n",
    "#             break\n",
    "#         A1 = train_classifier(Bh, Bl, data, label_1, label_2, true_label, x_test, y_test)  # Design filter and train classifier using CSP\n",
    "\n",
    "#     Bh += 1  # Adjust Bh back\n",
    "#     A1 = train_classifier(Bh, Bl, data, label_1, label_2, true_label, x_test, y_test)\n",
    "\n",
    "#     if A1 >= A2:\n",
    "#         A2 = A1\n",
    "#     else:\n",
    "#         Bh = Bh + 1 \n",
    "#         A1 = A2\n",
    "\n",
    "#     # Step 2: Finding Bl\n",
    "#     while A1 >= A2:\n",
    "#         Bl += 2\n",
    "#         A2 = A1\n",
    "#         if(Bl >= Bh):\n",
    "#             break\n",
    "#         A1 = train_classifier(Bh, Bl, data, label_1, label_2, true_label, x_test, y_test)  # Design filter and train classifier using CSP\n",
    "\n",
    "#     Bl -= 1  # Adjust Bl back\n",
    "#     A1 = train_classifier(Bh, Bl, data, label_1, label_2, true_label, x_test, y_test)\n",
    "\n",
    "#     if A1 >= A2:\n",
    "#         A2 = A1\n",
    "#     else:\n",
    "#         Bh = Bh + 1 \n",
    "#         A1 = A2\n",
    "\n",
    "#     return Bh, Bl, A1\n",
    "\n",
    "\n",
    "# def train_classifier(Bh, Bl, data, label_1, label_2, true_label, x_test, y_test):\n",
    "\n",
    "#     init_data = data[np.where((true_label == label_1) | (true_label == label_2))]\n",
    "#     init_label = true_label[np.where((true_label == label_1) | (true_label == label_2))]\n",
    "\n",
    "#     print(init_data.shape, init_label.shape)\n",
    "\n",
    "#     x_test = x_test[np.where((y_test == label_1) | (y_test == label_2))]\n",
    "#     y_test = y_test[np.where((y_test == label_1) | (y_test == label_2))]\n",
    "\n",
    "#     filtered_data = butter_bandpass_filter(init_data, lowcut= Bl, highcut= Bh)\n",
    "#     x_test = butter_bandpass_filter(x_test, lowcut= Bl, highcut= Bh)\n",
    "\n",
    "#     csp = CSP(n_components = 5, reg=None, log=None, rank= 'info')\n",
    "#     csp.fit(filtered_data, init_label)\n",
    "\n",
    "#     x_train = csp.transform(filtered_data)\n",
    "#     x_test = csp.transform(x_test)\n",
    "\n",
    "#     # Initialize SVM with a linear kernel\n",
    "#     clf = SVC()\n",
    "\n",
    "#     param_grid = {\n",
    "#         'C':[1],\n",
    "#         'kernel': ['rbf'],  # Example kernels\n",
    "#     }\n",
    "\n",
    "#     grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "#     grid_search.fit(x_train, init_label)\n",
    "#     y_pred = grid_search.predict(x_test)\n",
    "\n",
    "#     return accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# def get_frequency_band(EEG_Epochs, target_data, condition = \"noEA\"):\n",
    "\n",
    "#     train_data = None\n",
    "#     train_label = None\n",
    "#     test_data = None\n",
    "#     test_label = None\n",
    "\n",
    "#     if condition == \"noEA\":\n",
    "#         query = \"Raw_Epoch\"\n",
    "#     else:\n",
    "#         query = \"EA_Epoch\"\n",
    "\n",
    "#     for sub in EEG_Epochs.keys():\n",
    "#         if sub == target_data:\n",
    "#             test_data = EEG_Epochs[sub][query]\n",
    "#             test_label = EEG_Epochs[sub]['label']\n",
    "\n",
    "#         else:\n",
    "#             if train_data is None:\n",
    "#                 train_data = EEG_Epochs[sub][query]\n",
    "#             else:\n",
    "#                 train_data = np.concatenate((train_data, EEG_Epochs[sub][query]), axis=0)\n",
    "\n",
    "#             if train_label is None:\n",
    "#                 train_label = EEG_Epochs[sub]['label']\n",
    "#             else:\n",
    "#                 train_label = np.concatenate((train_label, EEG_Epochs[sub]['label']), axis=0)\n",
    "\n",
    "#     indices = [0, 1, 2, 3]\n",
    "#     pairs = []\n",
    "#     band_high = []\n",
    "#     band_low = []\n",
    "#     acc= []\n",
    "#     class_name = ['left', 'right', 'non', 'feet']\n",
    "\n",
    "#     train_data, x_test_temp, train_label, y_test_temp = train_test_split(train_data, train_label, test_size=0.3, random_state = 42, stratify=train_label)\n",
    "\n",
    "#     # Nested loop to generate all pairs without reversing and without self-pairing\n",
    "#     for i in range(len(indices)):\n",
    "#         for j in range(i + 1, len(indices)):\n",
    "#             pairs.append((indices[i], indices[j]))\n",
    "#             Bh, Bl, A1 = discriminative_frequency_band_selection(data=train_data, true_label=train_label, label_1=indices[i], label_2=indices[j], x_test=x_test_temp, y_test=y_test_temp)\n",
    "#             band_high.append(Bh)\n",
    "#             band_low.append(Bl)\n",
    "#             acc.append(A1)\n",
    "\n",
    "#     stack_csp_train = []\n",
    "#     stack_csp_test = []\n",
    "#     stack_cnn_train = []\n",
    "#     stack_cnn_test = []\n",
    "\n",
    "#     for j in range(0,1):\n",
    "#         stack_csp_train = []\n",
    "#         stack_csp_test = []\n",
    "#         for i in range(0,len(band_high)):\n",
    "            \n",
    "#             # filter_x_train = butter_bandpass_filter(train_data[:,:,0:256+(128*j)] ,lowcut= band_low[i], highcut=band_high[i])\n",
    "#             # filter_x_test = butter_bandpass_filter(test_data[:,:,0:256+(128*j)] ,lowcut= band_low[i], highcut=band_high[i])\n",
    "\n",
    "#             filter_x_train = butter_bandpass_filter(train_data ,lowcut= band_low[i], highcut=band_high[i])\n",
    "#             filter_x_test = butter_bandpass_filter(test_data ,lowcut= band_low[i], highcut=band_high[i])\n",
    "\n",
    "#             csp = CSP(n_components = 5, reg=None, log=None, rank= 'info')\n",
    "#             csp.fit(filter_x_train, train_label)\n",
    "\n",
    "#             filter_x_train = csp.transform(filter_x_train)\n",
    "#             filter_x_test = csp.transform(filter_x_test)\n",
    "\n",
    "#             stack_csp_train.append(filter_x_train)\n",
    "#             stack_csp_test.append(filter_x_test)\n",
    "\n",
    "#         stack_cnn_train.append(np.hstack(np.array(stack_csp_train)))\n",
    "#         stack_cnn_test.append(np.hstack(np.array(stack_csp_test)))\n",
    "\n",
    "#     for i in range(len(indices)):\n",
    "#         for j in range(i + 1, len(indices)):\n",
    "#             print(f\"The selected frequency band of {class_name[i]} vs {class_name[j]} is: Bl = {band_low[j-1+i]}, Bh = {band_high[j-1+i]}, Acc = {acc[j-1+i]}\" )\n",
    "\n",
    "#     train_for_cnn = np.transpose(stack_cnn_train, (1, 2, 0))\n",
    "#     test_for_cnn = np.transpose(stack_cnn_test, (1, 2, 0))\n",
    "\n",
    "#     return train_for_cnn, test_for_cnn, train_label, test_label\n",
    "\n",
    "# train_for_cnn, test_for_cnn, train_label, test_label = get_frequency_band(EEG_Epochs, target_data, condition = con_for_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda = LinearDiscriminantAnalysis()\n",
    "# score = cross_val_score(lda, train_for_cnn[:,:,0], train_label, cv= 5)\n",
    "# print(\"LDA+CSP Cross-validation scores:\", np.mean(score))\n",
    "\n",
    "# lda.fit(train_for_cnn[:,:,0], train_label)\n",
    "\n",
    "# y_pred = lda.predict(train_for_cnn[:,:,0])\n",
    "\n",
    "# print(\"Classification TRAIN DATA \\n=======================\")\n",
    "# print(classification_report(y_true=train_label, y_pred=y_pred))\n",
    "# print(\"Confusion matrix \\n=======================\")\n",
    "# print(confusion_matrix(y_true=train_label, y_pred=y_pred))\n",
    "\n",
    "# y_pred = lda.predict(test_for_cnn[:,:,0])\n",
    "\n",
    "# print(\"Classification TEST DATA \\n=======================\")\n",
    "# print(classification_report(y_true=test_label, y_pred=y_pred))\n",
    "# print(\"Confusion matrix \\n=======================\")\n",
    "# print(confusion_matrix(y_true=test_label, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WLTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    MAG: rank 5 after 0 projectors applied to 5 channels\n",
      "Reducing data rank from 5 -> 5\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "CSP2D_Epoch = AllBCIClass.computeCSPFeatures(EEG_Epochs, target_subject = target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.595022678375244\n",
      "Epoch 20, Loss: 2.323911428451538\n",
      "Epoch 40, Loss: 1.499566912651062\n",
      "Epoch 60, Loss: 1.423752784729004\n",
      "Epoch 80, Loss: 1.4065316915512085\n",
      "Epoch 100, Loss: 1.4046186208724976\n",
      "Epoch 120, Loss: 1.4024800062179565\n",
      "Epoch 140, Loss: 1.4002559185028076\n",
      "Epoch 160, Loss: 1.3979237079620361\n",
      "Epoch 180, Loss: 1.3955087661743164\n",
      "Epoch 200, Loss: 1.3930059671401978\n",
      "Epoch 220, Loss: 1.3904439210891724\n",
      "Epoch 240, Loss: 1.3878345489501953\n",
      "Epoch 260, Loss: 1.3851951360702515\n",
      "Epoch 280, Loss: 1.3825395107269287\n",
      "Epoch 300, Loss: 1.3798829317092896\n",
      "Epoch 320, Loss: 1.3772389888763428\n",
      "Epoch 340, Loss: 1.3746213912963867\n",
      "Epoch 360, Loss: 1.372043251991272\n",
      "Epoch 380, Loss: 1.3695178031921387\n",
      "Epoch 400, Loss: 1.367056965827942\n",
      "Epoch 420, Loss: 1.3646740913391113\n",
      "Epoch 440, Loss: 1.3623806238174438\n",
      "Epoch 460, Loss: 1.3601887226104736\n",
      "Epoch 480, Loss: 1.3581104278564453\n",
      "Epoch 500, Loss: 1.3561564683914185\n",
      "Epoch 520, Loss: 1.3543387651443481\n",
      "Epoch 540, Loss: 1.3526684045791626\n",
      "Epoch 560, Loss: 1.3511557579040527\n",
      "Epoch 580, Loss: 1.3498120307922363\n",
      "Epoch 600, Loss: 1.3486475944519043\n",
      "Epoch 620, Loss: 1.3476725816726685\n",
      "Epoch 640, Loss: 1.3468972444534302\n",
      "Epoch 660, Loss: 1.3463313579559326\n",
      "Epoch 680, Loss: 1.345984935760498\n",
      "Epoch 700, Loss: 1.3458669185638428\n",
      "Epoch 720, Loss: 1.3459868431091309\n",
      "Epoch 740, Loss: 1.3463540077209473\n",
      "Epoch 760, Loss: 1.3469771146774292\n",
      "Epoch 780, Loss: 1.3478648662567139\n",
      "Epoch 800, Loss: 1.3490256071090698\n",
      "Epoch 820, Loss: 1.3504676818847656\n",
      "Epoch 840, Loss: 1.3521994352340698\n",
      "Epoch 860, Loss: 1.3542282581329346\n",
      "Epoch 880, Loss: 1.3565623760223389\n",
      "Epoch 900, Loss: 1.3592082262039185\n",
      "Epoch 920, Loss: 1.362174153327942\n",
      "Epoch 940, Loss: 1.365466594696045\n",
      "Epoch 960, Loss: 1.369092345237732\n",
      "Epoch 980, Loss: 1.3730580806732178\n",
      "weights of  P001 :  [[array([[-0.23815383, -0.41537365,  0.7194097 ],\n",
      "       [ 0.12030806, -0.9620944 , -0.3236112 ],\n",
      "       [-0.28103486,  0.3508089 ,  0.00691244],\n",
      "       [-0.04146401,  0.55733806, -0.06385174],\n",
      "       [-0.37882155, -0.33387208, -0.7224686 ]], dtype=float32), array([-0.3223971 , -0.34191063,  0.74775195], dtype=float32)]]\n",
      "Lowest loss of  P001 :  1.3458669\n",
      "Epoch 0, Loss: 3.119878053665161\n",
      "Epoch 20, Loss: 1.898094892501831\n",
      "Epoch 40, Loss: 1.3428622484207153\n",
      "Epoch 60, Loss: 1.3416826725006104\n",
      "Epoch 80, Loss: 1.3392839431762695\n",
      "Epoch 100, Loss: 1.3370139598846436\n",
      "Epoch 120, Loss: 1.3367739915847778\n",
      "Epoch 140, Loss: 1.3368983268737793\n",
      "Epoch 160, Loss: 1.337554693222046\n",
      "Epoch 180, Loss: 1.3387216329574585\n",
      "Epoch 200, Loss: 1.3404895067214966\n",
      "Epoch 220, Loss: 1.3429079055786133\n",
      "Epoch 240, Loss: 1.3460326194763184\n",
      "Epoch 260, Loss: 1.3499093055725098\n",
      "Epoch 280, Loss: 1.3545801639556885\n",
      "Epoch 300, Loss: 1.3600820302963257\n",
      "Epoch 320, Loss: 1.3664482831954956\n",
      "Epoch 340, Loss: 1.3737074136734009\n",
      "Epoch 360, Loss: 1.3818849325180054\n",
      "Epoch 380, Loss: 1.3910032510757446\n",
      "Epoch 400, Loss: 1.4010810852050781\n",
      "Epoch 420, Loss: 1.4121348857879639\n",
      "Epoch 440, Loss: 1.4241783618927002\n",
      "Epoch 460, Loss: 1.437222957611084\n",
      "Epoch 480, Loss: 1.4512783288955688\n",
      "Epoch 500, Loss: 1.466351866722107\n",
      "Epoch 520, Loss: 1.4824494123458862\n",
      "Epoch 540, Loss: 1.499575138092041\n",
      "Epoch 560, Loss: 1.51773202419281\n",
      "Epoch 580, Loss: 1.5369219779968262\n",
      "Epoch 600, Loss: 1.557145357131958\n",
      "Epoch 620, Loss: 1.5784019231796265\n",
      "Epoch 640, Loss: 1.6006906032562256\n",
      "Epoch 660, Loss: 1.6240088939666748\n",
      "Epoch 680, Loss: 1.6483550071716309\n",
      "Epoch 700, Loss: 1.6737254858016968\n",
      "Epoch 720, Loss: 1.7001168727874756\n",
      "Epoch 740, Loss: 1.727524757385254\n",
      "Epoch 760, Loss: 1.7559446096420288\n",
      "Epoch 780, Loss: 1.785372257232666\n",
      "Epoch 800, Loss: 1.8158023357391357\n",
      "Epoch 820, Loss: 1.8472298383712769\n",
      "Epoch 840, Loss: 1.8796494007110596\n",
      "Epoch 860, Loss: 1.913055181503296\n",
      "Epoch 880, Loss: 1.9474419355392456\n",
      "Epoch 900, Loss: 1.9828040599822998\n",
      "Epoch 920, Loss: 2.0191354751586914\n",
      "Epoch 940, Loss: 2.056431293487549\n",
      "Epoch 960, Loss: 2.0946853160858154\n",
      "Epoch 980, Loss: 2.1338906288146973\n",
      "weights of  P002 :  [[array([[-0.68720835, -0.04812013, -0.15200895],\n",
      "       [-0.5730793 ,  0.08184108, -0.25294426],\n",
      "       [ 0.19654112, -0.255642  ,  0.04908255],\n",
      "       [-0.4709482 ,  0.3086189 , -0.21382739],\n",
      "       [ 0.36298558, -0.8121983 , -0.40103436]], dtype=float32), array([-0.13165903,  0.15364483, -0.15038727], dtype=float32)]]\n",
      "Lowest loss of  P002 :  1.3367274\n",
      "Epoch 0, Loss: 2.7578985691070557\n",
      "Epoch 20, Loss: 1.5547627210617065\n",
      "Epoch 40, Loss: 1.464566707611084\n",
      "Epoch 60, Loss: 1.4434006214141846\n",
      "Epoch 80, Loss: 1.439979076385498\n",
      "Epoch 100, Loss: 1.4377087354660034\n",
      "Epoch 120, Loss: 1.4353545904159546\n",
      "Epoch 140, Loss: 1.4328428506851196\n",
      "Epoch 160, Loss: 1.4302215576171875\n",
      "Epoch 180, Loss: 1.4274864196777344\n",
      "Epoch 200, Loss: 1.4246690273284912\n",
      "Epoch 220, Loss: 1.4217854738235474\n",
      "Epoch 240, Loss: 1.4188534021377563\n",
      "Epoch 260, Loss: 1.4158889055252075\n",
      "Epoch 280, Loss: 1.4129068851470947\n",
      "Epoch 300, Loss: 1.409921407699585\n",
      "Epoch 320, Loss: 1.406944990158081\n",
      "Epoch 340, Loss: 1.403989553451538\n",
      "Epoch 360, Loss: 1.4010664224624634\n",
      "Epoch 380, Loss: 1.3981854915618896\n",
      "Epoch 400, Loss: 1.39535653591156\n",
      "Epoch 420, Loss: 1.3925879001617432\n",
      "Epoch 440, Loss: 1.3898881673812866\n",
      "Epoch 460, Loss: 1.3872640132904053\n",
      "Epoch 480, Loss: 1.3847224712371826\n",
      "Epoch 500, Loss: 1.3822698593139648\n",
      "Epoch 520, Loss: 1.379911184310913\n",
      "Epoch 540, Loss: 1.377651572227478\n",
      "Epoch 560, Loss: 1.3754955530166626\n",
      "Epoch 580, Loss: 1.3734467029571533\n",
      "Epoch 600, Loss: 1.3715087175369263\n",
      "Epoch 620, Loss: 1.3696837425231934\n",
      "Epoch 640, Loss: 1.3679752349853516\n",
      "Epoch 660, Loss: 1.3663846254348755\n",
      "Epoch 680, Loss: 1.3649134635925293\n",
      "Epoch 700, Loss: 1.3635634183883667\n",
      "Epoch 720, Loss: 1.3623353242874146\n",
      "Epoch 740, Loss: 1.361229419708252\n",
      "Epoch 760, Loss: 1.3602464199066162\n",
      "Epoch 780, Loss: 1.3593857288360596\n",
      "Epoch 800, Loss: 1.3586479425430298\n",
      "Epoch 820, Loss: 1.3580322265625\n",
      "Epoch 840, Loss: 1.3575377464294434\n",
      "Epoch 860, Loss: 1.357163667678833\n",
      "Epoch 880, Loss: 1.356909155845642\n",
      "Epoch 900, Loss: 1.356773018836975\n",
      "Epoch 920, Loss: 1.3567538261413574\n",
      "Epoch 940, Loss: 1.356850266456604\n",
      "Epoch 960, Loss: 1.3570605516433716\n",
      "Epoch 980, Loss: 1.357383370399475\n",
      "weights of  P003 :  [[array([[-0.9603157 , -0.51474434, -0.16799374],\n",
      "       [ 0.06811364, -0.02030594,  0.2666948 ],\n",
      "       [-0.10936667, -0.555362  ,  0.12956804],\n",
      "       [ 0.7578861 ,  0.2759958 , -0.2825189 ],\n",
      "       [ 0.13601369,  0.59945494, -0.0260772 ]], dtype=float32), array([-0.01242922, -0.02224835,  0.11448427], dtype=float32)]]\n",
      "Lowest loss of  P003 :  1.3567474\n",
      "Epoch 0, Loss: 4.009241104125977\n",
      "Epoch 20, Loss: 2.438913345336914\n",
      "Epoch 40, Loss: 1.5902507305145264\n",
      "Epoch 60, Loss: 1.4257748126983643\n",
      "Epoch 80, Loss: 1.413312315940857\n",
      "Epoch 100, Loss: 1.4117131233215332\n",
      "Epoch 120, Loss: 1.4106007814407349\n",
      "Epoch 140, Loss: 1.4098080396652222\n",
      "Epoch 160, Loss: 1.4090341329574585\n",
      "Epoch 180, Loss: 1.4082392454147339\n",
      "Epoch 200, Loss: 1.4074592590332031\n",
      "Epoch 220, Loss: 1.4066873788833618\n",
      "Epoch 240, Loss: 1.4059419631958008\n",
      "Epoch 260, Loss: 1.405229926109314\n",
      "Epoch 280, Loss: 1.4045616388320923\n",
      "Epoch 300, Loss: 1.4039463996887207\n",
      "Epoch 320, Loss: 1.4033939838409424\n",
      "Epoch 340, Loss: 1.402912974357605\n",
      "Epoch 360, Loss: 1.4025126695632935\n",
      "Epoch 380, Loss: 1.4022018909454346\n",
      "Epoch 400, Loss: 1.4019895792007446\n",
      "Epoch 420, Loss: 1.4018843173980713\n",
      "Epoch 440, Loss: 1.4018946886062622\n",
      "Epoch 460, Loss: 1.402029275894165\n",
      "Epoch 480, Loss: 1.4022966623306274\n",
      "Epoch 500, Loss: 1.4027047157287598\n",
      "Epoch 520, Loss: 1.4032615423202515\n",
      "Epoch 540, Loss: 1.4039757251739502\n",
      "Epoch 560, Loss: 1.4048545360565186\n",
      "Epoch 580, Loss: 1.4059062004089355\n",
      "Epoch 600, Loss: 1.4071383476257324\n",
      "Epoch 620, Loss: 1.4085586071014404\n",
      "Epoch 640, Loss: 1.4101742506027222\n",
      "Epoch 660, Loss: 1.4119927883148193\n",
      "Epoch 680, Loss: 1.4140208959579468\n",
      "Epoch 700, Loss: 1.4162663221359253\n",
      "Epoch 720, Loss: 1.4187355041503906\n",
      "Epoch 740, Loss: 1.4214352369308472\n",
      "Epoch 760, Loss: 1.4243723154067993\n",
      "Epoch 780, Loss: 1.4275527000427246\n",
      "Epoch 800, Loss: 1.4309831857681274\n",
      "Epoch 820, Loss: 1.4346694946289062\n",
      "Epoch 840, Loss: 1.4386177062988281\n",
      "Epoch 860, Loss: 1.4428337812423706\n",
      "Epoch 880, Loss: 1.4473227262496948\n",
      "Epoch 900, Loss: 1.4520903825759888\n",
      "Epoch 920, Loss: 1.4571419954299927\n",
      "Epoch 940, Loss: 1.4624823331832886\n",
      "Epoch 960, Loss: 1.468116283416748\n",
      "Epoch 980, Loss: 1.4740487337112427\n",
      "weights of  P004 :  [[array([[ 0.48728955, -0.63719726,  0.35071287],\n",
      "       [-0.10202771, -0.7739985 ,  0.5580369 ],\n",
      "       [-0.26958555,  0.33223936, -0.15565231],\n",
      "       [-0.3501803 , -0.23551232,  0.24025558],\n",
      "       [-0.18994543,  0.66688406, -0.9625099 ]], dtype=float32), array([-0.29539183, -0.2435267 ,  0.48133832], dtype=float32)]]\n",
      "Lowest loss of  P004 :  1.4018741\n",
      "Epoch 0, Loss: 5.328625679016113\n",
      "Epoch 20, Loss: 3.7921206951141357\n",
      "Epoch 40, Loss: 2.620828151702881\n",
      "Epoch 60, Loss: 1.5684353113174438\n",
      "Epoch 80, Loss: 1.355126976966858\n",
      "Epoch 100, Loss: 1.3420222997665405\n",
      "Epoch 120, Loss: 1.3402553796768188\n",
      "Epoch 140, Loss: 1.3387892246246338\n",
      "Epoch 160, Loss: 1.3378422260284424\n",
      "Epoch 180, Loss: 1.336952805519104\n",
      "Epoch 200, Loss: 1.3360316753387451\n",
      "Epoch 220, Loss: 1.3351540565490723\n",
      "Epoch 240, Loss: 1.3343005180358887\n",
      "Epoch 260, Loss: 1.3334932327270508\n",
      "Epoch 280, Loss: 1.3327395915985107\n",
      "Epoch 300, Loss: 1.3320491313934326\n",
      "Epoch 320, Loss: 1.3314321041107178\n",
      "Epoch 340, Loss: 1.3308963775634766\n",
      "Epoch 360, Loss: 1.3304511308670044\n",
      "Epoch 380, Loss: 1.3301033973693848\n",
      "Epoch 400, Loss: 1.3298611640930176\n",
      "Epoch 420, Loss: 1.3297312259674072\n",
      "Epoch 440, Loss: 1.3297200202941895\n",
      "Epoch 460, Loss: 1.329833745956421\n",
      "Epoch 480, Loss: 1.330077886581421\n",
      "Epoch 500, Loss: 1.3304574489593506\n",
      "Epoch 520, Loss: 1.3309779167175293\n",
      "Epoch 540, Loss: 1.3316431045532227\n",
      "Epoch 560, Loss: 1.3324570655822754\n",
      "Epoch 580, Loss: 1.333423376083374\n",
      "Epoch 600, Loss: 1.3345448970794678\n",
      "Epoch 620, Loss: 1.3358250856399536\n",
      "Epoch 640, Loss: 1.3372654914855957\n",
      "Epoch 660, Loss: 1.3388688564300537\n",
      "Epoch 680, Loss: 1.3406360149383545\n",
      "Epoch 700, Loss: 1.3425688743591309\n",
      "Epoch 720, Loss: 1.3446682691574097\n",
      "Epoch 740, Loss: 1.3469346761703491\n",
      "Epoch 760, Loss: 1.3493684530258179\n",
      "Epoch 780, Loss: 1.3519693613052368\n",
      "Epoch 800, Loss: 1.354737401008606\n",
      "Epoch 820, Loss: 1.3576719760894775\n",
      "Epoch 840, Loss: 1.3607720136642456\n",
      "Epoch 860, Loss: 1.3640367984771729\n",
      "Epoch 880, Loss: 1.3674646615982056\n",
      "Epoch 900, Loss: 1.3710541725158691\n",
      "Epoch 920, Loss: 1.3748036623001099\n",
      "Epoch 940, Loss: 1.3787108659744263\n",
      "Epoch 960, Loss: 1.3827742338180542\n",
      "Epoch 980, Loss: 1.3869909048080444\n",
      "weights of  P005 :  [[array([[-0.52000237,  0.07610371, -0.5718564 ],\n",
      "       [-0.5732384 , -0.46455625,  0.25575608],\n",
      "       [-0.33389127, -0.67489547, -0.4779445 ],\n",
      "       [-0.39523578, -0.38379723, -0.10701506],\n",
      "       [ 0.5410081 ,  0.3557383 , -0.09416074]], dtype=float32), array([-0.29736683,  0.23715964,  0.2092042 ], dtype=float32)]]\n",
      "Lowest loss of  P005 :  1.3297099\n",
      "Epoch 0, Loss: 4.519873142242432\n",
      "Epoch 20, Loss: 2.503309488296509\n",
      "Epoch 40, Loss: 1.7331061363220215\n",
      "Epoch 60, Loss: 1.4349992275238037\n",
      "Epoch 80, Loss: 1.430284857749939\n",
      "Epoch 100, Loss: 1.4279183149337769\n",
      "Epoch 120, Loss: 1.4268007278442383\n",
      "Epoch 140, Loss: 1.4260661602020264\n",
      "Epoch 160, Loss: 1.4252586364746094\n",
      "Epoch 180, Loss: 1.4244072437286377\n",
      "Epoch 200, Loss: 1.4235090017318726\n",
      "Epoch 220, Loss: 1.4225666522979736\n",
      "Epoch 240, Loss: 1.4215826988220215\n",
      "Epoch 260, Loss: 1.4205596446990967\n",
      "Epoch 280, Loss: 1.4194995164871216\n",
      "Epoch 300, Loss: 1.418404459953308\n",
      "Epoch 320, Loss: 1.417276382446289\n",
      "Epoch 340, Loss: 1.4161170721054077\n",
      "Epoch 360, Loss: 1.4149284362792969\n",
      "Epoch 380, Loss: 1.4137120246887207\n",
      "Epoch 400, Loss: 1.4124689102172852\n",
      "Epoch 420, Loss: 1.4112014770507812\n",
      "Epoch 440, Loss: 1.4099105596542358\n",
      "Epoch 460, Loss: 1.4085980653762817\n",
      "Epoch 480, Loss: 1.4072649478912354\n",
      "Epoch 500, Loss: 1.4059127569198608\n",
      "Epoch 520, Loss: 1.4045425653457642\n",
      "Epoch 540, Loss: 1.4031559228897095\n",
      "Epoch 560, Loss: 1.4017541408538818\n",
      "Epoch 580, Loss: 1.4003381729125977\n",
      "Epoch 600, Loss: 1.398909568786621\n",
      "Epoch 620, Loss: 1.3974692821502686\n",
      "Epoch 640, Loss: 1.3960185050964355\n",
      "Epoch 660, Loss: 1.3945585489273071\n",
      "Epoch 680, Loss: 1.3930904865264893\n",
      "Epoch 700, Loss: 1.3916155099868774\n",
      "Epoch 720, Loss: 1.390134572982788\n",
      "Epoch 740, Loss: 1.3886486291885376\n",
      "Epoch 760, Loss: 1.3871592283248901\n",
      "Epoch 780, Loss: 1.385667324066162\n",
      "Epoch 800, Loss: 1.3841737508773804\n",
      "Epoch 820, Loss: 1.3826797008514404\n",
      "Epoch 840, Loss: 1.3811862468719482\n",
      "Epoch 860, Loss: 1.3796945810317993\n",
      "Epoch 880, Loss: 1.3782051801681519\n",
      "Epoch 900, Loss: 1.3767197132110596\n",
      "Epoch 920, Loss: 1.3752386569976807\n",
      "Epoch 940, Loss: 1.3737632036209106\n",
      "Epoch 960, Loss: 1.372294545173645\n",
      "Epoch 980, Loss: 1.370833396911621\n",
      "weights of  P006 :  [[array([[ 4.4230434e-01, -3.3063337e-01, -4.8113528e-01],\n",
      "       [-5.0928015e-01,  2.2742610e-01, -1.2615371e-04],\n",
      "       [-3.6270133e-01, -5.8573169e-01, -6.8801278e-01],\n",
      "       [-2.9277390e-01,  3.0446145e-01,  1.3424316e-01],\n",
      "       [ 4.5522228e-01,  2.0691526e-01,  6.4846283e-01]], dtype=float32), array([ 0.08001204,  0.05772964, -0.13378426], dtype=float32)]]\n",
      "Lowest loss of  P006 :  1.3694532\n",
      "Epoch 0, Loss: 6.5802435874938965\n",
      "Epoch 20, Loss: 4.583643913269043\n",
      "Epoch 40, Loss: 3.5919957160949707\n",
      "Epoch 60, Loss: 2.5459892749786377\n",
      "Epoch 80, Loss: 1.6240837574005127\n",
      "Epoch 100, Loss: 1.435034155845642\n",
      "Epoch 120, Loss: 1.4215941429138184\n",
      "Epoch 140, Loss: 1.4182178974151611\n",
      "Epoch 160, Loss: 1.4146169424057007\n",
      "Epoch 180, Loss: 1.4113352298736572\n",
      "Epoch 200, Loss: 1.4079229831695557\n",
      "Epoch 220, Loss: 1.4043519496917725\n",
      "Epoch 240, Loss: 1.4006692171096802\n",
      "Epoch 260, Loss: 1.3968762159347534\n",
      "Epoch 280, Loss: 1.392988681793213\n",
      "Epoch 300, Loss: 1.3890188932418823\n",
      "Epoch 320, Loss: 1.3849785327911377\n",
      "Epoch 340, Loss: 1.3808786869049072\n",
      "Epoch 360, Loss: 1.3767305612564087\n",
      "Epoch 380, Loss: 1.3725448846817017\n",
      "Epoch 400, Loss: 1.3683316707611084\n",
      "Epoch 420, Loss: 1.3641009330749512\n",
      "Epoch 440, Loss: 1.3598623275756836\n",
      "Epoch 460, Loss: 1.3556251525878906\n",
      "Epoch 480, Loss: 1.3513988256454468\n",
      "Epoch 500, Loss: 1.3471922874450684\n",
      "Epoch 520, Loss: 1.3430144786834717\n",
      "Epoch 540, Loss: 1.3388733863830566\n",
      "Epoch 560, Loss: 1.33477783203125\n",
      "Epoch 580, Loss: 1.3307359218597412\n",
      "Epoch 600, Loss: 1.3267552852630615\n",
      "Epoch 620, Loss: 1.3228445053100586\n",
      "Epoch 640, Loss: 1.3190102577209473\n",
      "Epoch 660, Loss: 1.3152605295181274\n",
      "Epoch 680, Loss: 1.3116025924682617\n",
      "Epoch 700, Loss: 1.308043360710144\n",
      "Epoch 720, Loss: 1.3045895099639893\n",
      "Epoch 740, Loss: 1.3012480735778809\n",
      "Epoch 760, Loss: 1.298025369644165\n",
      "Epoch 780, Loss: 1.294927954673767\n",
      "Epoch 800, Loss: 1.291961669921875\n",
      "Epoch 820, Loss: 1.2891327142715454\n",
      "Epoch 840, Loss: 1.2864468097686768\n",
      "Epoch 860, Loss: 1.283909559249878\n",
      "Epoch 880, Loss: 1.2815265655517578\n",
      "Epoch 900, Loss: 1.2793025970458984\n",
      "Epoch 920, Loss: 1.27724289894104\n",
      "Epoch 940, Loss: 1.275352954864502\n",
      "Epoch 960, Loss: 1.2736365795135498\n",
      "Epoch 980, Loss: 1.2720985412597656\n",
      "weights of  P008 :  [[array([[ 0.4319515 ,  0.12649652, -0.44608054],\n",
      "       [-0.0163597 ,  0.00995189,  0.03871019],\n",
      "       [-0.1320187 ,  0.4760672 ,  1.0032514 ],\n",
      "       [ 0.3916559 ,  0.0070628 , -0.10773578],\n",
      "       [-0.22854693, -0.05469525,  0.00952412]], dtype=float32), array([-0.5694093 ,  0.21473253,  0.18593459], dtype=float32)]]\n",
      "Lowest loss of  P008 :  1.2708068\n",
      "Epoch 0, Loss: 1.7517412900924683\n",
      "Epoch 20, Loss: 1.4390455484390259\n",
      "Epoch 40, Loss: 1.4157096147537231\n",
      "Epoch 60, Loss: 1.4097367525100708\n",
      "Epoch 80, Loss: 1.4050697088241577\n",
      "Epoch 100, Loss: 1.400890827178955\n",
      "Epoch 120, Loss: 1.396788239479065\n",
      "Epoch 140, Loss: 1.3928723335266113\n",
      "Epoch 160, Loss: 1.3892700672149658\n",
      "Epoch 180, Loss: 1.3860567808151245\n",
      "Epoch 200, Loss: 1.3832958936691284\n",
      "Epoch 220, Loss: 1.381035566329956\n",
      "Epoch 240, Loss: 1.3793096542358398\n",
      "Epoch 260, Loss: 1.3781388998031616\n",
      "Epoch 280, Loss: 1.3775333166122437\n",
      "Epoch 300, Loss: 1.3774936199188232\n",
      "Epoch 320, Loss: 1.3780126571655273\n",
      "Epoch 340, Loss: 1.3790771961212158\n",
      "Epoch 360, Loss: 1.3806684017181396\n",
      "Epoch 380, Loss: 1.3827641010284424\n",
      "Epoch 400, Loss: 1.385338306427002\n",
      "Epoch 420, Loss: 1.3883633613586426\n",
      "Epoch 440, Loss: 1.3918095827102661\n",
      "Epoch 460, Loss: 1.395647644996643\n",
      "Epoch 480, Loss: 1.399847388267517\n",
      "Epoch 500, Loss: 1.4043796062469482\n",
      "Epoch 520, Loss: 1.4092158079147339\n",
      "Epoch 540, Loss: 1.414329171180725\n",
      "Epoch 560, Loss: 1.4196938276290894\n",
      "Epoch 580, Loss: 1.4252865314483643\n",
      "Epoch 600, Loss: 1.4310853481292725\n",
      "Epoch 620, Loss: 1.4370710849761963\n",
      "Epoch 640, Loss: 1.4432262182235718\n",
      "Epoch 660, Loss: 1.4495353698730469\n",
      "Epoch 680, Loss: 1.4559860229492188\n",
      "Epoch 700, Loss: 1.4625669717788696\n",
      "Epoch 720, Loss: 1.4692693948745728\n",
      "Epoch 740, Loss: 1.4760864973068237\n",
      "Epoch 760, Loss: 1.4830132722854614\n",
      "Epoch 780, Loss: 1.490046739578247\n",
      "Epoch 800, Loss: 1.4971849918365479\n",
      "Epoch 820, Loss: 1.504427433013916\n",
      "Epoch 840, Loss: 1.5117754936218262\n",
      "Epoch 860, Loss: 1.5192309617996216\n",
      "Epoch 880, Loss: 1.5267977714538574\n",
      "Epoch 900, Loss: 1.5344792604446411\n",
      "Epoch 920, Loss: 1.5422804355621338\n",
      "Epoch 940, Loss: 1.550207257270813\n",
      "Epoch 960, Loss: 1.5582647323608398\n",
      "Epoch 980, Loss: 1.5664594173431396\n",
      "weights of  P009 :  [[array([[ 0.2275735 , -0.16775045,  0.49922863],\n",
      "       [ 0.00502132,  0.67998236,  0.04749759],\n",
      "       [-0.25933722, -0.23260435, -0.5033628 ],\n",
      "       [-0.6138    ,  0.46008804,  0.64626354],\n",
      "       [ 0.48373878, -0.5532104 , -0.41249338]], dtype=float32), array([-0.02397995, -0.07303386,  0.32878724], dtype=float32)]]\n",
      "Lowest loss of  P009 :  1.3774415\n",
      "Epoch 0, Loss: 4.548048973083496\n",
      "Epoch 20, Loss: 3.358147382736206\n",
      "Epoch 40, Loss: 2.178706407546997\n",
      "Epoch 60, Loss: 1.2703016996383667\n",
      "Epoch 80, Loss: 1.2277792692184448\n",
      "Epoch 100, Loss: 1.2055326700210571\n",
      "Epoch 120, Loss: 1.2000194787979126\n",
      "Epoch 140, Loss: 1.1956688165664673\n",
      "Epoch 160, Loss: 1.1917057037353516\n",
      "Epoch 180, Loss: 1.1881855726242065\n",
      "Epoch 200, Loss: 1.1852539777755737\n",
      "Epoch 220, Loss: 1.182976245880127\n",
      "Epoch 240, Loss: 1.1814292669296265\n",
      "Epoch 260, Loss: 1.1806734800338745\n",
      "Epoch 280, Loss: 1.1807624101638794\n",
      "Epoch 300, Loss: 1.1817400455474854\n",
      "Epoch 320, Loss: 1.1836421489715576\n",
      "Epoch 340, Loss: 1.1864986419677734\n",
      "Epoch 360, Loss: 1.1903315782546997\n",
      "Epoch 380, Loss: 1.1951587200164795\n",
      "Epoch 400, Loss: 1.2009917497634888\n",
      "Epoch 420, Loss: 1.2078382968902588\n",
      "Epoch 440, Loss: 1.2157018184661865\n",
      "Epoch 460, Loss: 1.2245819568634033\n",
      "Epoch 480, Loss: 1.2344757318496704\n",
      "Epoch 500, Loss: 1.2453768253326416\n",
      "Epoch 520, Loss: 1.2572764158248901\n",
      "Epoch 540, Loss: 1.270164132118225\n",
      "Epoch 560, Loss: 1.284026861190796\n",
      "Epoch 580, Loss: 1.2988505363464355\n",
      "Epoch 600, Loss: 1.3146198987960815\n",
      "Epoch 620, Loss: 1.3313180208206177\n",
      "Epoch 640, Loss: 1.3489274978637695\n",
      "Epoch 660, Loss: 1.3674299716949463\n",
      "Epoch 680, Loss: 1.3868061304092407\n",
      "Epoch 700, Loss: 1.4070370197296143\n",
      "Epoch 720, Loss: 1.4281026124954224\n",
      "Epoch 740, Loss: 1.4499835968017578\n",
      "Epoch 760, Loss: 1.4726593494415283\n",
      "Epoch 780, Loss: 1.4961100816726685\n",
      "Epoch 800, Loss: 1.5203161239624023\n",
      "Epoch 820, Loss: 1.5452570915222168\n",
      "Epoch 840, Loss: 1.5709136724472046\n",
      "Epoch 860, Loss: 1.597266674041748\n",
      "Epoch 880, Loss: 1.6242969036102295\n",
      "Epoch 900, Loss: 1.6519854068756104\n",
      "Epoch 920, Loss: 1.680314064025879\n",
      "Epoch 940, Loss: 1.7092647552490234\n",
      "Epoch 960, Loss: 1.7388195991516113\n",
      "Epoch 980, Loss: 1.768961787223816\n",
      "weights of  P010 :  [[array([[-0.3119751 ,  0.20097688,  0.36857706],\n",
      "       [ 0.3234218 , -0.26899925, -0.05077782],\n",
      "       [ 0.38306034,  0.14257513,  0.44396973],\n",
      "       [ 0.06674461,  0.38834026, -0.4496256 ],\n",
      "       [-0.26936963, -0.04341903, -0.11856674]], dtype=float32), array([-0.37350258,  0.28666446, -0.2366276 ], dtype=float32)]]\n",
      "Lowest loss of  P010 :  1.180605\n",
      "Epoch 0, Loss: 6.019071578979492\n",
      "Epoch 20, Loss: 4.2525715827941895\n",
      "Epoch 40, Loss: 3.212402820587158\n",
      "Epoch 60, Loss: 2.143129587173462\n",
      "Epoch 80, Loss: 1.3853340148925781\n",
      "Epoch 100, Loss: 1.3609206676483154\n",
      "Epoch 120, Loss: 1.3478889465332031\n",
      "Epoch 140, Loss: 1.3480284214019775\n",
      "Epoch 160, Loss: 1.3479864597320557\n",
      "Epoch 180, Loss: 1.3482167720794678\n",
      "Epoch 200, Loss: 1.3484547138214111\n",
      "Epoch 220, Loss: 1.3487160205841064\n",
      "Epoch 240, Loss: 1.3489969968795776\n",
      "Epoch 260, Loss: 1.3492960929870605\n",
      "Epoch 280, Loss: 1.3496145009994507\n",
      "Epoch 300, Loss: 1.3499529361724854\n",
      "Epoch 320, Loss: 1.350311040878296\n",
      "Epoch 340, Loss: 1.3506898880004883\n",
      "Epoch 360, Loss: 1.3510899543762207\n",
      "Epoch 380, Loss: 1.351511836051941\n",
      "Epoch 400, Loss: 1.3519556522369385\n",
      "Epoch 420, Loss: 1.3524225950241089\n",
      "Epoch 440, Loss: 1.3529129028320312\n",
      "Epoch 460, Loss: 1.3534274101257324\n",
      "Epoch 480, Loss: 1.353966474533081\n",
      "Epoch 500, Loss: 1.3545308113098145\n",
      "Epoch 520, Loss: 1.355121374130249\n",
      "Epoch 540, Loss: 1.355738639831543\n",
      "Epoch 560, Loss: 1.3563833236694336\n",
      "Epoch 580, Loss: 1.357055902481079\n",
      "Epoch 600, Loss: 1.3577576875686646\n",
      "Epoch 620, Loss: 1.35848867893219\n",
      "Epoch 640, Loss: 1.3592501878738403\n",
      "Epoch 660, Loss: 1.3600425720214844\n",
      "Epoch 680, Loss: 1.3608663082122803\n",
      "Epoch 700, Loss: 1.361722469329834\n",
      "Epoch 720, Loss: 1.362612009048462\n",
      "Epoch 740, Loss: 1.3635356426239014\n",
      "Epoch 760, Loss: 1.364493727684021\n",
      "Epoch 780, Loss: 1.3654870986938477\n",
      "Epoch 800, Loss: 1.3665167093276978\n",
      "Epoch 820, Loss: 1.3675832748413086\n",
      "Epoch 840, Loss: 1.368687391281128\n",
      "Epoch 860, Loss: 1.3698301315307617\n",
      "Epoch 880, Loss: 1.371011734008789\n",
      "Epoch 900, Loss: 1.372233271598816\n",
      "Epoch 920, Loss: 1.3734954595565796\n",
      "Epoch 940, Loss: 1.3747988939285278\n",
      "Epoch 960, Loss: 1.3761447668075562\n",
      "Epoch 980, Loss: 1.377533197402954\n",
      "weights of  P011 :  [[array([[-0.21235022, -0.34093162,  0.31595403],\n",
      "       [-0.63092226, -0.382374  ,  0.431298  ],\n",
      "       [ 0.629349  , -0.18189834,  0.05194735],\n",
      "       [ 0.5428878 ,  0.5033944 , -0.63916135],\n",
      "       [-0.07242723,  0.4907315 ,  0.1490377 ]], dtype=float32), array([-0.11770245, -0.2995258 ,  0.41326854], dtype=float32)]]\n",
      "Lowest loss of  P011 :  1.3478779\n",
      "Epoch 0, Loss: 7.545942783355713\n",
      "Epoch 20, Loss: 5.0469465255737305\n",
      "Epoch 40, Loss: 2.7657463550567627\n",
      "Epoch 60, Loss: 1.8295060396194458\n",
      "Epoch 80, Loss: 1.538597583770752\n",
      "Epoch 100, Loss: 1.5411243438720703\n",
      "Epoch 120, Loss: 1.5360496044158936\n",
      "Epoch 140, Loss: 1.533740758895874\n",
      "Epoch 160, Loss: 1.5319461822509766\n",
      "Epoch 180, Loss: 1.529966950416565\n",
      "Epoch 200, Loss: 1.5278973579406738\n",
      "Epoch 220, Loss: 1.5257564783096313\n",
      "Epoch 240, Loss: 1.523529291152954\n",
      "Epoch 260, Loss: 1.5212345123291016\n",
      "Epoch 280, Loss: 1.518876075744629\n",
      "Epoch 300, Loss: 1.5164613723754883\n",
      "Epoch 320, Loss: 1.5139967203140259\n",
      "Epoch 340, Loss: 1.5114880800247192\n",
      "Epoch 360, Loss: 1.508941411972046\n",
      "Epoch 380, Loss: 1.5063624382019043\n",
      "Epoch 400, Loss: 1.503756046295166\n",
      "Epoch 420, Loss: 1.5011277198791504\n",
      "Epoch 440, Loss: 1.498482346534729\n",
      "Epoch 460, Loss: 1.4958245754241943\n",
      "Epoch 480, Loss: 1.4931590557098389\n",
      "Epoch 500, Loss: 1.490490198135376\n",
      "Epoch 520, Loss: 1.487822413444519\n",
      "Epoch 540, Loss: 1.4851595163345337\n",
      "Epoch 560, Loss: 1.4825057983398438\n",
      "Epoch 580, Loss: 1.4798649549484253\n",
      "Epoch 600, Loss: 1.4772406816482544\n",
      "Epoch 620, Loss: 1.4746360778808594\n",
      "Epoch 640, Loss: 1.4720553159713745\n",
      "Epoch 660, Loss: 1.469501256942749\n",
      "Epoch 680, Loss: 1.4669768810272217\n",
      "Epoch 700, Loss: 1.4644852876663208\n",
      "Epoch 720, Loss: 1.462029218673706\n",
      "Epoch 740, Loss: 1.4596115350723267\n",
      "Epoch 760, Loss: 1.4572343826293945\n",
      "Epoch 780, Loss: 1.4549005031585693\n",
      "Epoch 800, Loss: 1.4526124000549316\n",
      "Epoch 820, Loss: 1.4503719806671143\n",
      "Epoch 840, Loss: 1.4481812715530396\n",
      "Epoch 860, Loss: 1.4460417032241821\n",
      "Epoch 880, Loss: 1.443955898284912\n",
      "Epoch 900, Loss: 1.441925048828125\n",
      "Epoch 920, Loss: 1.4399508237838745\n",
      "Epoch 940, Loss: 1.4380342960357666\n",
      "Epoch 960, Loss: 1.4361770153045654\n",
      "Epoch 980, Loss: 1.4343805313110352\n",
      "weights of  P012 :  [[array([[-0.37356564,  0.32747763,  0.1417096 ],\n",
      "       [ 0.16912837, -0.9301583 , -0.4111683 ],\n",
      "       [-0.34043112, -0.8158578 , -0.27437487],\n",
      "       [-0.78731906, -0.24135482, -0.63869244],\n",
      "       [-0.01233302,  0.25317323, -0.27269483]], dtype=float32), array([ 0.29852304,  0.16206066, -0.2286628 ], dtype=float32)]]\n",
      "Lowest loss of  P012 :  1.4327304\n",
      "Epoch 0, Loss: 4.875522613525391\n",
      "Epoch 20, Loss: 2.7679922580718994\n",
      "Epoch 40, Loss: 1.8613814115524292\n",
      "Epoch 60, Loss: 1.3330239057540894\n",
      "Epoch 80, Loss: 1.3296828269958496\n",
      "Epoch 100, Loss: 1.321885108947754\n",
      "Epoch 120, Loss: 1.3205375671386719\n",
      "Epoch 140, Loss: 1.319533348083496\n",
      "Epoch 160, Loss: 1.3184813261032104\n",
      "Epoch 180, Loss: 1.3174444437026978\n",
      "Epoch 200, Loss: 1.3163880109786987\n",
      "Epoch 220, Loss: 1.3153290748596191\n",
      "Epoch 240, Loss: 1.314275860786438\n",
      "Epoch 260, Loss: 1.3132375478744507\n",
      "Epoch 280, Loss: 1.312223196029663\n",
      "Epoch 300, Loss: 1.3112410306930542\n",
      "Epoch 320, Loss: 1.3102996349334717\n",
      "Epoch 340, Loss: 1.3094062805175781\n",
      "Epoch 360, Loss: 1.3085687160491943\n",
      "Epoch 380, Loss: 1.3077937364578247\n",
      "Epoch 400, Loss: 1.3070887327194214\n",
      "Epoch 420, Loss: 1.306459665298462\n",
      "Epoch 440, Loss: 1.3059132099151611\n",
      "Epoch 460, Loss: 1.3054553270339966\n",
      "Epoch 480, Loss: 1.3050915002822876\n",
      "Epoch 500, Loss: 1.3048274517059326\n",
      "Epoch 520, Loss: 1.3046683073043823\n",
      "Epoch 540, Loss: 1.3046185970306396\n",
      "Epoch 560, Loss: 1.3046835660934448\n",
      "Epoch 580, Loss: 1.304867148399353\n",
      "Epoch 600, Loss: 1.305173635482788\n",
      "Epoch 620, Loss: 1.3056068420410156\n",
      "Epoch 640, Loss: 1.3061702251434326\n",
      "Epoch 660, Loss: 1.3068675994873047\n",
      "Epoch 680, Loss: 1.3077013492584229\n",
      "Epoch 700, Loss: 1.3086750507354736\n",
      "Epoch 720, Loss: 1.309791088104248\n",
      "Epoch 740, Loss: 1.3110514879226685\n",
      "Epoch 760, Loss: 1.3124587535858154\n",
      "Epoch 780, Loss: 1.3140150308609009\n",
      "Epoch 800, Loss: 1.3157216310501099\n",
      "Epoch 820, Loss: 1.3175803422927856\n",
      "Epoch 840, Loss: 1.3195921182632446\n",
      "Epoch 860, Loss: 1.3217583894729614\n",
      "Epoch 880, Loss: 1.3240798711776733\n",
      "Epoch 900, Loss: 1.3265572786331177\n",
      "Epoch 920, Loss: 1.3291912078857422\n",
      "Epoch 940, Loss: 1.331982135772705\n",
      "Epoch 960, Loss: 1.3349299430847168\n",
      "Epoch 980, Loss: 1.3380351066589355\n",
      "weights of  P013 :  [[array([[-0.4371041 , -0.30706462,  0.34778833],\n",
      "       [ 0.54541487,  0.620113  , -0.24734843],\n",
      "       [-0.7476568 , -0.04942694, -0.49172378],\n",
      "       [ 0.16327475, -0.21002175, -0.12238945],\n",
      "       [ 0.13397023, -0.40981057,  0.275469  ]], dtype=float32), array([-0.00103903, -0.36808768,  0.4932095 ], dtype=float32)]]\n",
      "Lowest loss of  P013 :  1.3046186\n",
      "Epoch 0, Loss: 5.687724590301514\n",
      "Epoch 20, Loss: 3.2289621829986572\n",
      "Epoch 40, Loss: 1.6027134656906128\n",
      "Epoch 60, Loss: 1.4854376316070557\n",
      "Epoch 80, Loss: 1.4684169292449951\n",
      "Epoch 100, Loss: 1.4661766290664673\n",
      "Epoch 120, Loss: 1.465156078338623\n",
      "Epoch 140, Loss: 1.4645792245864868\n",
      "Epoch 160, Loss: 1.463883399963379\n",
      "Epoch 180, Loss: 1.4632279872894287\n",
      "Epoch 200, Loss: 1.4625440835952759\n",
      "Epoch 220, Loss: 1.4618507623672485\n",
      "Epoch 240, Loss: 1.4611563682556152\n",
      "Epoch 260, Loss: 1.460463285446167\n",
      "Epoch 280, Loss: 1.4597781896591187\n",
      "Epoch 300, Loss: 1.4591059684753418\n",
      "Epoch 320, Loss: 1.4584518671035767\n",
      "Epoch 340, Loss: 1.4578208923339844\n",
      "Epoch 360, Loss: 1.4572175741195679\n",
      "Epoch 380, Loss: 1.4566470384597778\n",
      "Epoch 400, Loss: 1.4561139345169067\n",
      "Epoch 420, Loss: 1.4556231498718262\n",
      "Epoch 440, Loss: 1.4551790952682495\n",
      "Epoch 460, Loss: 1.4547865390777588\n",
      "Epoch 480, Loss: 1.4544497728347778\n",
      "Epoch 500, Loss: 1.45417320728302\n",
      "Epoch 520, Loss: 1.4539612531661987\n",
      "Epoch 540, Loss: 1.4538183212280273\n",
      "Epoch 560, Loss: 1.4537490606307983\n",
      "Epoch 580, Loss: 1.4537569284439087\n",
      "Epoch 600, Loss: 1.4538466930389404\n",
      "Epoch 620, Loss: 1.4540224075317383\n",
      "Epoch 640, Loss: 1.4542880058288574\n",
      "Epoch 660, Loss: 1.454647421836853\n",
      "Epoch 680, Loss: 1.4551043510437012\n",
      "Epoch 700, Loss: 1.4556634426116943\n",
      "Epoch 720, Loss: 1.4563276767730713\n",
      "Epoch 740, Loss: 1.4571011066436768\n",
      "Epoch 760, Loss: 1.4579875469207764\n",
      "Epoch 780, Loss: 1.458990454673767\n",
      "Epoch 800, Loss: 1.460113525390625\n",
      "Epoch 820, Loss: 1.461359977722168\n",
      "Epoch 840, Loss: 1.4627333879470825\n",
      "Epoch 860, Loss: 1.4642366170883179\n",
      "Epoch 880, Loss: 1.4658737182617188\n",
      "Epoch 900, Loss: 1.4676470756530762\n",
      "Epoch 920, Loss: 1.469560146331787\n",
      "Epoch 940, Loss: 1.4716160297393799\n",
      "Epoch 960, Loss: 1.473817229270935\n",
      "Epoch 980, Loss: 1.4761672019958496\n",
      "weights of  P014 :  [[array([[-0.29577538, -0.34177038, -0.4321008 ],\n",
      "       [-0.7037438 ,  0.12477196,  0.45859575],\n",
      "       [-0.9255806 , -0.40074372, -0.4980595 ],\n",
      "       [ 0.15976329, -0.24293971,  0.575615  ],\n",
      "       [ 0.51973486, -0.31969696, -0.85979956]], dtype=float32), array([-0.02834491, -0.27954802,  0.52211744], dtype=float32)]]\n",
      "Lowest loss of  P014 :  1.4537425\n",
      "Epoch 0, Loss: 3.2165634632110596\n",
      "Epoch 20, Loss: 1.598877191543579\n",
      "Epoch 40, Loss: 1.4511851072311401\n",
      "Epoch 60, Loss: 1.4324427843093872\n",
      "Epoch 80, Loss: 1.4309521913528442\n",
      "Epoch 100, Loss: 1.42825448513031\n",
      "Epoch 120, Loss: 1.425977110862732\n",
      "Epoch 140, Loss: 1.4235236644744873\n",
      "Epoch 160, Loss: 1.421093463897705\n",
      "Epoch 180, Loss: 1.4186575412750244\n",
      "Epoch 200, Loss: 1.416278600692749\n",
      "Epoch 220, Loss: 1.4139875173568726\n",
      "Epoch 240, Loss: 1.4118224382400513\n",
      "Epoch 260, Loss: 1.4098188877105713\n",
      "Epoch 280, Loss: 1.408010721206665\n",
      "Epoch 300, Loss: 1.4064308404922485\n",
      "Epoch 320, Loss: 1.4051117897033691\n",
      "Epoch 340, Loss: 1.4040846824645996\n",
      "Epoch 360, Loss: 1.403380036354065\n",
      "Epoch 380, Loss: 1.403027057647705\n",
      "Epoch 400, Loss: 1.40305495262146\n",
      "Epoch 420, Loss: 1.4034916162490845\n",
      "Epoch 440, Loss: 1.404363989830017\n",
      "Epoch 460, Loss: 1.405698299407959\n",
      "Epoch 480, Loss: 1.4075202941894531\n",
      "Epoch 500, Loss: 1.409854769706726\n",
      "Epoch 520, Loss: 1.4127249717712402\n",
      "Epoch 540, Loss: 1.4161546230316162\n",
      "Epoch 560, Loss: 1.4201654195785522\n",
      "Epoch 580, Loss: 1.424778699874878\n",
      "Epoch 600, Loss: 1.4300153255462646\n",
      "Epoch 620, Loss: 1.4358947277069092\n",
      "Epoch 640, Loss: 1.4424357414245605\n",
      "Epoch 660, Loss: 1.4496562480926514\n",
      "Epoch 680, Loss: 1.457573413848877\n",
      "Epoch 700, Loss: 1.4662036895751953\n",
      "Epoch 720, Loss: 1.4755619764328003\n",
      "Epoch 740, Loss: 1.4856634140014648\n",
      "Epoch 760, Loss: 1.496521234512329\n",
      "Epoch 780, Loss: 1.5081485509872437\n",
      "Epoch 800, Loss: 1.5205577611923218\n",
      "Epoch 820, Loss: 1.5337598323822021\n",
      "Epoch 840, Loss: 1.5477653741836548\n",
      "Epoch 860, Loss: 1.5625839233398438\n",
      "Epoch 880, Loss: 1.5782251358032227\n",
      "Epoch 900, Loss: 1.594696283340454\n",
      "Epoch 920, Loss: 1.6120052337646484\n",
      "Epoch 940, Loss: 1.6301591396331787\n",
      "Epoch 960, Loss: 1.6491637229919434\n",
      "Epoch 980, Loss: 1.6690242290496826\n",
      "weights of  P015 :  [[array([[ 0.37821707,  0.3229325 , -0.05222521],\n",
      "       [ 0.2957745 ,  0.21128763,  0.8383691 ],\n",
      "       [ 0.43287095, -0.74394536, -0.25930563],\n",
      "       [-0.65487754,  0.09070352,  0.7306071 ],\n",
      "       [ 0.24417035,  0.73389935, -0.22721985]], dtype=float32), array([ 0.01231763, -0.18936834,  0.4166057 ], dtype=float32)]]\n",
      "Lowest loss of  P015 :  1.4029908\n",
      "Epoch 0, Loss: 4.127416610717773\n",
      "Epoch 20, Loss: 2.8973710536956787\n",
      "Epoch 40, Loss: 1.7611156702041626\n",
      "Epoch 60, Loss: 1.4991735219955444\n",
      "Epoch 80, Loss: 1.4721544981002808\n",
      "Epoch 100, Loss: 1.4673494100570679\n",
      "Epoch 120, Loss: 1.4621517658233643\n",
      "Epoch 140, Loss: 1.4564021825790405\n",
      "Epoch 160, Loss: 1.450569748878479\n",
      "Epoch 180, Loss: 1.444461464881897\n",
      "Epoch 200, Loss: 1.4381344318389893\n",
      "Epoch 220, Loss: 1.4316210746765137\n",
      "Epoch 240, Loss: 1.4249606132507324\n",
      "Epoch 260, Loss: 1.4181854724884033\n",
      "Epoch 280, Loss: 1.4113258123397827\n",
      "Epoch 300, Loss: 1.4044109582901\n",
      "Epoch 320, Loss: 1.3974676132202148\n",
      "Epoch 340, Loss: 1.3905208110809326\n",
      "Epoch 360, Loss: 1.383595585823059\n",
      "Epoch 380, Loss: 1.3767139911651611\n",
      "Epoch 400, Loss: 1.369897484779358\n",
      "Epoch 420, Loss: 1.363166332244873\n",
      "Epoch 440, Loss: 1.3565398454666138\n",
      "Epoch 460, Loss: 1.3500362634658813\n",
      "Epoch 480, Loss: 1.343672513961792\n",
      "Epoch 500, Loss: 1.3374643325805664\n",
      "Epoch 520, Loss: 1.3314272165298462\n",
      "Epoch 540, Loss: 1.3255751132965088\n",
      "Epoch 560, Loss: 1.319921612739563\n",
      "Epoch 580, Loss: 1.3144789934158325\n",
      "Epoch 600, Loss: 1.3092585802078247\n",
      "Epoch 620, Loss: 1.3042718172073364\n",
      "Epoch 640, Loss: 1.2995284795761108\n",
      "Epoch 660, Loss: 1.295037865638733\n",
      "Epoch 680, Loss: 1.2908085584640503\n",
      "Epoch 700, Loss: 1.2868486642837524\n",
      "Epoch 720, Loss: 1.2831655740737915\n",
      "Epoch 740, Loss: 1.2797657251358032\n",
      "Epoch 760, Loss: 1.2766555547714233\n",
      "Epoch 780, Loss: 1.2738404273986816\n",
      "Epoch 800, Loss: 1.2713251113891602\n",
      "Epoch 820, Loss: 1.2691148519515991\n",
      "Epoch 840, Loss: 1.2672126293182373\n",
      "Epoch 860, Loss: 1.2656230926513672\n",
      "Epoch 880, Loss: 1.2643487453460693\n",
      "Epoch 900, Loss: 1.2633925676345825\n",
      "Epoch 920, Loss: 1.2627567052841187\n",
      "Epoch 940, Loss: 1.2624434232711792\n",
      "Epoch 960, Loss: 1.2624540328979492\n",
      "Epoch 980, Loss: 1.2627904415130615\n",
      "weights of  P016 :  [[array([[-0.64463073,  0.19483447,  0.5266015 ],\n",
      "       [-0.15211986,  0.08279385,  0.122932  ],\n",
      "       [ 0.5068626 , -0.43830344,  0.2278906 ],\n",
      "       [ 0.80726135, -0.09882645,  0.06963789],\n",
      "       [-0.05289584,  0.4488798 ,  0.04921785]], dtype=float32), array([-0.865265 , -1.3686631,  1.0426928], dtype=float32)]]\n",
      "Lowest loss of  P016 :  1.262408\n",
      "Epoch 0, Loss: 4.589792728424072\n",
      "Epoch 20, Loss: 2.475684881210327\n",
      "Epoch 40, Loss: 1.7156853675842285\n",
      "Epoch 60, Loss: 1.5903172492980957\n",
      "Epoch 80, Loss: 1.5731196403503418\n",
      "Epoch 100, Loss: 1.570974349975586\n",
      "Epoch 120, Loss: 1.5696922540664673\n",
      "Epoch 140, Loss: 1.568298101425171\n",
      "Epoch 160, Loss: 1.5668299198150635\n",
      "Epoch 180, Loss: 1.5653016567230225\n",
      "Epoch 200, Loss: 1.5636959075927734\n",
      "Epoch 220, Loss: 1.562032699584961\n",
      "Epoch 240, Loss: 1.560314655303955\n",
      "Epoch 260, Loss: 1.558549404144287\n",
      "Epoch 280, Loss: 1.5567419528961182\n",
      "Epoch 300, Loss: 1.5548977851867676\n",
      "Epoch 320, Loss: 1.5530214309692383\n",
      "Epoch 340, Loss: 1.5511176586151123\n",
      "Epoch 360, Loss: 1.5491902828216553\n",
      "Epoch 380, Loss: 1.5472431182861328\n",
      "Epoch 400, Loss: 1.5452792644500732\n",
      "Epoch 420, Loss: 1.543302059173584\n",
      "Epoch 440, Loss: 1.5413144826889038\n",
      "Epoch 460, Loss: 1.5393187999725342\n",
      "Epoch 480, Loss: 1.5373176336288452\n",
      "Epoch 500, Loss: 1.535313367843628\n",
      "Epoch 520, Loss: 1.5333070755004883\n",
      "Epoch 540, Loss: 1.5313011407852173\n",
      "Epoch 560, Loss: 1.529297113418579\n",
      "Epoch 580, Loss: 1.5272963047027588\n",
      "Epoch 600, Loss: 1.5253000259399414\n",
      "Epoch 620, Loss: 1.5233091115951538\n",
      "Epoch 640, Loss: 1.5213243961334229\n",
      "Epoch 660, Loss: 1.519347071647644\n",
      "Epoch 680, Loss: 1.5173777341842651\n",
      "Epoch 700, Loss: 1.5154168605804443\n",
      "Epoch 720, Loss: 1.5134649276733398\n",
      "Epoch 740, Loss: 1.5115224123001099\n",
      "Epoch 760, Loss: 1.5095895528793335\n",
      "Epoch 780, Loss: 1.507666826248169\n",
      "Epoch 800, Loss: 1.5057543516159058\n",
      "Epoch 820, Loss: 1.5038522481918335\n",
      "Epoch 840, Loss: 1.5019607543945312\n",
      "Epoch 860, Loss: 1.500079870223999\n",
      "Epoch 880, Loss: 1.4982099533081055\n",
      "Epoch 900, Loss: 1.496350646018982\n",
      "Epoch 920, Loss: 1.4945024251937866\n",
      "Epoch 940, Loss: 1.4926650524139404\n",
      "Epoch 960, Loss: 1.490838885307312\n",
      "Epoch 980, Loss: 1.4890239238739014\n",
      "weights of  P017 :  [[array([[ 0.23691347,  0.12828206,  0.7621686 ],\n",
      "       [-0.51330453, -0.3688826 , -0.57045287],\n",
      "       [-0.5776271 ,  0.14886744, -0.703993  ],\n",
      "       [-0.64404225, -0.33962727,  0.46837002],\n",
      "       [ 0.5436103 , -0.5203299 , -0.5750374 ]], dtype=float32), array([ 0.38787845, -0.44262153,  0.36445925], dtype=float32)]]\n",
      "Lowest loss of  P017 :  1.4873102\n",
      "Epoch 0, Loss: 4.735158443450928\n",
      "Epoch 20, Loss: 2.5693728923797607\n",
      "Epoch 40, Loss: 1.746290922164917\n",
      "Epoch 60, Loss: 1.4866676330566406\n",
      "Epoch 80, Loss: 1.479665756225586\n",
      "Epoch 100, Loss: 1.4780783653259277\n",
      "Epoch 120, Loss: 1.4771478176116943\n",
      "Epoch 140, Loss: 1.4764388799667358\n",
      "Epoch 160, Loss: 1.475709319114685\n",
      "Epoch 180, Loss: 1.4749398231506348\n",
      "Epoch 200, Loss: 1.4741337299346924\n",
      "Epoch 220, Loss: 1.4732922315597534\n",
      "Epoch 240, Loss: 1.4724187850952148\n",
      "Epoch 260, Loss: 1.4715158939361572\n",
      "Epoch 280, Loss: 1.4705862998962402\n",
      "Epoch 300, Loss: 1.469632863998413\n",
      "Epoch 320, Loss: 1.4686579704284668\n",
      "Epoch 340, Loss: 1.4676636457443237\n",
      "Epoch 360, Loss: 1.4666520357131958\n",
      "Epoch 380, Loss: 1.4656258821487427\n",
      "Epoch 400, Loss: 1.464586853981018\n",
      "Epoch 420, Loss: 1.4635374546051025\n",
      "Epoch 440, Loss: 1.4624793529510498\n",
      "Epoch 460, Loss: 1.4614145755767822\n",
      "Epoch 480, Loss: 1.4603455066680908\n",
      "Epoch 500, Loss: 1.4592738151550293\n",
      "Epoch 520, Loss: 1.458201289176941\n",
      "Epoch 540, Loss: 1.4571300745010376\n",
      "Epoch 560, Loss: 1.4560621976852417\n",
      "Epoch 580, Loss: 1.4549994468688965\n",
      "Epoch 600, Loss: 1.453943133354187\n",
      "Epoch 620, Loss: 1.4528956413269043\n",
      "Epoch 640, Loss: 1.4518588781356812\n",
      "Epoch 660, Loss: 1.4508345127105713\n",
      "Epoch 680, Loss: 1.449824333190918\n",
      "Epoch 700, Loss: 1.4488301277160645\n",
      "Epoch 720, Loss: 1.447853684425354\n",
      "Epoch 740, Loss: 1.4468969106674194\n",
      "Epoch 760, Loss: 1.4459612369537354\n",
      "Epoch 780, Loss: 1.4450488090515137\n",
      "Epoch 800, Loss: 1.4441611766815186\n",
      "Epoch 820, Loss: 1.4432998895645142\n",
      "Epoch 840, Loss: 1.4424669742584229\n",
      "Epoch 860, Loss: 1.441664218902588\n",
      "Epoch 880, Loss: 1.4408930540084839\n",
      "Epoch 900, Loss: 1.440155267715454\n",
      "Epoch 920, Loss: 1.4394527673721313\n",
      "Epoch 940, Loss: 1.4387867450714111\n",
      "Epoch 960, Loss: 1.4381592273712158\n",
      "Epoch 980, Loss: 1.43757164478302\n",
      "weights of  P018 :  [[array([[-0.14004667, -0.1467308 ,  0.35595965],\n",
      "       [-0.69765943, -0.43468565, -0.73622346],\n",
      "       [ 0.52084094,  0.6871059 ,  0.64731413],\n",
      "       [ 0.43055943,  0.5457153 , -0.02560114],\n",
      "       [-0.04330934, -0.5899174 , -0.11575175]], dtype=float32), array([ 0.15058252, -0.28538686,  0.39924785], dtype=float32)]]\n",
      "Lowest loss of  P018 :  1.4370522\n",
      "Epoch 0, Loss: 2.6790685653686523\n",
      "Epoch 20, Loss: 1.6271507740020752\n",
      "Epoch 40, Loss: 1.5821940898895264\n",
      "Epoch 60, Loss: 1.5641928911209106\n",
      "Epoch 80, Loss: 1.5554261207580566\n",
      "Epoch 100, Loss: 1.549229621887207\n",
      "Epoch 120, Loss: 1.5425513982772827\n",
      "Epoch 140, Loss: 1.5356941223144531\n",
      "Epoch 160, Loss: 1.5285930633544922\n",
      "Epoch 180, Loss: 1.5213541984558105\n",
      "Epoch 200, Loss: 1.5140621662139893\n",
      "Epoch 220, Loss: 1.5067849159240723\n",
      "Epoch 240, Loss: 1.4995920658111572\n",
      "Epoch 260, Loss: 1.4925469160079956\n",
      "Epoch 280, Loss: 1.4857087135314941\n",
      "Epoch 300, Loss: 1.4791325330734253\n",
      "Epoch 320, Loss: 1.4728707075119019\n",
      "Epoch 340, Loss: 1.4669723510742188\n",
      "Epoch 360, Loss: 1.4614828824996948\n",
      "Epoch 380, Loss: 1.4564458131790161\n",
      "Epoch 400, Loss: 1.4519010782241821\n",
      "Epoch 420, Loss: 1.4478864669799805\n",
      "Epoch 440, Loss: 1.4444366693496704\n",
      "Epoch 460, Loss: 1.4415844678878784\n",
      "Epoch 480, Loss: 1.439360499382019\n",
      "Epoch 500, Loss: 1.4377920627593994\n",
      "Epoch 520, Loss: 1.4369053840637207\n",
      "Epoch 540, Loss: 1.4367237091064453\n",
      "Epoch 560, Loss: 1.437268614768982\n",
      "Epoch 580, Loss: 1.438559889793396\n",
      "Epoch 600, Loss: 1.4406147003173828\n",
      "Epoch 620, Loss: 1.443448543548584\n",
      "Epoch 640, Loss: 1.4470757246017456\n",
      "Epoch 660, Loss: 1.4515082836151123\n",
      "Epoch 680, Loss: 1.456756830215454\n",
      "Epoch 700, Loss: 1.4628301858901978\n",
      "Epoch 720, Loss: 1.469735860824585\n",
      "Epoch 740, Loss: 1.4774799346923828\n",
      "Epoch 760, Loss: 1.486066222190857\n",
      "Epoch 780, Loss: 1.4954986572265625\n",
      "Epoch 800, Loss: 1.5057796239852905\n",
      "Epoch 820, Loss: 1.5169093608856201\n",
      "Epoch 840, Loss: 1.5288875102996826\n",
      "Epoch 860, Loss: 1.541712999343872\n",
      "Epoch 880, Loss: 1.5553834438323975\n",
      "Epoch 900, Loss: 1.5698959827423096\n",
      "Epoch 920, Loss: 1.585245966911316\n",
      "Epoch 940, Loss: 1.601428747177124\n",
      "Epoch 960, Loss: 1.618438482284546\n",
      "Epoch 980, Loss: 1.6362686157226562\n",
      "weights of  P019 :  [[array([[ 0.06257956, -0.24718346,  0.04825475],\n",
      "       [-0.30328032,  0.12077484,  1.1109666 ],\n",
      "       [-0.43534148, -0.45234177, -0.33507195],\n",
      "       [-0.744275  , -0.07199575, -0.6942221 ],\n",
      "       [ 0.4010235 , -0.31840977, -0.7545164 ]], dtype=float32), array([ 0.0093112 , -0.5429115 ,  0.44243747], dtype=float32)]]\n",
      "Lowest loss of  P019 :  1.436702\n",
      "Epoch 0, Loss: 7.421858310699463\n",
      "Epoch 20, Loss: 4.98765754699707\n",
      "Epoch 40, Loss: 3.680891990661621\n",
      "Epoch 60, Loss: 2.6898844242095947\n",
      "Epoch 80, Loss: 1.8023995161056519\n",
      "Epoch 100, Loss: 1.5104373693466187\n",
      "Epoch 120, Loss: 1.5091092586517334\n",
      "Epoch 140, Loss: 1.5016210079193115\n",
      "Epoch 160, Loss: 1.496756911277771\n",
      "Epoch 180, Loss: 1.4920835494995117\n",
      "Epoch 200, Loss: 1.4871031045913696\n",
      "Epoch 220, Loss: 1.4819685220718384\n",
      "Epoch 240, Loss: 1.4766606092453003\n",
      "Epoch 260, Loss: 1.4712036848068237\n",
      "Epoch 280, Loss: 1.4656177759170532\n",
      "Epoch 300, Loss: 1.4599210023880005\n",
      "Epoch 320, Loss: 1.4541308879852295\n",
      "Epoch 340, Loss: 1.4482641220092773\n",
      "Epoch 360, Loss: 1.4423363208770752\n",
      "Epoch 380, Loss: 1.4363634586334229\n",
      "Epoch 400, Loss: 1.4303598403930664\n",
      "Epoch 420, Loss: 1.4243396520614624\n",
      "Epoch 440, Loss: 1.4183170795440674\n",
      "Epoch 460, Loss: 1.4123049974441528\n",
      "Epoch 480, Loss: 1.4063163995742798\n",
      "Epoch 500, Loss: 1.4003639221191406\n",
      "Epoch 520, Loss: 1.3944590091705322\n",
      "Epoch 540, Loss: 1.388613224029541\n",
      "Epoch 560, Loss: 1.3828375339508057\n",
      "Epoch 580, Loss: 1.3771427869796753\n",
      "Epoch 600, Loss: 1.371538758277893\n",
      "Epoch 620, Loss: 1.3660354614257812\n",
      "Epoch 640, Loss: 1.3606420755386353\n",
      "Epoch 660, Loss: 1.3553671836853027\n",
      "Epoch 680, Loss: 1.350219488143921\n",
      "Epoch 700, Loss: 1.3452067375183105\n",
      "Epoch 720, Loss: 1.3403360843658447\n",
      "Epoch 740, Loss: 1.3356150388717651\n",
      "Epoch 760, Loss: 1.3310495615005493\n",
      "Epoch 780, Loss: 1.326646327972412\n",
      "Epoch 800, Loss: 1.3224103450775146\n",
      "Epoch 820, Loss: 1.3183475732803345\n",
      "Epoch 840, Loss: 1.3144619464874268\n",
      "Epoch 860, Loss: 1.3107582330703735\n",
      "Epoch 880, Loss: 1.3072395324707031\n",
      "Epoch 900, Loss: 1.3039100170135498\n",
      "Epoch 920, Loss: 1.3007718324661255\n",
      "Epoch 940, Loss: 1.297827959060669\n",
      "Epoch 960, Loss: 1.2950804233551025\n",
      "Epoch 980, Loss: 1.2925305366516113\n",
      "weights of  P020 :  [[array([[-0.07370786, -0.46883008, -0.09979287],\n",
      "       [-0.07397386, -0.14042157, -0.23593733],\n",
      "       [ 0.13312702, -0.20740657,  0.03050139],\n",
      "       [-0.98192525, -0.2398937 ,  0.14342712],\n",
      "       [-0.3564918 , -0.42600283, -0.6928111 ]], dtype=float32), array([ 0.09035043, -0.6223911 ,  1.2072897 ], dtype=float32)]]\n",
      "Lowest loss of  P020 :  1.2902923\n",
      "Epoch 0, Loss: 2.2115659713745117\n",
      "Epoch 20, Loss: 1.3503398895263672\n",
      "Epoch 40, Loss: 1.3564848899841309\n",
      "Epoch 60, Loss: 1.3493105173110962\n",
      "Epoch 80, Loss: 1.3509047031402588\n",
      "Epoch 100, Loss: 1.3548747301101685\n",
      "Epoch 120, Loss: 1.3599644899368286\n",
      "Epoch 140, Loss: 1.365968108177185\n",
      "Epoch 160, Loss: 1.3728218078613281\n",
      "Epoch 180, Loss: 1.3804545402526855\n",
      "Epoch 200, Loss: 1.3887907266616821\n",
      "Epoch 220, Loss: 1.3977503776550293\n",
      "Epoch 240, Loss: 1.4072551727294922\n",
      "Epoch 260, Loss: 1.4172307252883911\n",
      "Epoch 280, Loss: 1.4276100397109985\n",
      "Epoch 300, Loss: 1.4383341073989868\n",
      "Epoch 320, Loss: 1.4493528604507446\n",
      "Epoch 340, Loss: 1.46062433719635\n",
      "Epoch 360, Loss: 1.47211492061615\n",
      "Epoch 380, Loss: 1.4837987422943115\n",
      "Epoch 400, Loss: 1.4956543445587158\n",
      "Epoch 420, Loss: 1.5076656341552734\n",
      "Epoch 440, Loss: 1.519819974899292\n",
      "Epoch 460, Loss: 1.5321071147918701\n",
      "Epoch 480, Loss: 1.544517159461975\n",
      "Epoch 500, Loss: 1.5570416450500488\n",
      "Epoch 520, Loss: 1.5696711540222168\n",
      "Epoch 540, Loss: 1.5823956727981567\n",
      "Epoch 560, Loss: 1.5952041149139404\n",
      "Epoch 580, Loss: 1.6080842018127441\n",
      "Epoch 600, Loss: 1.6210227012634277\n",
      "Epoch 620, Loss: 1.6340043544769287\n",
      "Epoch 640, Loss: 1.6470134258270264\n",
      "Epoch 660, Loss: 1.6600334644317627\n",
      "Epoch 680, Loss: 1.6730470657348633\n",
      "Epoch 700, Loss: 1.6860361099243164\n",
      "Epoch 720, Loss: 1.6989834308624268\n",
      "Epoch 740, Loss: 1.71187162399292\n",
      "Epoch 760, Loss: 1.7246828079223633\n",
      "Epoch 780, Loss: 1.7374012470245361\n",
      "Epoch 800, Loss: 1.7500112056732178\n",
      "Epoch 820, Loss: 1.7624982595443726\n",
      "Epoch 840, Loss: 1.7748494148254395\n",
      "Epoch 860, Loss: 1.787052869796753\n",
      "Epoch 880, Loss: 1.7990981340408325\n",
      "Epoch 900, Loss: 1.8109761476516724\n",
      "Epoch 920, Loss: 1.8226795196533203\n",
      "Epoch 940, Loss: 1.8342030048370361\n",
      "Epoch 960, Loss: 1.8455419540405273\n",
      "Epoch 980, Loss: 1.8566930294036865\n",
      "weights of  P021 :  [[array([[-0.8043232 ,  0.03663287, -0.59240913],\n",
      "       [-0.10627753,  0.26976246,  0.5371015 ],\n",
      "       [-0.09293151, -0.71583956, -0.02644187],\n",
      "       [ 0.56977284,  0.28551784, -0.02946669],\n",
      "       [-0.13106842, -0.32173926, -0.36704677]], dtype=float32), array([-0.08077249, -0.10532428,  0.10426412], dtype=float32)]]\n",
      "Lowest loss of  P021 :  1.3435682\n",
      "Epoch 0, Loss: 1.794880747795105\n",
      "Epoch 20, Loss: 1.4431771039962769\n",
      "Epoch 40, Loss: 1.3893038034439087\n",
      "Epoch 60, Loss: 1.3596301078796387\n",
      "Epoch 80, Loss: 1.33864426612854\n",
      "Epoch 100, Loss: 1.324846625328064\n",
      "Epoch 120, Loss: 1.317315697669983\n",
      "Epoch 140, Loss: 1.315506100654602\n",
      "Epoch 160, Loss: 1.3189109563827515\n",
      "Epoch 180, Loss: 1.3270254135131836\n",
      "Epoch 200, Loss: 1.339375615119934\n",
      "Epoch 220, Loss: 1.3555246591567993\n",
      "Epoch 240, Loss: 1.3750742673873901\n",
      "Epoch 260, Loss: 1.397666335105896\n",
      "Epoch 280, Loss: 1.4229834079742432\n",
      "Epoch 300, Loss: 1.4507472515106201\n",
      "Epoch 320, Loss: 1.480717658996582\n",
      "Epoch 340, Loss: 1.5126904249191284\n",
      "Epoch 360, Loss: 1.5464938879013062\n",
      "Epoch 380, Loss: 1.581986904144287\n",
      "Epoch 400, Loss: 1.6190552711486816\n",
      "Epoch 420, Loss: 1.6576086282730103\n",
      "Epoch 440, Loss: 1.6975774765014648\n",
      "Epoch 460, Loss: 1.7389085292816162\n",
      "Epoch 480, Loss: 1.7815632820129395\n",
      "Epoch 500, Loss: 1.8255138397216797\n",
      "Epoch 520, Loss: 1.8707401752471924\n",
      "Epoch 540, Loss: 1.917229175567627\n",
      "Epoch 560, Loss: 1.9649711847305298\n",
      "Epoch 580, Loss: 2.013957977294922\n",
      "Epoch 600, Loss: 2.064182758331299\n",
      "Epoch 620, Loss: 2.1156373023986816\n",
      "Epoch 640, Loss: 2.1683127880096436\n",
      "Epoch 660, Loss: 2.2221975326538086\n",
      "Epoch 680, Loss: 2.2772769927978516\n",
      "Epoch 700, Loss: 2.3335347175598145\n",
      "Epoch 720, Loss: 2.3909504413604736\n",
      "Epoch 740, Loss: 2.4495010375976562\n",
      "Epoch 760, Loss: 2.509161949157715\n",
      "Epoch 780, Loss: 2.569903612136841\n",
      "Epoch 800, Loss: 2.6316967010498047\n",
      "Epoch 820, Loss: 2.6945064067840576\n",
      "Epoch 840, Loss: 2.7582998275756836\n",
      "Epoch 860, Loss: 2.8230397701263428\n",
      "Epoch 880, Loss: 2.8886895179748535\n",
      "Epoch 900, Loss: 2.955209732055664\n",
      "Epoch 920, Loss: 3.022562026977539\n",
      "Epoch 940, Loss: 3.0907058715820312\n",
      "Epoch 960, Loss: 3.159602642059326\n",
      "Epoch 980, Loss: 3.22921085357666\n",
      "weights of  P022 :  [[array([[-0.01703988, -0.04938707,  0.7518571 ],\n",
      "       [ 0.32663903,  0.07454006,  0.5271936 ],\n",
      "       [-0.14034018,  0.4965102 ,  0.42977226],\n",
      "       [-0.47662953, -0.1653391 , -0.52625376],\n",
      "       [ 0.82800686,  0.32924625, -0.3325817 ]], dtype=float32), array([-0.19060287,  0.00831366, -0.00305182], dtype=float32)]]\n",
      "Lowest loss of  P022 :  1.315432\n",
      "Epoch 0, Loss: 4.785697937011719\n",
      "Epoch 20, Loss: 3.1431448459625244\n",
      "Epoch 40, Loss: 2.1240382194519043\n",
      "Epoch 60, Loss: 1.3404686450958252\n",
      "Epoch 80, Loss: 1.2957346439361572\n",
      "Epoch 100, Loss: 1.2774800062179565\n",
      "Epoch 120, Loss: 1.273903727531433\n",
      "Epoch 140, Loss: 1.2702217102050781\n",
      "Epoch 160, Loss: 1.2665024995803833\n",
      "Epoch 180, Loss: 1.2626909017562866\n",
      "Epoch 200, Loss: 1.2588132619857788\n",
      "Epoch 220, Loss: 1.2548983097076416\n",
      "Epoch 240, Loss: 1.2509770393371582\n",
      "Epoch 260, Loss: 1.247079610824585\n",
      "Epoch 280, Loss: 1.2432336807250977\n",
      "Epoch 300, Loss: 1.2394657135009766\n",
      "Epoch 320, Loss: 1.23580002784729\n",
      "Epoch 340, Loss: 1.2322603464126587\n",
      "Epoch 360, Loss: 1.2288689613342285\n",
      "Epoch 380, Loss: 1.2256461381912231\n",
      "Epoch 400, Loss: 1.222611427307129\n",
      "Epoch 420, Loss: 1.2197834253311157\n",
      "Epoch 440, Loss: 1.2171789407730103\n",
      "Epoch 460, Loss: 1.2148138284683228\n",
      "Epoch 480, Loss: 1.212702989578247\n",
      "Epoch 500, Loss: 1.210859775543213\n",
      "Epoch 520, Loss: 1.2092969417572021\n",
      "Epoch 540, Loss: 1.2080258131027222\n",
      "Epoch 560, Loss: 1.207056999206543\n",
      "Epoch 580, Loss: 1.2063994407653809\n",
      "Epoch 600, Loss: 1.206061840057373\n",
      "Epoch 620, Loss: 1.2060511112213135\n",
      "Epoch 640, Loss: 1.2063742876052856\n",
      "Epoch 660, Loss: 1.2070367336273193\n",
      "Epoch 680, Loss: 1.208042860031128\n",
      "Epoch 700, Loss: 1.2093966007232666\n",
      "Epoch 720, Loss: 1.2111011743545532\n",
      "Epoch 740, Loss: 1.2131587266921997\n",
      "Epoch 760, Loss: 1.2155708074569702\n",
      "Epoch 780, Loss: 1.2183386087417603\n",
      "Epoch 800, Loss: 1.2214621305465698\n",
      "Epoch 820, Loss: 1.2249411344528198\n",
      "Epoch 840, Loss: 1.2287747859954834\n",
      "Epoch 860, Loss: 1.2329620122909546\n",
      "Epoch 880, Loss: 1.237500548362732\n",
      "Epoch 900, Loss: 1.2423882484436035\n",
      "Epoch 920, Loss: 1.2476224899291992\n",
      "Epoch 940, Loss: 1.2532005310058594\n",
      "Epoch 960, Loss: 1.2591187953948975\n",
      "Epoch 980, Loss: 1.2653734683990479\n",
      "weights of  P023 :  [[array([[-0.16510394,  0.52997446,  0.08863807],\n",
      "       [-0.8021975 ,  0.0470137 ,  0.09617015],\n",
      "       [ 0.4233017 , -0.3836626 ,  0.29670966],\n",
      "       [ 0.0074811 , -0.41543695,  0.08174557],\n",
      "       [ 0.30158377, -0.04787559, -0.3801854 ]], dtype=float32), array([ 0.00540566, -0.40027094,  0.86683154], dtype=float32)]]\n",
      "Lowest loss of  P023 :  1.206015\n",
      "Epoch 0, Loss: 5.510473728179932\n",
      "Epoch 20, Loss: 3.659454345703125\n",
      "Epoch 40, Loss: 2.692603826522827\n",
      "Epoch 60, Loss: 1.762279987335205\n",
      "Epoch 80, Loss: 1.4652490615844727\n",
      "Epoch 100, Loss: 1.4621022939682007\n",
      "Epoch 120, Loss: 1.4592351913452148\n",
      "Epoch 140, Loss: 1.4574379920959473\n",
      "Epoch 160, Loss: 1.456197738647461\n",
      "Epoch 180, Loss: 1.4549275636672974\n",
      "Epoch 200, Loss: 1.4535757303237915\n",
      "Epoch 220, Loss: 1.4521969556808472\n",
      "Epoch 240, Loss: 1.450780987739563\n",
      "Epoch 260, Loss: 1.4493391513824463\n",
      "Epoch 280, Loss: 1.4478778839111328\n",
      "Epoch 300, Loss: 1.4464035034179688\n",
      "Epoch 320, Loss: 1.4449230432510376\n",
      "Epoch 340, Loss: 1.4434423446655273\n",
      "Epoch 360, Loss: 1.441967487335205\n",
      "Epoch 380, Loss: 1.4405043125152588\n",
      "Epoch 400, Loss: 1.4390588998794556\n",
      "Epoch 420, Loss: 1.4376368522644043\n",
      "Epoch 440, Loss: 1.4362438917160034\n",
      "Epoch 460, Loss: 1.4348855018615723\n",
      "Epoch 480, Loss: 1.433566927909851\n",
      "Epoch 500, Loss: 1.4322936534881592\n",
      "Epoch 520, Loss: 1.4310708045959473\n",
      "Epoch 540, Loss: 1.4299036264419556\n",
      "Epoch 560, Loss: 1.4287970066070557\n",
      "Epoch 580, Loss: 1.4277561902999878\n",
      "Epoch 600, Loss: 1.426785945892334\n",
      "Epoch 620, Loss: 1.4258911609649658\n",
      "Epoch 640, Loss: 1.4250767230987549\n",
      "Epoch 660, Loss: 1.4243470430374146\n",
      "Epoch 680, Loss: 1.4237068891525269\n",
      "Epoch 700, Loss: 1.4231606721878052\n",
      "Epoch 720, Loss: 1.4227131605148315\n",
      "Epoch 740, Loss: 1.4223682880401611\n",
      "Epoch 760, Loss: 1.422130823135376\n",
      "Epoch 780, Loss: 1.4220045804977417\n",
      "Epoch 800, Loss: 1.421993613243103\n",
      "Epoch 820, Loss: 1.422102451324463\n",
      "Epoch 840, Loss: 1.4223347902297974\n",
      "Epoch 860, Loss: 1.422694444656372\n",
      "Epoch 880, Loss: 1.4231854677200317\n",
      "Epoch 900, Loss: 1.4238113164901733\n",
      "Epoch 920, Loss: 1.4245755672454834\n",
      "Epoch 940, Loss: 1.425482153892517\n",
      "Epoch 960, Loss: 1.4265344142913818\n",
      "Epoch 980, Loss: 1.4277355670928955\n",
      "weights of  P024 :  [[array([[-0.6641064 , -0.31496578,  0.45225558],\n",
      "       [-0.5170395 , -0.6787407 ,  0.39810926],\n",
      "       [ 0.4827345 ,  0.4998053 , -0.02303289],\n",
      "       [-0.40014473,  0.15866089, -0.28655982],\n",
      "       [ 1.0673554 ,  0.5019175 ,  0.07138865]], dtype=float32), array([-0.47786558, -0.37884605,  0.7356753 ], dtype=float32)]]\n",
      "Lowest loss of  P024 :  1.421984\n",
      "Epoch 0, Loss: 2.337651252746582\n",
      "Epoch 20, Loss: 1.4724235534667969\n",
      "Epoch 40, Loss: 1.4541072845458984\n",
      "Epoch 60, Loss: 1.4435077905654907\n",
      "Epoch 80, Loss: 1.437873363494873\n",
      "Epoch 100, Loss: 1.4329676628112793\n",
      "Epoch 120, Loss: 1.4284257888793945\n",
      "Epoch 140, Loss: 1.424126386642456\n",
      "Epoch 160, Loss: 1.4202460050582886\n",
      "Epoch 180, Loss: 1.4169305562973022\n",
      "Epoch 200, Loss: 1.4143002033233643\n",
      "Epoch 220, Loss: 1.4124629497528076\n",
      "Epoch 240, Loss: 1.4115151166915894\n",
      "Epoch 260, Loss: 1.4115395545959473\n",
      "Epoch 280, Loss: 1.4126076698303223\n",
      "Epoch 300, Loss: 1.4147799015045166\n",
      "Epoch 320, Loss: 1.4181057214736938\n",
      "Epoch 340, Loss: 1.4226248264312744\n",
      "Epoch 360, Loss: 1.4283678531646729\n",
      "Epoch 380, Loss: 1.435356616973877\n",
      "Epoch 400, Loss: 1.4436060190200806\n",
      "Epoch 420, Loss: 1.4531224966049194\n",
      "Epoch 440, Loss: 1.46390700340271\n",
      "Epoch 460, Loss: 1.475954294204712\n",
      "Epoch 480, Loss: 1.4892542362213135\n",
      "Epoch 500, Loss: 1.5037918090820312\n",
      "Epoch 520, Loss: 1.5195486545562744\n",
      "Epoch 540, Loss: 1.536502718925476\n",
      "Epoch 560, Loss: 1.5546298027038574\n",
      "Epoch 580, Loss: 1.5739026069641113\n",
      "Epoch 600, Loss: 1.5942926406860352\n",
      "Epoch 620, Loss: 1.6157703399658203\n",
      "Epoch 640, Loss: 1.6383047103881836\n",
      "Epoch 660, Loss: 1.6618638038635254\n",
      "Epoch 680, Loss: 1.6864163875579834\n",
      "Epoch 700, Loss: 1.711930274963379\n",
      "Epoch 720, Loss: 1.7383742332458496\n",
      "Epoch 740, Loss: 1.765716552734375\n",
      "Epoch 760, Loss: 1.7939261198043823\n",
      "Epoch 780, Loss: 1.8229732513427734\n",
      "Epoch 800, Loss: 1.8528282642364502\n",
      "Epoch 820, Loss: 1.8834621906280518\n",
      "Epoch 840, Loss: 1.9148476123809814\n",
      "Epoch 860, Loss: 1.9469575881958008\n",
      "Epoch 880, Loss: 1.9797661304473877\n",
      "Epoch 900, Loss: 2.0132484436035156\n",
      "Epoch 920, Loss: 2.0473806858062744\n",
      "Epoch 940, Loss: 2.082139015197754\n",
      "Epoch 960, Loss: 2.117501974105835\n",
      "Epoch 980, Loss: 2.153449296951294\n",
      "weights of  P025 :  [[array([[ 0.5775683 , -0.24549547, -0.06939105],\n",
      "       [-0.1759865 , -0.10916992,  0.9050196 ],\n",
      "       [-0.7586484 , -0.03171365, -0.40827808],\n",
      "       [-0.39859313, -0.5284855 , -0.66704285],\n",
      "       [-0.3275041 , -0.2591321 , -0.6777358 ]], dtype=float32), array([-0.05307431, -0.1253879 ,  0.40981993], dtype=float32)]]\n",
      "Lowest loss of  P025 :  1.411401\n",
      "Epoch 0, Loss: 3.173926591873169\n",
      "Epoch 20, Loss: 1.5491706132888794\n",
      "Epoch 40, Loss: 1.4189997911453247\n",
      "Epoch 60, Loss: 1.40006422996521\n",
      "Epoch 80, Loss: 1.3989589214324951\n",
      "Epoch 100, Loss: 1.3972082138061523\n",
      "Epoch 120, Loss: 1.395765781402588\n",
      "Epoch 140, Loss: 1.3942525386810303\n",
      "Epoch 160, Loss: 1.3928147554397583\n",
      "Epoch 180, Loss: 1.3914241790771484\n",
      "Epoch 200, Loss: 1.3901257514953613\n",
      "Epoch 220, Loss: 1.3889358043670654\n",
      "Epoch 240, Loss: 1.3878768682479858\n",
      "Epoch 260, Loss: 1.3869651556015015\n",
      "Epoch 280, Loss: 1.3862156867980957\n",
      "Epoch 300, Loss: 1.3856406211853027\n",
      "Epoch 320, Loss: 1.3852496147155762\n",
      "Epoch 340, Loss: 1.3850507736206055\n",
      "Epoch 360, Loss: 1.3850497007369995\n",
      "Epoch 380, Loss: 1.3852510452270508\n",
      "Epoch 400, Loss: 1.3856571912765503\n",
      "Epoch 420, Loss: 1.3862690925598145\n",
      "Epoch 440, Loss: 1.3870874643325806\n",
      "Epoch 460, Loss: 1.388110876083374\n",
      "Epoch 480, Loss: 1.3893375396728516\n",
      "Epoch 500, Loss: 1.3907653093338013\n",
      "Epoch 520, Loss: 1.3923907279968262\n",
      "Epoch 540, Loss: 1.394210934638977\n",
      "Epoch 560, Loss: 1.396221399307251\n",
      "Epoch 580, Loss: 1.398418664932251\n",
      "Epoch 600, Loss: 1.4007983207702637\n",
      "Epoch 620, Loss: 1.403356909751892\n",
      "Epoch 640, Loss: 1.4060897827148438\n",
      "Epoch 660, Loss: 1.4089932441711426\n",
      "Epoch 680, Loss: 1.4120639562606812\n",
      "Epoch 700, Loss: 1.415298342704773\n",
      "Epoch 720, Loss: 1.4186925888061523\n",
      "Epoch 740, Loss: 1.4222445487976074\n",
      "Epoch 760, Loss: 1.4259506464004517\n",
      "Epoch 780, Loss: 1.4298095703125\n",
      "Epoch 800, Loss: 1.4338182210922241\n",
      "Epoch 820, Loss: 1.4379750490188599\n",
      "Epoch 840, Loss: 1.442278504371643\n",
      "Epoch 860, Loss: 1.4467273950576782\n",
      "Epoch 880, Loss: 1.4513204097747803\n",
      "Epoch 900, Loss: 1.4560565948486328\n",
      "Epoch 920, Loss: 1.4609348773956299\n",
      "Epoch 940, Loss: 1.4659552574157715\n",
      "Epoch 960, Loss: 1.4711169004440308\n",
      "Epoch 980, Loss: 1.476419448852539\n",
      "weights of  P026 :  [[array([[ 0.3591038 ,  0.566214  ,  0.45108047],\n",
      "       [ 0.10598831, -0.6605029 ,  0.42761686],\n",
      "       [-0.06511572,  0.5085777 , -0.17415997],\n",
      "       [ 0.9071295 ,  0.36501467, -0.44529587],\n",
      "       [-0.34088323,  0.10063132,  0.57478344]], dtype=float32), array([-0.16136335, -0.04589276,  0.40492484], dtype=float32)]]\n",
      "Lowest loss of  P026 :  1.3850253\n",
      "Epoch 0, Loss: 5.895236968994141\n",
      "Epoch 20, Loss: 3.33933687210083\n",
      "Epoch 40, Loss: 1.4685347080230713\n",
      "Epoch 60, Loss: 1.341086983680725\n",
      "Epoch 80, Loss: 1.3237078189849854\n",
      "Epoch 100, Loss: 1.321151852607727\n",
      "Epoch 120, Loss: 1.3205853700637817\n",
      "Epoch 140, Loss: 1.3205825090408325\n",
      "Epoch 160, Loss: 1.3205846548080444\n",
      "Epoch 180, Loss: 1.3205934762954712\n",
      "Epoch 200, Loss: 1.3206120729446411\n",
      "Epoch 220, Loss: 1.320639967918396\n",
      "Epoch 240, Loss: 1.3206770420074463\n",
      "Epoch 260, Loss: 1.3207252025604248\n",
      "Epoch 280, Loss: 1.320785403251648\n",
      "Epoch 300, Loss: 1.3208588361740112\n",
      "Epoch 320, Loss: 1.320946455001831\n",
      "Epoch 340, Loss: 1.3210495710372925\n",
      "Epoch 360, Loss: 1.3211690187454224\n",
      "Epoch 380, Loss: 1.3213064670562744\n",
      "Epoch 400, Loss: 1.3214625120162964\n",
      "Epoch 420, Loss: 1.321638822555542\n",
      "Epoch 440, Loss: 1.321835994720459\n",
      "Epoch 460, Loss: 1.322055697441101\n",
      "Epoch 480, Loss: 1.322298526763916\n",
      "Epoch 500, Loss: 1.3225659132003784\n",
      "Epoch 520, Loss: 1.3228590488433838\n",
      "Epoch 540, Loss: 1.323179006576538\n",
      "Epoch 560, Loss: 1.3235266208648682\n",
      "Epoch 580, Loss: 1.3239037990570068\n",
      "Epoch 600, Loss: 1.3243110179901123\n",
      "Epoch 620, Loss: 1.32474946975708\n",
      "Epoch 640, Loss: 1.3252203464508057\n",
      "Epoch 660, Loss: 1.3257250785827637\n",
      "Epoch 680, Loss: 1.3262643814086914\n",
      "Epoch 700, Loss: 1.3268390893936157\n",
      "Epoch 720, Loss: 1.327451229095459\n",
      "Epoch 740, Loss: 1.3281011581420898\n",
      "Epoch 760, Loss: 1.3287899494171143\n",
      "Epoch 780, Loss: 1.3295191526412964\n",
      "Epoch 800, Loss: 1.3302897214889526\n",
      "Epoch 820, Loss: 1.3311026096343994\n",
      "Epoch 840, Loss: 1.3319586515426636\n",
      "Epoch 860, Loss: 1.3328591585159302\n",
      "Epoch 880, Loss: 1.3338050842285156\n",
      "Epoch 900, Loss: 1.334797739982605\n",
      "Epoch 920, Loss: 1.3358373641967773\n",
      "Epoch 940, Loss: 1.336925745010376\n",
      "Epoch 960, Loss: 1.3380635976791382\n",
      "Epoch 980, Loss: 1.33925199508667\n",
      "weights of  P027 :  [[array([[-0.2506445 ,  0.18852112,  0.03678686],\n",
      "       [-0.4263426 , -0.6149485 , -0.09701062],\n",
      "       [-0.420572  ,  0.313544  ,  0.26527914],\n",
      "       [ 0.61984664, -0.0956374 ,  0.61215645],\n",
      "       [ 0.23539166, -0.11402628, -0.80133677]], dtype=float32), array([ 0.19597326, -0.22662927,  0.25460255], dtype=float32)]]\n",
      "Lowest loss of  P027 :  1.3205729\n",
      "Epoch 0, Loss: 6.975647926330566\n",
      "Epoch 20, Loss: 4.609740257263184\n",
      "Epoch 40, Loss: 3.483802318572998\n",
      "Epoch 60, Loss: 2.469676971435547\n",
      "Epoch 80, Loss: 1.6106516122817993\n",
      "Epoch 100, Loss: 1.4289448261260986\n",
      "Epoch 120, Loss: 1.4226728677749634\n",
      "Epoch 140, Loss: 1.4212632179260254\n",
      "Epoch 160, Loss: 1.4207403659820557\n",
      "Epoch 180, Loss: 1.420581340789795\n",
      "Epoch 200, Loss: 1.420400619506836\n",
      "Epoch 220, Loss: 1.4202406406402588\n",
      "Epoch 240, Loss: 1.4200900793075562\n",
      "Epoch 260, Loss: 1.4199559688568115\n",
      "Epoch 280, Loss: 1.4198397397994995\n",
      "Epoch 300, Loss: 1.4197444915771484\n",
      "Epoch 320, Loss: 1.4196736812591553\n",
      "Epoch 340, Loss: 1.419629454612732\n",
      "Epoch 360, Loss: 1.4196150302886963\n",
      "Epoch 380, Loss: 1.419633150100708\n",
      "Epoch 400, Loss: 1.4196863174438477\n",
      "Epoch 420, Loss: 1.4197778701782227\n",
      "Epoch 440, Loss: 1.4199104309082031\n",
      "Epoch 460, Loss: 1.4200862646102905\n",
      "Epoch 480, Loss: 1.4203084707260132\n",
      "Epoch 500, Loss: 1.4205795526504517\n",
      "Epoch 520, Loss: 1.4209020137786865\n",
      "Epoch 540, Loss: 1.421278715133667\n",
      "Epoch 560, Loss: 1.4217121601104736\n",
      "Epoch 580, Loss: 1.422204852104187\n",
      "Epoch 600, Loss: 1.4227590560913086\n",
      "Epoch 620, Loss: 1.4233773946762085\n",
      "Epoch 640, Loss: 1.4240624904632568\n",
      "Epoch 660, Loss: 1.4248164892196655\n",
      "Epoch 680, Loss: 1.4256418943405151\n",
      "Epoch 700, Loss: 1.4265409708023071\n",
      "Epoch 720, Loss: 1.427515983581543\n",
      "Epoch 740, Loss: 1.4285691976547241\n",
      "Epoch 760, Loss: 1.4297031164169312\n",
      "Epoch 780, Loss: 1.4309191703796387\n",
      "Epoch 800, Loss: 1.4322199821472168\n",
      "Epoch 820, Loss: 1.433607578277588\n",
      "Epoch 840, Loss: 1.4350836277008057\n",
      "Epoch 860, Loss: 1.4366509914398193\n",
      "Epoch 880, Loss: 1.4383105039596558\n",
      "Epoch 900, Loss: 1.4400649070739746\n",
      "Epoch 920, Loss: 1.441915512084961\n",
      "Epoch 940, Loss: 1.443864345550537\n",
      "Epoch 960, Loss: 1.4459128379821777\n",
      "Epoch 980, Loss: 1.4480630159378052\n",
      "weights of  P028 :  [[array([[-0.15728514,  0.20569833, -0.5024573 ],\n",
      "       [-0.40697712,  0.09268585,  1.211732  ],\n",
      "       [-0.5573589 ,  0.2510415 , -0.3376957 ],\n",
      "       [ 0.09906089, -0.74922276, -0.27476886],\n",
      "       [ 0.67102385,  0.04242747, -0.3084721 ]], dtype=float32), array([-0.10877242,  0.38017502, -0.19663179], dtype=float32)]]\n",
      "Lowest loss of  P028 :  1.4196149\n",
      "Epoch 0, Loss: 3.1920597553253174\n",
      "Epoch 20, Loss: 1.926356315612793\n",
      "Epoch 40, Loss: 1.3620091676712036\n",
      "Epoch 60, Loss: 1.3602205514907837\n",
      "Epoch 80, Loss: 1.3535749912261963\n",
      "Epoch 100, Loss: 1.3501520156860352\n",
      "Epoch 120, Loss: 1.3477258682250977\n",
      "Epoch 140, Loss: 1.3456757068634033\n",
      "Epoch 160, Loss: 1.344005823135376\n",
      "Epoch 180, Loss: 1.3428140878677368\n",
      "Epoch 200, Loss: 1.3421913385391235\n",
      "Epoch 220, Loss: 1.3422244787216187\n",
      "Epoch 240, Loss: 1.342991590499878\n",
      "Epoch 260, Loss: 1.3445641994476318\n",
      "Epoch 280, Loss: 1.3470070362091064\n",
      "Epoch 300, Loss: 1.3503785133361816\n",
      "Epoch 320, Loss: 1.3547309637069702\n",
      "Epoch 340, Loss: 1.3601105213165283\n",
      "Epoch 360, Loss: 1.3665578365325928\n",
      "Epoch 380, Loss: 1.374107837677002\n",
      "Epoch 400, Loss: 1.3827908039093018\n",
      "Epoch 420, Loss: 1.3926317691802979\n",
      "Epoch 440, Loss: 1.4036507606506348\n",
      "Epoch 460, Loss: 1.4158637523651123\n",
      "Epoch 480, Loss: 1.4292824268341064\n",
      "Epoch 500, Loss: 1.4439151287078857\n",
      "Epoch 520, Loss: 1.4597654342651367\n",
      "Epoch 540, Loss: 1.476834774017334\n",
      "Epoch 560, Loss: 1.4951207637786865\n",
      "Epoch 580, Loss: 1.5146180391311646\n",
      "Epoch 600, Loss: 1.535319209098816\n",
      "Epoch 620, Loss: 1.5572144985198975\n",
      "Epoch 640, Loss: 1.5802913904190063\n",
      "Epoch 660, Loss: 1.6045359373092651\n",
      "Epoch 680, Loss: 1.6299328804016113\n",
      "Epoch 700, Loss: 1.6564652919769287\n",
      "Epoch 720, Loss: 1.6841139793395996\n",
      "Epoch 740, Loss: 1.712860345840454\n",
      "Epoch 760, Loss: 1.7426836490631104\n",
      "Epoch 780, Loss: 1.7735627889633179\n",
      "Epoch 800, Loss: 1.805476188659668\n",
      "Epoch 820, Loss: 1.838401436805725\n",
      "Epoch 840, Loss: 1.8723163604736328\n",
      "Epoch 860, Loss: 1.9071975946426392\n",
      "Epoch 880, Loss: 1.943023681640625\n",
      "Epoch 900, Loss: 1.979770541191101\n",
      "Epoch 920, Loss: 2.017416000366211\n",
      "Epoch 940, Loss: 2.0559370517730713\n",
      "Epoch 960, Loss: 2.0953116416931152\n",
      "Epoch 980, Loss: 2.135516881942749\n",
      "weights of  P029 :  [[array([[-0.01136779,  0.6480921 ,  0.1442448 ],\n",
      "       [-0.6314697 , -0.7069867 ,  0.41086343],\n",
      "       [ 0.5102445 ,  0.737285  , -0.23837198],\n",
      "       [ 0.50815374, -0.35923412,  0.6279265 ],\n",
      "       [-0.09658799, -0.11229496, -0.5795956 ]], dtype=float32), array([ 0.12441666, -0.04666444, -0.08059414], dtype=float32)]]\n",
      "Lowest loss of  P029 :  1.3421204\n",
      "Epoch 0, Loss: 2.7597768306732178\n",
      "Epoch 20, Loss: 1.5455368757247925\n",
      "Epoch 40, Loss: 1.3806750774383545\n",
      "Epoch 60, Loss: 1.3555177450180054\n",
      "Epoch 80, Loss: 1.3525493144989014\n",
      "Epoch 100, Loss: 1.3510030508041382\n",
      "Epoch 120, Loss: 1.3494609594345093\n",
      "Epoch 140, Loss: 1.3478845357894897\n",
      "Epoch 160, Loss: 1.3462870121002197\n",
      "Epoch 180, Loss: 1.3446803092956543\n",
      "Epoch 200, Loss: 1.3430854082107544\n",
      "Epoch 220, Loss: 1.3415204286575317\n",
      "Epoch 240, Loss: 1.340000867843628\n",
      "Epoch 260, Loss: 1.3385404348373413\n",
      "Epoch 280, Loss: 1.3371517658233643\n",
      "Epoch 300, Loss: 1.3358453512191772\n",
      "Epoch 320, Loss: 1.3346309661865234\n",
      "Epoch 340, Loss: 1.3335165977478027\n",
      "Epoch 360, Loss: 1.3325090408325195\n",
      "Epoch 380, Loss: 1.3316147327423096\n",
      "Epoch 400, Loss: 1.3308383226394653\n",
      "Epoch 420, Loss: 1.3301843404769897\n",
      "Epoch 440, Loss: 1.3296561241149902\n",
      "Epoch 460, Loss: 1.3292566537857056\n",
      "Epoch 480, Loss: 1.328987956047058\n",
      "Epoch 500, Loss: 1.3288518190383911\n",
      "Epoch 520, Loss: 1.3288500308990479\n",
      "Epoch 540, Loss: 1.3289834260940552\n",
      "Epoch 560, Loss: 1.3292527198791504\n",
      "Epoch 580, Loss: 1.3296582698822021\n",
      "Epoch 600, Loss: 1.3302010297775269\n",
      "Epoch 620, Loss: 1.3308809995651245\n",
      "Epoch 640, Loss: 1.3316984176635742\n",
      "Epoch 660, Loss: 1.332653284072876\n",
      "Epoch 680, Loss: 1.3337458372116089\n",
      "Epoch 700, Loss: 1.334976077079773\n",
      "Epoch 720, Loss: 1.3363444805145264\n",
      "Epoch 740, Loss: 1.33785080909729\n",
      "Epoch 760, Loss: 1.339495301246643\n",
      "Epoch 780, Loss: 1.3412781953811646\n",
      "Epoch 800, Loss: 1.343199610710144\n",
      "Epoch 820, Loss: 1.3452599048614502\n",
      "Epoch 840, Loss: 1.3474595546722412\n",
      "Epoch 860, Loss: 1.3497984409332275\n",
      "Epoch 880, Loss: 1.3522775173187256\n",
      "Epoch 900, Loss: 1.3548963069915771\n",
      "Epoch 920, Loss: 1.3576561212539673\n",
      "Epoch 940, Loss: 1.360556721687317\n",
      "Epoch 960, Loss: 1.3635987043380737\n",
      "Epoch 980, Loss: 1.3667826652526855\n",
      "weights of  P030 :  [[array([[-0.13635476,  0.06788038,  0.41247815],\n",
      "       [-0.5624929 ,  0.3820953 ,  0.16910647],\n",
      "       [ 0.31616122, -0.27281597,  0.51587707],\n",
      "       [ 0.6432043 ,  0.18964837, -0.6103604 ],\n",
      "       [-0.32526442, -0.42174158, -0.48383248]], dtype=float32), array([-0.0385971, -0.1564088,  0.4630632], dtype=float32)]]\n",
      "Lowest loss of  P030 :  1.328834\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Custom loss function\n",
    "class CustomLossLL1(tf.keras.losses.Loss):\n",
    "    def __init__(self, lambda_t, model):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        ws = self.get_weights_from_model()\n",
    "        reg_term = self.regularization_term(ws)\n",
    "        return ce_loss + self.lambda_t * reg_term\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        # return tf.concat([tf.reshape(w, [-1]) for w in model_weights], axis=0)\n",
    "        return model_weights\n",
    "\n",
    "    def regularization_term(self, ws):\n",
    "        reg_term = tf.pow(tf.norm(ws, ord='euclidean'),2)\n",
    "        return reg_term\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True) # Perform a forward pass and compute the predictions\n",
    "        loss = custom_loss(y, y_pred) # Compute the custom loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_weight_LL(X_train, y_train, lambd, num_tier=10, learning_rate = 0.01):\n",
    "    n_classes = np.unique(y_train).size\n",
    "    _, sequential_indices = np.unique(y_train, return_inverse=True)\n",
    "    y_one_hot = tf.keras.utils.to_categorical(sequential_indices, num_classes=n_classes)\n",
    "\n",
    "    lambda_t = lambd  # Regularization parameter\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_classes, input_shape=(X_train.shape[1],), activation='softmax')  # Adjust input_shape to match the number of features in X\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate)\n",
    "    custom_loss = CustomLossLL1(lambda_t, model)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = num_tier\n",
    "    lowest_loss = float('inf')\n",
    "    best_weights = None\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X_train, y_one_hot, custom_loss)\n",
    "        loss_value = loss.numpy()\n",
    "        if loss_value < lowest_loss:\n",
    "            lowest_loss = loss_value\n",
    "            best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    # best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "    return best_weights, lowest_loss\n",
    "\n",
    "def build_clf_params(data, target_subjects ,condition):\n",
    "\n",
    "    for sub in data.keys():\n",
    "        if (sub  != target_subjects) and  (sub != target_data_0): #Don't apply weight to target subject\n",
    "            # Where the tranining data is stored\n",
    "            if condition == \"noEA\":\n",
    "                X = data[sub]['Raw_csp']\n",
    "                y = data[sub]['Raw_csp_label']\n",
    "                store_ws = 'ws_Raw'\n",
    "\n",
    "            else:\n",
    "                X = data[sub]['EA_csp']\n",
    "                y = data[sub]['EA_csp_label']\n",
    "                store_ws = 'ws_EA'\n",
    "\n",
    "            weights, loss = train_weight_LL(X_train=X, y_train=y, lambd= 0.1, num_tier=1000, learning_rate= 0.005)\n",
    "            print(\"weights of \", str(sub), \": \", weights)\n",
    "            print(\"Lowest loss of \", str(sub), \": \", loss)\n",
    "            data[sub][store_ws] = weights\n",
    "\n",
    "build_clf_params(CSP2D_Epoch, target_subjects= target_data ,condition = condition_wLTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Raw_csp', 'Raw_csp_label', 'EA_csp', 'EA_csp_label'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSP2D_Epoch[target_data].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the kl divergence\n",
    "def KL_div(P, Q):\n",
    "    # First convert to np array\n",
    "    P = np.array(P)\n",
    "    Q = np.array(Q)\n",
    "    \n",
    "    # Then compute their means, datain shape of samples x feat\n",
    "    mu_P = np.mean(P, axis=0)\n",
    "    mu_Q = np.mean(Q, axis=0)    \n",
    "\n",
    "    \n",
    "    # Compute their covariance\n",
    "    sigma_P = np.cov(P, rowvar=False)\n",
    "    sigma_Q = np.cov(Q, rowvar=False)  \n",
    "\n",
    "    diff = mu_Q - mu_P\n",
    "\n",
    "    inv_sigma_Q = np.linalg.inv(sigma_Q)\n",
    "    term1 = np.dot(np.dot(diff.T, inv_sigma_Q), diff)\n",
    "    \n",
    "    # Calculate the trace term trace(Sigma_Q^{-1} * Sigma_P)\n",
    "    term2 = np.trace(np.dot(inv_sigma_Q, sigma_P))\n",
    "    \n",
    "    # Calculate the determinant term ln(det(Sigma_P) / det(Sigma_Q))\n",
    "    det_sigma0 = np.linalg.det(sigma_P)\n",
    "    det_sigma1 = np.linalg.det(sigma_Q)\n",
    "\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    term3 = np.log((det_sigma0+epsilon) / (det_sigma1+epsilon))\n",
    "    \n",
    "    print(term3)\n",
    "    \n",
    "    # Dimensionality of the data\n",
    "    K = mu_P.shape[0]\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_div = 0.5 * (term1 + term2 - term3 - K)\n",
    "    \n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10229703248862927\n",
      "0.0018504274013096628\n",
      "0.049785329237039024\n",
      "nan\n",
      "0.10695465379963104\n",
      "0.0032232506896612396\n",
      "-0.39569116994468195\n",
      "nan\n",
      "0.10877999369091472\n",
      "-0.08401380581420284\n",
      "0.026632418549640755\n",
      "nan\n",
      "0.10899569936309919\n",
      "0.007208964217125076\n",
      "0.05074341020993564\n",
      "nan\n",
      "0.06523228002551716\n",
      "-0.0021310633125235876\n",
      "0.009826454788384549\n",
      "nan\n",
      "0.09940557110932145\n",
      "-0.004082862409767903\n",
      "0.03948479473464662\n",
      "nan\n",
      "0.10339452938778265\n",
      "-0.002739120746363367\n",
      "0.0004964598402161748\n",
      "nan\n",
      "0.10637527916649778\n",
      "0.0068111322218710885\n",
      "0.044494064534279384\n",
      "nan\n",
      "0.08840376882842864\n",
      "0.006953018742465782\n",
      "0.018443917415206144\n",
      "nan\n",
      "0.10756008213073144\n",
      "0.003954573871558288\n",
      "0.04618385807974789\n",
      "nan\n",
      "0.07320051543000514\n",
      "-0.06871055610669395\n",
      "0.05214084357288975\n",
      "nan\n",
      "0.0661325969769259\n",
      "-0.02918395810920568\n",
      "-0.02459649027969104\n",
      "nan\n",
      "0.10940095753192033\n",
      "-0.0027621747522611744\n",
      "-0.07960577585878781\n",
      "nan\n",
      "0.10825111011770695\n",
      "0.004872159232428473\n",
      "0.05338006894847506\n",
      "nan\n",
      "0.08943178455640784\n",
      "-0.0017225733967886007\n",
      "0.04841320221691047\n",
      "nan\n",
      "0.09266941593700184\n",
      "-0.011539913113096403\n",
      "0.025239046940393983\n",
      "nan\n",
      "0.10966756921537067\n",
      "0.005587213698031768\n",
      "0.052837467903452374\n",
      "nan\n",
      "0.09462740694722788\n",
      "0.006731350692366903\n",
      "0.03242879899746284\n",
      "nan\n",
      "0.1035785311077217\n",
      "0.005816595311067108\n",
      "-0.05774677244392387\n",
      "nan\n",
      "0.013689098403995237\n",
      "-0.06731018036221778\n",
      "-0.03151061498545581\n",
      "nan\n",
      "0.10792165692901982\n",
      "0.008693170879764152\n",
      "0.053144060448706634\n",
      "nan\n",
      "0.08620716056145722\n",
      "-0.037444908949563475\n",
      "-0.055590015722798154\n",
      "nan\n",
      "0.08348342114276137\n",
      "-0.04717205980972605\n",
      "0.04429274350076527\n",
      "nan\n",
      "0.11035532400467296\n",
      "0.004057867464711428\n",
      "0.03402664707983298\n",
      "nan\n",
      "0.08366846882195793\n",
      "-0.08162906740986807\n",
      "0.02365382322819707\n",
      "nan\n",
      "0.09586441507093361\n",
      "-0.1040800308589694\n",
      "-0.17050244176497417\n",
      "nan\n",
      "0.036410629111386796\n",
      "-0.04218762425084627\n",
      "-0.0014354154437454057\n",
      "nan\n",
      "-0.01169798413211174\n",
      "-0.6653501556720587\n",
      "-0.11853612827893328\n",
      "nan\n",
      "0.004401824498121165\n",
      "-0.4954181538194649\n",
      "0.03704362347070307\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\891804420.py:13: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  sigma_P = np.cov(P, rowvar=False)\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2705: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2705: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n",
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_23432\\891804420.py:14: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  sigma_Q = np.cov(Q, rowvar=False)\n",
      "c:\\Python311\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2139: RuntimeWarning: invalid value encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n"
     ]
    }
   ],
   "source": [
    "# Compute kl divergence of target subject to each source subject\n",
    "def compute_all_kl_div(data, target_subjects , condition):\n",
    "    '''\n",
    "    Parameter:\n",
    "    data, is the whole data containing target and source data\n",
    "    '''\n",
    "    kl_div_score = []\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        target_data = 'Raw_csp'\n",
    "        label_name = 'Raw_csp_label'\n",
    "\n",
    "    else:\n",
    "        target_data = 'EA_csp'\n",
    "        label_name = 'EA_csp_label'\n",
    "        \n",
    "    # cal P from target data\n",
    "    label_tgt =  data[target_subjects][label_name]\n",
    "    P_left =  data[target_subjects][target_data][np.where(label_tgt == 0)]\n",
    "    P_right = data[target_subjects][target_data][np.where(label_tgt == 1)]\n",
    "    P_non = data[target_subjects][target_data][np.where(label_tgt == 2)]\n",
    "    P_feet = data[target_subjects][target_data][np.where(label_tgt == 3)]\n",
    "\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "\n",
    "    #cal Q from each source subject\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            label_src =  data[sub][label_name]\n",
    "            Q_left =  data[sub][target_data][np.where(label_src == 0)]\n",
    "            Q_right = data[sub][target_data][np.where(label_src == 1)]\n",
    "            Q_non = data[sub][target_data][np.where(label_src == 2)]\n",
    "            Q_feet = data[sub][target_data][np.where(label_src == 3)]\n",
    "\n",
    "            kl_left = KL_div(P_left, Q_left)\n",
    "            kl_right = KL_div(P_right, Q_right)\n",
    "            kl_non = KL_div(P_non, Q_non)\n",
    "            kl_feet = KL_div(P_feet, Q_feet)\n",
    "\n",
    "            # kl_div = (kl_left + kl_right+ kl_non + kl_feet)/4\n",
    "\n",
    "            kl_div_temp = [kl_left, kl_right, kl_non, kl_feet]\n",
    "\n",
    "            kl_div_score.append(kl_div_temp)\n",
    "\n",
    "    data[target_subjects]['kl_div'] = kl_div_score\n",
    "\n",
    "\n",
    "compute_all_kl_div(CSP2D_Epoch, target_subjects=target_data_0 ,condition = condition_wLTL) #target_sub for cal KL is calibrate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.40080461,  3.77642588,  7.05485501,         nan],\n",
       "       [ 9.89580141, 10.52444587,  0.54988031,         nan],\n",
       "       [13.28231681,  2.75609206,  3.00377511,         nan],\n",
       "       [11.24085164,  6.30648442, 11.50730529,         nan],\n",
       "       [ 6.89217331,  8.82071535,  7.13506949,         nan],\n",
       "       [10.26395138,  8.18904565,  6.35780678,         nan],\n",
       "       [ 8.28305816,  5.82317642,  2.76843468,         nan],\n",
       "       [ 5.31838809, 10.40244743,  6.0642871 ,         nan],\n",
       "       [ 4.06653579,  9.76273921,  3.05128951,         nan],\n",
       "       [10.82844831,  7.68417537,  7.48672341,         nan],\n",
       "       [ 3.21290616,  5.92314285,  9.63718785,         nan],\n",
       "       [ 3.64185178,  5.47040362,  3.90875732,         nan],\n",
       "       [13.87786895,  4.43091317,  1.80127685,         nan],\n",
       "       [ 8.81255711,  6.44232471, 11.04524664,         nan],\n",
       "       [ 6.81740254,  9.60198579,  6.92681675,         nan],\n",
       "       [ 6.30709163,  6.14082641,  4.63855151,         nan],\n",
       "       [14.24036914, 10.78011697,  7.14145323,         nan],\n",
       "       [ 5.97245342,  7.60695669,  4.77085784,         nan],\n",
       "       [ 4.24331199,  9.28221301,  2.03825684,         nan],\n",
       "       [ 3.20778649,  5.39600125,  5.17710927,         nan],\n",
       "       [ 5.79689665, 20.44687346, 11.81540393,         nan],\n",
       "       [ 4.40340057,  4.2097606 ,  1.80637394,         nan],\n",
       "       [ 6.54947902,  3.48367816,  9.54776812,         nan],\n",
       "       [ 9.80020718,  8.79652097,  4.27418838,         nan],\n",
       "       [ 6.07623732,  3.1264189 ,  3.60826518,         nan],\n",
       "       [ 4.71304667,  1.92447956,  0.28897642,         nan],\n",
       "       [ 2.32531407,  3.63038419,  2.24044953,         nan],\n",
       "       [ 2.46249821,  0.60200041,  4.80124225,         nan],\n",
       "       [ 3.54908277,  3.96752526,  9.61606754,         nan]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(CSP2D_Epoch[target_data_0]['kl_div'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001280328509003737, 0.00010427460664755261, 3.212858370999344e-05, 6.263076003701985e-05, 0.00044314926767848544, 9.009998745714325e-05, 0.00021243016356103039, 0.001249819328721526, 0.0036564436603680078, 7.273091420431783e-05, 0.009383260410525743, 0.005684119984765362, 2.695853912690839e-05, 0.00016579541253403026, 0.00046291234916815246, 0.0006319109684255113, 2.431659782538074e-05, 0.0007858864988799015, 0.0030841763630272637, 0.009443305299787257, 0.0008854984423231425, 0.0026595476282341756, 0.0005434335600775665, 0.00010840298645947809, 0.0007335546927487045, 0.0020265446582339967, 0.03419785120146954, 0.027191036295779767, 0.006302118159841352]\n",
      "[0.004916206745552706, 8.150542921962119e-05, 0.01732854470043665, 0.0006321543706035462, 0.00016518289585395557, 0.0002223543437557585, 0.0008696217179319007, 8.539671839792355e-05, 0.00011007671315121094, 0.00028680605926043886, 0.000812384906195501, 0.0011165823530840506, 0.0025941067058282345, 0.0005805004917364637, 0.000117635302424201, 0.0007031766286746052, 7.404402053687472e-05, 0.0002986299688447277, 0.00013470234681776878, 0.0011794502898321009, 5.721157009820399e-06, 0.0031836780110014733, 0.006788878393808444, 0.00016700769494189973, 0.010465398421665297, 0.07288808155836941, 0.005756278569053305, 7.608942469239446, 0.004035314746960812]\n",
      "[[1.15979801e-03 6.34796327e-04 2.61300863e-06]\n",
      " [9.44581645e-04 1.05242415e-05 7.07504619e-02]\n",
      " [2.91039894e-04 2.23751707e-03 7.95042511e-05]\n",
      " [5.67346819e-04 8.16257927e-05 3.69154531e-07]\n",
      " [4.01431065e-03 2.13289434e-05 2.49747221e-06]\n",
      " [8.16179482e-04 2.87111035e-05 3.96151417e-06]\n",
      " [1.92431926e-03 1.12288335e-04 1.10183856e-04]\n",
      " [1.13216097e-02 1.10266972e-05 4.78597226e-06]\n",
      " [3.31222498e-02 1.42134570e-05 7.46667487e-05]\n",
      " [6.58840046e-04 3.70333150e-05 2.06028585e-06]\n",
      " [8.49991752e-02 1.04897735e-04 7.50409832e-07]\n",
      " [5.14901526e-02 1.44176682e-04 2.77280012e-05]\n",
      " [2.44206543e-04 3.34959348e-04 6.14751398e-04]\n",
      " [1.50187384e-03 7.49560787e-05 4.34911046e-07]\n",
      " [4.19333645e-03 1.51894462e-05 2.81162868e-06]\n",
      " [5.72422684e-03 9.07964136e-05 1.39814062e-05]\n",
      " [2.20274262e-04 9.56080057e-06 2.48855434e-06]\n",
      " [7.11902913e-03 3.85600560e-05 1.24938280e-05]\n",
      " [2.79383109e-02 1.73931975e-05 3.74970393e-04]\n",
      " [8.55430976e-02 1.52294391e-04 9.01021094e-06]\n",
      " [8.02137359e-03 7.38734078e-07 3.32130565e-07]\n",
      " [2.40917703e-02 4.11086680e-04 6.07842457e-04]\n",
      " [4.92274565e-03 8.76601675e-04 7.78918811e-07]\n",
      " [9.81978975e-04 2.15645673e-05 1.93937841e-05]\n",
      " [6.64497638e-03 1.35132569e-03 3.81835150e-05]\n",
      " [1.83576515e-02 9.41154204e-03 9.26973145e-01]\n",
      " [3.09784554e-01 7.43269086e-04 2.56861874e-04]\n",
      " [2.46312641e-01 9.82490969e-01 1.21805579e-05]\n",
      " [5.70883490e-02 5.21052737e-04 7.57024184e-07]]\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_weights(data, target_subjects):\n",
    "    kl = data[target_subjects]['kl_div']\n",
    "    KL_inv_left = []\n",
    "    KL_inv_right = []\n",
    "    KL_inv_non = []\n",
    "    KL_inv_feet = []\n",
    "\n",
    "    alpha_s = []\n",
    "    eps = 0.0001\n",
    "    \n",
    "    #equation (9)\n",
    "    for val in kl:\n",
    "        if val != 0: \n",
    "            KL_inv_left.append(1/((val[0] + eps)**4))\n",
    "            KL_inv_right.append(1/((val[1] + eps)**4))\n",
    "            KL_inv_non.append(1/((val[2] + eps)**4))\n",
    "            KL_inv_feet.append(1/((val[3] + eps)**4))\n",
    "\n",
    "    print(KL_inv_left)\n",
    "    print(KL_inv_right)\n",
    "    \n",
    "    for i in range(0,len(KL_inv_left)):\n",
    "        temp = [KL_inv_left[i]/sum(KL_inv_left), KL_inv_right[i]/sum(KL_inv_right), KL_inv_non[i]/sum(KL_inv_non), KL_inv_feet[i]/sum(KL_inv_feet)]\n",
    "        alpha_s.append(temp)\n",
    "\n",
    "    alpha_s = np.array(alpha_s)\n",
    "    print(np.array(alpha_s[:, ~np.isnan(alpha_s).any(axis=0)]))\n",
    "                \n",
    "    data[target_subjects]['alpha_s'] = alpha_s[:, ~np.isnan(alpha_s).any(axis=0)]\n",
    "\n",
    "compute_similarity_weights(CSP2D_Epoch, target_subjects=target_data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.20361998  0.63783641  0.02297078]\n",
      " [-0.30742249 -0.70227308 -0.10719426]\n",
      " [-0.09383623  0.72695871  0.24928293]\n",
      " [ 0.15439319 -0.35298721  0.55261605]\n",
      " [ 0.17576212 -0.1100445  -0.77231342]]\n",
      "[[ 12.53385487 -10.90939753  13.66955605  -6.79387663  -3.44492415]\n",
      " [-10.90939753  16.71174699 -14.25187706   3.98091897   2.9850123 ]\n",
      " [ 13.66955605 -14.25187706  16.81814877  -3.70406489  -8.10560135]\n",
      " [ -6.79387663   3.98091897  -3.70406489  12.72591859 -10.12158134]\n",
      " [ -3.44492415   2.9850123   -8.10560135 -10.12158134  17.87512982]]\n"
     ]
    }
   ],
   "source": [
    "def compute_ETL_and_mu_ws(data, target_subjects, condition):\n",
    "\n",
    "    mu_ws = 0\n",
    "    temp_ws = 0\n",
    "\n",
    "    if condition == \"noEA\":\n",
    "        ws_name = 'ws_Raw'\n",
    "    else:\n",
    "        ws_name = 'ws_EA'\n",
    "\n",
    "    alpha_s = np.array(data[target_subjects]['alpha_s'])\n",
    "\n",
    "    tgt_data = target_subjects + \"_test\"\n",
    "    index_count = 0\n",
    "\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            ws = data[sub][ws_name][0][0]\n",
    "            # mu_ws += ws @ alpha_s  #equation (10)\n",
    "            # mu_ws += np.dot(ws, np.transpose(alpha_s))\n",
    "            mu_ws += ws * alpha_s[index_count]\n",
    "            index_count += 1\n",
    "\n",
    "    print(np.array(mu_ws))\n",
    "\n",
    "    index_count = 0\n",
    "    for sub in data.keys():\n",
    "        if (sub != target_subjects) and (sub != tgt_data):\n",
    "            ws = data[sub][ws_name][0][0]\n",
    "            # ws_min_mu = np.dot((np.dot(ws,np.transpose(alpha_s)) - mu_ws) , np.transpose((np.dot(ws,np.transpose(alpha_s)) - mu_ws)))\n",
    "            ws_min_mu = np.dot(((ws * alpha_s[index_count]) - mu_ws), np.transpose((ws * alpha_s[index_count]) - mu_ws))\n",
    "            temp_ws += ws_min_mu #equation (11)\n",
    "            index_count += 1\n",
    "\n",
    "    print(np.array(temp_ws))\n",
    "    \n",
    "    # den = np.diag(temp_ws) #get array in diagonal line\n",
    "\n",
    "    den = temp_ws\n",
    "    nom = np.trace(temp_ws) #Return the sum along diagonals of the array.\n",
    "    Sigma_TL = den/nom\n",
    "\n",
    "\n",
    "    data[target_subjects]['Sigma_TL'] = Sigma_TL\n",
    "    data[target_subjects]['mu_ws'] = mu_ws\n",
    "\n",
    "compute_ETL_and_mu_ws(CSP2D_Epoch, target_subjects = target_data_0, condition=condition_wLTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(CSP2D_Epoch[target_data_0]['Sigma_TL']).shape)\n",
    "print(np.array(CSP2D_Epoch[target_data_0]['mu_ws']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 167.94546508789062\n",
      "Epoch 20, Loss: 74.54048156738281\n",
      "Epoch 40, Loss: 65.38609313964844\n",
      "Epoch 60, Loss: 73.4594955444336\n",
      "Epoch 80, Loss: 69.89293670654297\n",
      "Epoch 100, Loss: 70.67900848388672\n",
      "Epoch 120, Loss: 69.95596313476562\n",
      "Epoch 140, Loss: 69.67904663085938\n",
      "Epoch 160, Loss: 69.31879425048828\n",
      "Epoch 180, Loss: 68.95577239990234\n",
      "Epoch 200, Loss: 68.6186752319336\n",
      "Epoch 220, Loss: 68.28462219238281\n",
      "Epoch 240, Loss: 67.96063232421875\n",
      "Epoch 260, Loss: 67.64575958251953\n",
      "Epoch 280, Loss: 67.3375015258789\n",
      "Epoch 300, Loss: 67.03396606445312\n",
      "Epoch 320, Loss: 66.73272705078125\n",
      "Epoch 340, Loss: 66.43109893798828\n",
      "Epoch 360, Loss: 66.1263427734375\n",
      "Epoch 380, Loss: 65.8156967163086\n",
      "Epoch 400, Loss: 65.49644470214844\n",
      "Epoch 420, Loss: 65.16597747802734\n",
      "Epoch 440, Loss: 64.82178497314453\n",
      "Epoch 460, Loss: 64.46162414550781\n",
      "Epoch 480, Loss: 64.0833511352539\n",
      "Epoch 500, Loss: 63.68505096435547\n",
      "Epoch 520, Loss: 63.26509475708008\n",
      "Epoch 540, Loss: 62.82196807861328\n",
      "Epoch 560, Loss: 62.3543701171875\n",
      "Epoch 580, Loss: 61.861263275146484\n",
      "Epoch 600, Loss: 61.34172821044922\n",
      "Epoch 620, Loss: 60.79508972167969\n",
      "Epoch 640, Loss: 60.22077941894531\n",
      "Epoch 660, Loss: 59.61842346191406\n",
      "Epoch 680, Loss: 58.98782730102539\n",
      "Epoch 700, Loss: 58.32871627807617\n",
      "Epoch 720, Loss: 57.641151428222656\n",
      "Epoch 740, Loss: 56.92533493041992\n",
      "Epoch 760, Loss: 56.18123245239258\n",
      "Epoch 780, Loss: 55.40928649902344\n",
      "Epoch 800, Loss: 54.609718322753906\n",
      "Epoch 820, Loss: 53.78286361694336\n",
      "Epoch 840, Loss: 52.92924880981445\n",
      "Epoch 860, Loss: 52.049381256103516\n",
      "Epoch 880, Loss: 51.143577575683594\n",
      "Epoch 900, Loss: 50.212562561035156\n",
      "Epoch 920, Loss: 49.256690979003906\n",
      "Epoch 940, Loss: 48.27664566040039\n",
      "Epoch 960, Loss: 47.27290725708008\n",
      "Epoch 980, Loss: 46.246028900146484\n",
      "Epoch 1000, Loss: 45.19670867919922\n",
      "Epoch 1020, Loss: 44.12540817260742\n",
      "Epoch 1040, Loss: 43.03267288208008\n",
      "Epoch 1060, Loss: 41.91902542114258\n",
      "Epoch 1080, Loss: 40.784908294677734\n",
      "Epoch 1100, Loss: 39.630821228027344\n",
      "Epoch 1120, Loss: 38.4573974609375\n",
      "Epoch 1140, Loss: 37.26494216918945\n",
      "Epoch 1160, Loss: 36.05397415161133\n",
      "Epoch 1180, Loss: 34.82482147216797\n",
      "Epoch 1200, Loss: 33.57767105102539\n",
      "Epoch 1220, Loss: 32.31291580200195\n",
      "Epoch 1240, Loss: 31.03093147277832\n",
      "Epoch 1260, Loss: 29.73221778869629\n",
      "Epoch 1280, Loss: 28.416255950927734\n",
      "Epoch 1300, Loss: 27.08428955078125\n",
      "Epoch 1320, Loss: 25.73545265197754\n",
      "Epoch 1340, Loss: 24.370454788208008\n",
      "Epoch 1360, Loss: 22.989147186279297\n",
      "Epoch 1380, Loss: 21.591541290283203\n",
      "Epoch 1400, Loss: 20.177892684936523\n",
      "Epoch 1420, Loss: 18.74767303466797\n",
      "Epoch 1440, Loss: 17.301115036010742\n",
      "Epoch 1460, Loss: 15.838045120239258\n",
      "Epoch 1480, Loss: 14.358052253723145\n",
      "Epoch 1500, Loss: 12.861042976379395\n",
      "Epoch 1520, Loss: 11.347241401672363\n",
      "Epoch 1540, Loss: 9.815815925598145\n",
      "Epoch 1560, Loss: 8.266727447509766\n",
      "Epoch 1580, Loss: 6.698807716369629\n",
      "Epoch 1600, Loss: 5.112870216369629\n",
      "Epoch 1620, Loss: 3.507031202316284\n",
      "Epoch 1640, Loss: 1.8821085691452026\n",
      "Epoch 1660, Loss: 0.23649495840072632\n",
      "Epoch 1680, Loss: -1.4295207262039185\n",
      "Epoch 1700, Loss: -3.1179566383361816\n",
      "Epoch 1720, Loss: -4.828242778778076\n",
      "Epoch 1740, Loss: -6.561946392059326\n",
      "Epoch 1760, Loss: -8.319596290588379\n",
      "Epoch 1780, Loss: -10.101530075073242\n",
      "Epoch 1800, Loss: -11.908772468566895\n",
      "Epoch 1820, Loss: -13.743416786193848\n",
      "Epoch 1840, Loss: -15.604948043823242\n",
      "Epoch 1860, Loss: -17.496191024780273\n",
      "Epoch 1880, Loss: -19.416521072387695\n",
      "Epoch 1900, Loss: -21.3675537109375\n",
      "Epoch 1920, Loss: -23.350292205810547\n",
      "Epoch 1940, Loss: -25.367870330810547\n",
      "Epoch 1960, Loss: -27.418880462646484\n",
      "Epoch 1980, Loss: -29.506486892700195\n",
      "Epoch 2000, Loss: -31.631175994873047\n",
      "Epoch 2020, Loss: -33.79475021362305\n",
      "Epoch 2040, Loss: -35.99974822998047\n",
      "Epoch 2060, Loss: -38.24595642089844\n",
      "Epoch 2080, Loss: -40.53535079956055\n",
      "Epoch 2100, Loss: -42.87053298950195\n",
      "Epoch 2120, Loss: -45.25240707397461\n",
      "Epoch 2140, Loss: -47.68303298950195\n",
      "Epoch 2160, Loss: -50.16450119018555\n",
      "Epoch 2180, Loss: -52.69803237915039\n",
      "Epoch 2200, Loss: -55.28683090209961\n",
      "Epoch 2220, Loss: -57.93115997314453\n",
      "Epoch 2240, Loss: -60.634056091308594\n",
      "Epoch 2260, Loss: -63.398826599121094\n",
      "Epoch 2280, Loss: -66.22513580322266\n",
      "Epoch 2300, Loss: -69.11631774902344\n",
      "Epoch 2320, Loss: -72.07530975341797\n",
      "Epoch 2340, Loss: -75.10343933105469\n",
      "Epoch 2360, Loss: -78.20457458496094\n",
      "Epoch 2380, Loss: -81.37936401367188\n",
      "Epoch 2400, Loss: -84.630859375\n",
      "Epoch 2420, Loss: -87.96090698242188\n",
      "Epoch 2440, Loss: -91.37488555908203\n",
      "Epoch 2460, Loss: -94.87189483642578\n",
      "Epoch 2480, Loss: -98.45638275146484\n",
      "Epoch 2500, Loss: -102.13126373291016\n",
      "Epoch 2520, Loss: -105.90105438232422\n",
      "Epoch 2540, Loss: -109.76390075683594\n",
      "Epoch 2560, Loss: -113.72727966308594\n",
      "Epoch 2580, Loss: -117.79183197021484\n",
      "Epoch 2600, Loss: -121.961669921875\n",
      "Epoch 2620, Loss: -126.2398452758789\n",
      "Epoch 2640, Loss: -130.62754821777344\n",
      "Epoch 2660, Loss: -135.1305694580078\n",
      "Epoch 2680, Loss: -139.75196838378906\n",
      "Epoch 2700, Loss: -144.49166870117188\n",
      "Epoch 2720, Loss: -149.35690307617188\n",
      "Epoch 2740, Loss: -154.3500518798828\n",
      "Epoch 2760, Loss: -159.47531127929688\n",
      "Epoch 2780, Loss: -164.7334747314453\n",
      "Epoch 2800, Loss: -170.13076782226562\n",
      "Epoch 2820, Loss: -175.66824340820312\n",
      "Epoch 2840, Loss: -181.35182189941406\n",
      "Epoch 2860, Loss: -187.1841278076172\n",
      "Epoch 2880, Loss: -193.17103576660156\n",
      "Epoch 2900, Loss: -199.31124877929688\n",
      "Epoch 2920, Loss: -205.61500549316406\n",
      "Epoch 2940, Loss: -212.08140563964844\n",
      "Epoch 2960, Loss: -218.71554565429688\n",
      "Epoch 2980, Loss: -225.5249786376953\n",
      "Epoch 3000, Loss: -232.50750732421875\n",
      "Epoch 3020, Loss: -239.66824340820312\n",
      "Epoch 3040, Loss: -247.01571655273438\n",
      "Epoch 3060, Loss: -254.5524444580078\n",
      "Epoch 3080, Loss: -262.2806701660156\n",
      "Epoch 3100, Loss: -270.20703125\n",
      "Epoch 3120, Loss: -278.3316955566406\n",
      "Epoch 3140, Loss: -286.6631774902344\n",
      "Epoch 3160, Loss: -295.20233154296875\n",
      "Epoch 3180, Loss: -303.9566955566406\n",
      "Epoch 3200, Loss: -312.92578125\n",
      "Epoch 3220, Loss: -322.1221618652344\n",
      "Epoch 3240, Loss: -331.5413513183594\n",
      "Epoch 3260, Loss: -341.1937255859375\n",
      "Epoch 3280, Loss: -351.0804748535156\n",
      "Epoch 3300, Loss: -361.2065124511719\n",
      "Epoch 3320, Loss: -371.58038330078125\n",
      "Epoch 3340, Loss: -382.2000427246094\n",
      "Epoch 3360, Loss: -393.0758361816406\n",
      "Epoch 3380, Loss: -404.2086486816406\n",
      "Epoch 3400, Loss: -415.6042175292969\n",
      "Epoch 3420, Loss: -427.26605224609375\n",
      "Epoch 3440, Loss: -439.19952392578125\n",
      "Epoch 3460, Loss: -451.4114074707031\n",
      "Epoch 3480, Loss: -463.89959716796875\n",
      "Epoch 3500, Loss: -476.6788635253906\n",
      "Epoch 3520, Loss: -489.7481689453125\n",
      "Epoch 3540, Loss: -503.1107177734375\n",
      "Epoch 3560, Loss: -516.771484375\n",
      "Epoch 3580, Loss: -530.7387084960938\n",
      "Epoch 3600, Loss: -545.0148315429688\n",
      "Epoch 3620, Loss: -559.603271484375\n",
      "Epoch 3640, Loss: -574.5133056640625\n",
      "Epoch 3660, Loss: -589.744140625\n",
      "Epoch 3680, Loss: -605.3035278320312\n",
      "Epoch 3700, Loss: -621.1950073242188\n",
      "Epoch 3720, Loss: -637.4263305664062\n",
      "Epoch 3740, Loss: -653.9984130859375\n",
      "Epoch 3760, Loss: -670.9136962890625\n",
      "Epoch 3780, Loss: -688.1839599609375\n",
      "Epoch 3800, Loss: -705.8084106445312\n",
      "Epoch 3820, Loss: -723.7926025390625\n",
      "Epoch 3840, Loss: -742.1416015625\n",
      "Epoch 3860, Loss: -760.8649291992188\n",
      "Epoch 3880, Loss: -779.9591674804688\n",
      "Epoch 3900, Loss: -799.4310302734375\n",
      "Epoch 3920, Loss: -819.2911987304688\n",
      "Epoch 3940, Loss: -839.5303955078125\n",
      "Epoch 3960, Loss: -860.1726684570312\n",
      "Epoch 3980, Loss: -881.2107543945312\n",
      "Epoch 4000, Loss: -902.6454467773438\n",
      "Epoch 4020, Loss: -924.4922485351562\n",
      "Epoch 4040, Loss: -946.7520141601562\n",
      "Epoch 4060, Loss: -969.4205932617188\n",
      "Epoch 4080, Loss: -992.512451171875\n",
      "Epoch 4100, Loss: -1016.0323486328125\n",
      "Epoch 4120, Loss: -1039.975830078125\n",
      "Epoch 4140, Loss: -1064.3565673828125\n",
      "Epoch 4160, Loss: -1089.17529296875\n",
      "Epoch 4180, Loss: -1114.433349609375\n",
      "Epoch 4200, Loss: -1140.1429443359375\n",
      "Epoch 4220, Loss: -1166.2977294921875\n",
      "Epoch 4240, Loss: -1192.91162109375\n",
      "Epoch 4260, Loss: -1219.9866943359375\n",
      "Epoch 4280, Loss: -1247.5194091796875\n",
      "Epoch 4300, Loss: -1275.5230712890625\n",
      "Epoch 4320, Loss: -1303.9981689453125\n",
      "Epoch 4340, Loss: -1332.9483642578125\n",
      "Epoch 4360, Loss: -1362.3802490234375\n",
      "Epoch 4380, Loss: -1392.2996826171875\n",
      "Epoch 4400, Loss: -1422.7049560546875\n",
      "Epoch 4420, Loss: -1453.59765625\n",
      "Epoch 4440, Loss: -1484.99169921875\n",
      "Epoch 4460, Loss: -1516.8861083984375\n",
      "Epoch 4480, Loss: -1549.2869873046875\n",
      "Epoch 4500, Loss: -1582.19384765625\n",
      "Epoch 4520, Loss: -1615.61572265625\n",
      "Epoch 4540, Loss: -1649.5535888671875\n",
      "Epoch 4560, Loss: -1684.0054931640625\n",
      "Epoch 4580, Loss: -1718.983154296875\n",
      "Epoch 4600, Loss: -1754.4857177734375\n",
      "Epoch 4620, Loss: -1790.526611328125\n",
      "Epoch 4640, Loss: -1827.0943603515625\n",
      "Epoch 4660, Loss: -1864.207763671875\n",
      "Epoch 4680, Loss: -1901.8621826171875\n",
      "Epoch 4700, Loss: -1940.0528564453125\n",
      "Epoch 4720, Loss: -1978.7987060546875\n",
      "Epoch 4740, Loss: -2018.098876953125\n",
      "Epoch 4760, Loss: -2057.944580078125\n",
      "Epoch 4780, Loss: -2098.360107421875\n",
      "Epoch 4800, Loss: -2139.33056640625\n",
      "Epoch 4820, Loss: -2180.877685546875\n",
      "Epoch 4840, Loss: -2222.98388671875\n",
      "Epoch 4860, Loss: -2265.664306640625\n",
      "Epoch 4880, Loss: -2308.9208984375\n",
      "Epoch 4900, Loss: -2352.75439453125\n",
      "Epoch 4920, Loss: -2397.169677734375\n",
      "Epoch 4940, Loss: -2442.171875\n",
      "Epoch 4960, Loss: -2487.763427734375\n",
      "Epoch 4980, Loss: -2533.9443359375\n",
      "weights of  P007 :  [[array([[-2.478364  ,  4.277366  , -0.9230365 ],\n",
      "       [ 1.9905776 , -3.032406  ,  1.195018  ],\n",
      "       [-0.54694104, -0.33714765, -0.33617514],\n",
      "       [-0.15093188, -0.09412734,  1.4092126 ],\n",
      "       [ 0.99502623, -0.4162065 , -0.4632106 ]], dtype=float32), array([-1.052138 , -1.331747 ,  1.1970183], dtype=float32)]]\n",
      "loss of  P007 :  0.011979166\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.60      0.63        10\n",
      "           1       0.91      1.00      0.95        10\n",
      "           2       0.60      0.60      0.60        10\n",
      "\n",
      "    accuracy                           0.73        30\n",
      "   macro avg       0.73      0.73      0.73        30\n",
      "weighted avg       0.73      0.73      0.73        30\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[ 6  0  4]\n",
      " [ 0 10  0]\n",
      " [ 3  1  6]]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      1.00      0.79        13\n",
      "           1       0.91      0.83      0.87        12\n",
      "           2       1.00      0.57      0.73        14\n",
      "\n",
      "    accuracy                           0.79        39\n",
      "   macro avg       0.85      0.80      0.79        39\n",
      "weighted avg       0.86      0.79      0.79        39\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[13  0  0]\n",
      " [ 2 10  0]\n",
      " [ 5  1  8]]\n"
     ]
    }
   ],
   "source": [
    "# Custom loss function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "class CustomLossLL2(tf.keras.losses.Loss):\n",
    "    \n",
    "    def __init__(self, lambda_t, model, mu, sigma_TL):\n",
    "        super().__init__()\n",
    "        self.lambda_t = lambda_t\n",
    "        self.cross_entropy = CategoricalCrossentropy()\n",
    "        self.model = model\n",
    "        self.mu = tf.convert_to_tensor(mu, dtype=tf.float32)\n",
    "        self.sigma_TL = tf.convert_to_tensor(sigma_TL, dtype=tf.float32)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce_loss = self.cross_entropy(y_true, y_pred)\n",
    "        wt = self.get_weights_from_model()\n",
    "        reg_term = self.regularization_term(wt)\n",
    "\n",
    "        return ce_loss + (self.lambda_t * tf.linalg.matmul(reg_term, wt))\n",
    "\n",
    "    def get_weights_from_model(self):\n",
    "        model_weights = []\n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.get_weights()) > 0:\n",
    "                model_weights.append(layer.get_weights()[0])\n",
    "        # return tf.concat([tf.reshape(w, [-1]) for w in model_weights], axis=0)\n",
    "        return model_weights\n",
    "\n",
    "    def regularization_term(self, wt):\n",
    "        diff = wt - self.mu\n",
    "        reg_term = 0.5 * tf.linalg.matmul(tf.linalg.matmul(tf.linalg.inv(self.sigma_TL), diff[0]), tf.transpose(diff[0]))\n",
    "        reg_term += 0.5 * tf.math.log(tf.linalg.det(self.sigma_TL))\n",
    "        return reg_term\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "def custom_train_step(model, optimizer, x, y, custom_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True) # Perform a forward pass and compute the predictions\n",
    "        loss = custom_loss(y, y_pred) # Compute the custom loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_weight_LL2(X_train, y_train, lambd, mu, sigma_TL, num_tier=10, learning_rate = 0.01):\n",
    "    n_classes = np.unique(y_train).size\n",
    "    _, sequential_indices = np.unique(y_train, return_inverse=True)\n",
    "    y_one_hot = tf.keras.utils.to_categorical(sequential_indices, num_classes=n_classes)\n",
    "\n",
    "    lambda_t = lambd  # Regularization parameter\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n_classes, input_shape=(X_train.shape[1],), activation='softmax')  # Adjust input_shape to match the number of features in X\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate)\n",
    "    custom_loss = CustomLossLL2(lambda_t, model, mu, sigma_TL)\n",
    "\n",
    "    # Custom training loop\n",
    "    epochs = num_tier\n",
    "    lowest_loss = float('inf')\n",
    "    best_weights = None\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = custom_train_step(model, optimizer, X_train, y_one_hot, custom_loss)\n",
    "        loss_value = loss.numpy()\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        if (abs(loss_value) < lowest_loss):\n",
    "            lowest_loss = abs(loss_value)\n",
    "            best_model = model\n",
    "            best_weights = [layer.get_weights() for layer in model.layers]\n",
    "\n",
    "    return best_model, best_weights, lowest_loss\n",
    "\n",
    "def GetConfusionMatrix(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred_prob = model.predict(X_train)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "    print(\"Classification TRAIN DATA \\n=======================\")\n",
    "    print(classification_report(y_true= y_train, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true= y_train, y_pred=y_pred))\n",
    "\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    print(\"Classification TEST DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "\n",
    "def tgt_test_wLTL(data, target_subjects ,condition):\n",
    "        tgt_data = target_subjects + \"_test\"\n",
    "\n",
    "        if condition == \"noEA\":\n",
    "            X = data[target_subjects]['Raw_csp']\n",
    "            y = data[target_subjects]['Raw_csp_label']\n",
    "            X_test = data[tgt_data]['Raw_csp']\n",
    "            y_test = data[tgt_data]['Raw_csp_label']\n",
    "            store_ws = 'wt_Raw'\n",
    "\n",
    "        else:\n",
    "            X = data[target_subjects]['EA_csp']\n",
    "            y = data[target_subjects]['EA_csp_label']\n",
    "            X_test = data[tgt_data]['EA_csp']\n",
    "            y_test = data[tgt_data]['EA_csp_label']\n",
    "            store_ws = 'wt_EA'\n",
    "\n",
    "        mu = data[target_subjects]['mu_ws']\n",
    "        sigma_TL = data[target_subjects]['Sigma_TL']\n",
    "\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "        \n",
    "        model, weights, loss = train_weight_LL2(X_train=X_train, y_train=y_train, mu =mu, sigma_TL=sigma_TL, lambd= 0.1, num_tier=5000, learning_rate= 0.01)\n",
    "        print(\"weights of \", str(target_subjects), \": \", weights)\n",
    "        print(\"loss of \", str(target_subjects), \": \", loss)\n",
    "        data[target_subjects][store_ws] = weights\n",
    "\n",
    "        GetConfusionMatrix(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "tgt_test_wLTL(CSP2D_Epoch, target_subjects= target_data_0 ,condition = condition_wLTL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
